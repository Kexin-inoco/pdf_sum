```markdown
Table of Contents
1. Introduction (Page 1)
2. Literature Review (Page 3)
3. Data (Page 5)
4. Methodology (Page 6)
    4.1. Removal of incomplete entries: All entries lacking application summaries were (Page 7)
    4.2. Elimination of duplicates: Duplicate records were identified and eliminated to maintain (Page 7)
    4.3. Translation of non-English content: Non-English content, such as French and Italian, (Page 7)
    4.4. Cleaning of non-textual elements: Non-textual elements, including punctuation, (Page 7)
    4.5. Text normalization: The text was converted to lowercase and tokenized, facilitating (Page 7)
    4.6. Stop-word removal: Common English stop-words were removed to reduce noise and (Page 7)
    4.7. Exclusion of domain-specific terms: In addition to general stop-words, domain-specific (Page 7)
    4.8. Lemmatization: Words were reduced to their root forms by lemmatization, aiding in the (Page 7)
    4.9. N-grams: Bigrams and trigrams were also considered to capture contextually relevant (Page 7)
    4.10. Triplet Matches (n=5): Groups consisting of one topic from each model (BERTopic, (Page 8)
    4.11. Unique Topics (n=8): Topics from a single model that did not achieve a cosine (Page 9)
5. Results (Page 12)
    5.1. BERTopic: Leverages pre-trained transformer models to generate context-aware (Page 16)
    5.2. LDA: As a generative probabilistic model, LDA assumes topics are distributions over (Page 17)
6. Conclusion (Page 20)
7. Limitations and Future Work (Page 21)
```
{
  "ai_generated_toc": "```markdown\nTable of Contents\n1. Introduction (Page 1)\n2. Literature Review (Page 3)\n3. Data (Page 5)\n4. Methodology (Page 6)\n    4.1. Removal of incomplete entries: All entries lacking application summaries were (Page 7)\n    4.2. Elimination of duplicates: Duplicate records were identified and eliminated to maintain (Page 7)\n    4.3. Translation of non-English content: Non-English content, such as French and Italian, (Page 7)\n    4.4. Cleaning of non-textual elements: Non-textual elements, including punctuation, (Page 7)\n    4.5. Text normalization: The text was converted to lowercase and tokenized, facilitating (Page 7)\n    4.6. Stop-word removal: Common English stop-words were removed to reduce noise and (Page 7)\n    4.7. Exclusion of domain-specific terms: In addition to general stop-words, domain-specific (Page 7)\n    4.8. Lemmatization: Words were reduced to their root forms by lemmatization, aiding in the (Page 7)\n    4.9. N-grams: Bigrams and trigrams were also considered to capture contextually relevant (Page 7)\n    4.10. Triplet Matches (n=5): Groups consisting of one topic from each model (BERTopic, (Page 8)\n    4.11. Unique Topics (n=8): Topics from a single model that did not achieve a cosine (Page 9)\n5. Results (Page 12)\n    5.1. BERTopic: Leverages pre-trained transformer models to generate context-aware (Page 16)\n    5.2. LDA: As a generative probabilistic model, LDA assumes topics are distributions over (Page 17)\n6. Conclusion (Page 20)\n7. Limitations and Future Work (Page 21)\n```",
  "extracted_titles": [
    {
      "title": "1. Introduction",
      "page": 1,
      "original_text": "1. Introduction (Page 1)"
    },
    {
      "title": "2. Literature Review",
      "page": 3,
      "original_text": "2. Literature Review (Page 3)"
    },
    {
      "title": "3. Data",
      "page": 5,
      "original_text": "3. Data (Page 5)"
    },
    {
      "title": "4. Methodology",
      "page": 6,
      "original_text": "4. Methodology (Page 6)"
    },
    {
      "title": "1. Removal of incomplete entries: All entries lacking application summaries were",
      "page": 7,
      "original_text": "1. Removal of incomplete entries: All entries lacking application summaries were (Page 7)"
    },
    {
      "title": "2.  Elimination of duplicates: Duplicate records were identified and eliminated to maintain",
      "page": 7,
      "original_text": "2.  Elimination of duplicates: Duplicate records were identified and eliminated to maintain (Page 7)"
    },
    {
      "title": "3. Translation of non-English content: Non-English content, such as French and Italian,",
      "page": 7,
      "original_text": "3. Translation of non-English content: Non-English content, such as French and Italian, (Page 7)"
    },
    {
      "title": "4. Cleaning of non-textual elements: Non-textual elements, including punctuation,",
      "page": 7,
      "original_text": "4. Cleaning of non-textual elements: Non-textual elements, including punctuation, (Page 7)"
    },
    {
      "title": "5. Text normalization: The text was converted to lowercase and tokenized, facilitating",
      "page": 7,
      "original_text": "5. Text normalization: The text was converted to lowercase and tokenized, facilitating (Page 7)"
    },
    {
      "title": "6. Stop-word removal: Common English stop-words were removed to reduce noise and",
      "page": 7,
      "original_text": "6. Stop-word removal: Common English stop-words were removed to reduce noise and (Page 7)"
    },
    {
      "title": "7. Exclusion of domain-specific terms: In addition to general stop-words, domain-specific",
      "page": 7,
      "original_text": "7. Exclusion of domain-specific terms: In addition to general stop-words, domain-specific (Page 7)"
    },
    {
      "title": "8. Lemmatization: Words were reduced to their root forms by lemmatization, aiding in the",
      "page": 7,
      "original_text": "8. Lemmatization: Words were reduced to their root forms by lemmatization, aiding in the (Page 7)"
    },
    {
      "title": "9. N-grams: Bigrams and trigrams were also considered to capture contextually relevant",
      "page": 7,
      "original_text": "9. N-grams: Bigrams and trigrams were also considered to capture contextually relevant (Page 7)"
    },
    {
      "title": "• Triplet Matches (n=5): Groups consisting of one topic from each model (BERTopic,",
      "page": 8,
      "original_text": "• Triplet Matches (n=5): Groups consisting of one topic from each model (BERTopic, (Page 8)"
    },
    {
      "title": "• Unique Topics (n=8): Topics from a single model that did not achieve a cosine",
      "page": 9,
      "original_text": "• Unique Topics (n=8): Topics from a single model that did not achieve a cosine (Page 9)"
    },
    {
      "title": "5. Results",
      "page": 12,
      "original_text": "5. Results (Page 12)"
    },
    {
      "title": "• BERTopic: Leverages pre-trained transformer models to generate context-aware",
      "page": 16,
      "original_text": "• BERTopic: Leverages pre-trained transformer models to generate context-aware (Page 16)"
    },
    {
      "title": "• LDA: As a generative probabilistic model, LDA assumes topics are distributions over",
      "page": 17,
      "original_text": "• LDA: As a generative probabilistic model, LDA assumes topics are distributions over (Page 17)"
    },
    {
      "title": "6. Conclusion",
      "page": 20,
      "original_text": "6. Conclusion (Page 20)"
    },
    {
      "title": "7. Limitations and Future Work",
      "page": 21,
      "original_text": "7. Limitations and Future Work (Page 21)"
    }
  ],
  "metadata": {
    "total_pages": 35,
    "titles_found": 39,
    "model": "gpt-4-turbo-preview",
    "temperature": 0.3,
    "timestamp": "2025-10-22T16:12:53.888976"
  }
}
{
  "source_file": "2510.18659v1.pdf",
  "total_documents": 19,
  "documents": [
    {
      "document_index": 0,
      "page": 1,
      "content": "Sherlock Your Queries: Learning to Ask the Right Questions\nfor Dialogue-Based Retrieval\n\nDong Yun*, Marco Schouten*, Dim Papadopoulos\nTechnical University of Denmark (DTU)\nAnker Engelunds Vej 101, 2800 Kongens Lyngby\n\n{s232293, marschou, dimp}@dtu.dk\n\nAbstract\n\nUser queries in information retrieval are often ambiguous,\nmaking it challenging for systems to identify a user’s target\nfrom a single query. While recent dialogue-based interac-\ntive retrieval systems can clarify user intent, they are ineffi-\ncient as they often lack an explicit strategy to ask the most\ninformative questions. To address this limitation, we pro-\npose SherlockLLM, a dialogue-driven retrieval framework\nthat learns an optimal questioning strategy via Reinforce-\nment Learning (RL) and avoids the need for large-scale\nannotated dialogue data. In our framework, an agent is\ntrained to generate a sequence of binary questions to effi-\nciently narrow down the search space. To validate our ap-\nproach, we introduce a benchmark with both structured and\nunstructured tasks. Experimental results show that Sher-\nlockLLM is a robust and efficient solution. On the structured\ntasks, its performance matches strong baselines and ap-\nproaches the theoretical optimal defined by binary search.\nOn the challenging unstructured task, our agent signifi-\ncantly outperforms these baselines, showcasing its ability\nto learn a highly effective information-seeking dialogue pol-\nicy. We will release the code and models upon acceptance.\n\n1. Introduction\n\nUser queries in information retrieval tasks are often am-\nbiguous or incomplete [49]. This poses a significant chal-\nlenge for traditional retrieval systems that rely on precise\nand comprehensive input. This is particularly evident in\nscenarios where users have a clear target in mind but lack\nthe specific keywords to describe it, such as trying to find\nan image of a celebrity whose name they cannot recall. In\nsuch cases, a simple, one-shot query is often insufficient to\nguarantee the retrieval of a specific target image.\nDialogue-based interactive retrieval offers an effective\nsolution by enabling systems to clarify user intent through\n\n*These authors contributed equally.\n\nNo.\n\nNo.\n\nYes.\n\nHelp me find a picture of a celebrity \nwhose name I can not recall.\n\nNo.\n\nIs she wearing glasses?\n\nAre you looking for one of these images:\n\nIs her hair blonde?\n\nDoes she have short hair?\n\nOk. Is the celebrity male?\n\nFigure 1.\nOur dialogue-based information retrieval agent.\nWithout precise target queries, the agent identifies the desired tar-\nget through a series of yes/no questions. At each turn, the agent\ngenerates the most informative question for the user, minimizing\nthe number of interactions needed to determine the target.\n\nmulti-turn interactions [45, 48]. As illustrated in Fig. 1, an\nagent can engage in a conversation, asking targeted ques-\n\n1\n\narXiv:2510.18659v1  [cs.AI]  21 Oct 2025",
      "content_length": 2915,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "Sherlock Your Queries: Learning to Ask the Right Questions\nfor Dialogue-Based Retrieval",
          "is_title": true,
          "page": 1,
          "text_length": 87
        },
        {
          "text": "Dong Yun*, Marco Schouten*, Dim Papadopoulos\nTechnical University of Denmark (DTU)\nAnker Engelunds Vej 101, 2800 Kongens Lyngby",
          "is_title": false,
          "page": 1,
          "text_length": 127
        },
        {
          "text": "{s232293, marschou, dimp}@dtu.dk",
          "is_title": false,
          "page": 1,
          "text_length": 32
        },
        {
          "text": "Abstract",
          "is_title": false,
          "page": 1,
          "text_length": 8
        },
        {
          "text": "User queries in information retrieval are often ambiguous,\nmaking it challenging for systems to identify a user’s target\nfrom a single query. While recent dialogue-based interac-\ntive retrieval systems can clarify user intent, they are ineffi-\ncient as they often lack an explicit strategy to ask the most\ninformative questions. To address this limitation, we pro-\npose SherlockLLM, a dialogue-driven retrieval framework\nthat learns an optimal questioning strategy via Reinforce-\nment Learning (RL) and avoids the need for large-scale\nannotated dialogue data. In our framework, an agent is\ntrained to generate a sequence of binary questions to effi-\nciently narrow down the search space. To validate our ap-\nproach, we introduce a benchmark with both structured and\nunstructured tasks. Experimental results show that Sher-\nlockLLM is a robust and efficient solution. On the structured\ntasks, its performance matches strong baselines and ap-\nproaches the theoretical optimal defined by binary search.\nOn the challenging unstructured task, our agent signifi-\ncantly outperforms these baselines, showcasing its ability\nto learn a highly effective information-seeking dialogue pol-\nicy. We will release the code and models upon acceptance.",
          "is_title": false,
          "page": 1,
          "text_length": 1235
        },
        {
          "text": "1. Introduction",
          "is_title": true,
          "page": 1,
          "text_length": 15
        },
        {
          "text": "User queries in information retrieval tasks are often am-\nbiguous or incomplete [49]. This poses a significant chal-\nlenge for traditional retrieval systems that rely on precise\nand comprehensive input. This is particularly evident in\nscenarios where users have a clear target in mind but lack\nthe specific keywords to describe it, such as trying to find\nan image of a celebrity whose name they cannot recall. In\nsuch cases, a simple, one-shot query is often insufficient to\nguarantee the retrieval of a specific target image.\nDialogue-based interactive retrieval offers an effective\nsolution by enabling systems to clarify user intent through",
          "is_title": false,
          "page": 1,
          "text_length": 643
        },
        {
          "text": "*These authors contributed equally.",
          "is_title": false,
          "page": 1,
          "text_length": 35
        },
        {
          "text": "No.",
          "is_title": false,
          "page": 1,
          "text_length": 3
        },
        {
          "text": "No.",
          "is_title": false,
          "page": 1,
          "text_length": 3
        },
        {
          "text": "Yes.",
          "is_title": false,
          "page": 1,
          "text_length": 4
        },
        {
          "text": "Help me find a picture of a celebrity \nwhose name I can not recall.",
          "is_title": false,
          "page": 1,
          "text_length": 67
        },
        {
          "text": "No.",
          "is_title": false,
          "page": 1,
          "text_length": 3
        },
        {
          "text": "Is she wearing glasses?",
          "is_title": false,
          "page": 1,
          "text_length": 23
        },
        {
          "text": "Are you looking for one of these images:",
          "is_title": false,
          "page": 1,
          "text_length": 40
        },
        {
          "text": "Is her hair blonde?",
          "is_title": false,
          "page": 1,
          "text_length": 19
        },
        {
          "text": "Does she have short hair?",
          "is_title": false,
          "page": 1,
          "text_length": 25
        },
        {
          "text": "Ok. Is the celebrity male?",
          "is_title": false,
          "page": 1,
          "text_length": 26
        },
        {
          "text": "Figure 1.\nOur dialogue-based information retrieval agent.\nWithout precise target queries, the agent identifies the desired tar-\nget through a series of yes/no questions. At each turn, the agent\ngenerates the most informative question for the user, minimizing\nthe number of interactions needed to determine the target.",
          "is_title": false,
          "page": 1,
          "text_length": 317
        },
        {
          "text": "multi-turn interactions [45, 48]. As illustrated in Fig. 1, an\nagent can engage in a conversation, asking targeted ques-",
          "is_title": false,
          "page": 1,
          "text_length": 120
        },
        {
          "text": "1",
          "is_title": false,
          "page": 1,
          "text_length": 1
        },
        {
          "text": "arXiv:2510.18659v1  [cs.AI]  21 Oct 2025",
          "is_title": false,
          "page": 1,
          "text_length": 40
        }
      ]
    },
    {
      "document_index": 1,
      "page": 2,
      "content": "tions to iteratively narrow down a large pool of candidates\nand pinpoint the correct item. A well-formulated question\nhas been shown to improve retrieval efficiency by as much\nas 100% [2]. Recent approaches have used Large Language\nModels (LLMs) to generate clarification questions in re-\ntrieval systems [20, 21, 23, 40]. However, the reasoning ca-\npabilities of LLMs do not inherently translate into effective\ninformation-seeking strategies. This is because the models\nlack an explicit learning signal aligned with the objective of\nuncertainty reduction [13]. Therefore, the central challenge\nis to generate the most informative question to minimize the\nnumber of iterations required at each dialogue turn.\n\nIn this work, we propose SherlockLLM, a dialogue-based\nretrieval framework that learns this strategic questioning\nability.\nWe formulate the task as a sequential decision-\nmaking problem solved via RL. The objective is to learn\na policy that generates a sequence of questions to mini-\nmize the number of turns needed to achieve a satisfactory\nretrieval outcome.\nWe train an agent to interact with a\nretrieval environment, where it receives rewards based on\nhow effectively its generated questions reduce the candidate\nspace. Through this process, the agent autonomously learns\na dialogue policy optimized for retrieval efficiency.\n\nTo validate SherlockLLM’s effectiveness and generaliz-\nability, we evaluate it on two distinct retrieval tasks with\nboth structured and unstructured data. For structured data,\nwe introduce a benchmark composed of two tasks: Guess\nNumber and Guess Who. These tasks provide a controlled\nenvironment for evaluating the agent’s ability to filter and\nretrieve targets via dialogue. The agent is guided by a re-\nward function based on Expected Information Gain (EIG),\nand evaluates retrieval efficiency using binary search as a\nreference for optimal performance. For the more complex\ntask of unstructured image retrieval, the agent’s questioning\npolicy is guided by image captions. Here, the reward func-\ntion is based on the rank of the target image in a text-based\nsearch, directly optimizing for retrieval performance.\n\nOur key contributions are threefold:\n\n1. We\npropose\nSherlockLLM,\na\nRL\nframework\nfor\ndialogue-driven retrieval that autonomously learns a pol-\nicy to generate optimally informative questions.\n2. We introduce a benchmark comprising both structured\n(Guess Number and Guess Who) and unstructured (Im-\nage Retrieval) tasks to assess dialogue policy.\n3. Experimental results demonstrate that our approach is\na robust and generalizable solution for interactive in-\nformation retrieval. On tabular tasks, its performance\nmatches strong baselines like DeepSeek-V3.1 [24] and\napproaches the theoretical optimum. On the more chal-\nlenging image retrieval task, it comprehensively outper-\nforms strong LLMs with 96× more parameters.\n\n2. Related Work\n\nInformation retrieval (IR) aims to efficiently retrieve rel-\nevant information from user queries, generally expressed\nin text form [8, 18, 35–37].\nWith the advent of LLMs,\nIR pipelines are composed of four components: Query\nRewriter, Retriever, Reranker, and Reader [50]. The Query\nRewriter refines the initial user query to better capture\ninformation needs.\nRetrievers project queries into high-\ndimensional vector spaces and compute relevance via in-\nner product scoring. Rerankers apply matching methods\nand domain-specific objectives to refine candidate sets to\nimprove result quality. Optionally, Readers process with\nLLMs retrieved information as an answer in natural lan-\nguage. LLMs can support different stages of the pipeline\nto better capture user intents and behavior [1]. We pro-\npose a dialogue-based framework for the Query Rewriter\nthat leverages LLMs to generate context-aware questions.\nThese questions are conditioned on the history of previous\nuser answers and guide users through a dialogue to itera-\ntively refine and improve search results.\nLLMs as Query Rewriters. Query retrieval often suffers\nfrom ambiguity or incompleteness in user queries.\nTwo\ncommon strategies to address this are query expansion and\nconversational query rewriting. Query expansion, enriches\nthe original query with semantically related terms [16, 43].\nConversational Query Rewriters addresses user query am-\nbiguity through iterative, multi-turn dialogues between the\nIR system and user [45, 48]. Traditional methods struggle\nwith diverse or long-tailed dialogue sessions [7], whereas\nLLMs’ contextual understanding allows them to manage\ncomplex interactions and generate versatile queries in for-\nmats such as questions or answer-incorporated passages\n[28].\nMoreover, RAG-based LLMs integrate explicit re-\ntrieval modules that allow the model to access external\nknowledge to support query generation [3, 10, 15, 27, 41].\nBuilding on these advances, our approach employs a Query\nRewriter LLM that formulates retrieval queries through di-\nalogue by generating questions and incorporating binary\n(yes/no) answers from the user’s conversation history.\nReinforcement Learning.\nQuery rewriters operate be-\ntween users and retrieval modules, but this intermediary\nrole often leaves them without a dedicated loss function\nfor optimization [50]. This limitation motivates the use of\nRL to better align query generation with downstream re-\ntrieval objectives. RL incorporates feedback from multi-\nple sources.\nRanking models provide preference signals\nthrough good-bad pairs [29, 34]. Offline scoring mecha-\nnisms, such as BEQUE [31], estimate query utility based\non retrieved items. Retrieval-metric rewards, inspired by\nDeepSeek-R1 [9] and adopted in DeepRetrieval [17], en-\nable optimization even without supervised data.\nCollec-\ntively, these strategies allow query rewriters to learn policies\nthat more effectively improve retrieval.\n\n2",
      "content_length": 5827,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "tions to iteratively narrow down a large pool of candidates\nand pinpoint the correct item. A well-formulated question\nhas been shown to improve retrieval efficiency by as much\nas 100% [2]. Recent approaches have used Large Language\nModels (LLMs) to generate clarification questions in re-\ntrieval systems [20, 21, 23, 40]. However, the reasoning ca-\npabilities of LLMs do not inherently translate into effective\ninformation-seeking strategies. This is because the models\nlack an explicit learning signal aligned with the objective of\nuncertainty reduction [13]. Therefore, the central challenge\nis to generate the most informative question to minimize the\nnumber of iterations required at each dialogue turn.",
          "is_title": false,
          "page": 2,
          "text_length": 708
        },
        {
          "text": "In this work, we propose SherlockLLM, a dialogue-based\nretrieval framework that learns this strategic questioning\nability.\nWe formulate the task as a sequential decision-\nmaking problem solved via RL. The objective is to learn\na policy that generates a sequence of questions to mini-\nmize the number of turns needed to achieve a satisfactory\nretrieval outcome.\nWe train an agent to interact with a\nretrieval environment, where it receives rewards based on\nhow effectively its generated questions reduce the candidate\nspace. Through this process, the agent autonomously learns\na dialogue policy optimized for retrieval efficiency.",
          "is_title": false,
          "page": 2,
          "text_length": 629
        },
        {
          "text": "To validate SherlockLLM’s effectiveness and generaliz-\nability, we evaluate it on two distinct retrieval tasks with\nboth structured and unstructured data. For structured data,\nwe introduce a benchmark composed of two tasks: Guess\nNumber and Guess Who. These tasks provide a controlled\nenvironment for evaluating the agent’s ability to filter and\nretrieve targets via dialogue. The agent is guided by a re-\nward function based on Expected Information Gain (EIG),\nand evaluates retrieval efficiency using binary search as a\nreference for optimal performance. For the more complex\ntask of unstructured image retrieval, the agent’s questioning\npolicy is guided by image captions. Here, the reward func-\ntion is based on the rank of the target image in a text-based\nsearch, directly optimizing for retrieval performance.",
          "is_title": false,
          "page": 2,
          "text_length": 815
        },
        {
          "text": "Our key contributions are threefold:",
          "is_title": false,
          "page": 2,
          "text_length": 36
        },
        {
          "text": "1. We\npropose\nSherlockLLM,\na\nRL\nframework\nfor\ndialogue-driven retrieval that autonomously learns a pol-\nicy to generate optimally informative questions.\n2. We introduce a benchmark comprising both structured\n(Guess Number and Guess Who) and unstructured (Im-\nage Retrieval) tasks to assess dialogue policy.\n3. Experimental results demonstrate that our approach is\na robust and generalizable solution for interactive in-\nformation retrieval. On tabular tasks, its performance\nmatches strong baselines like DeepSeek-V3.1 [24] and\napproaches the theoretical optimum. On the more chal-\nlenging image retrieval task, it comprehensively outper-\nforms strong LLMs with 96× more parameters.",
          "is_title": false,
          "page": 2,
          "text_length": 682
        },
        {
          "text": "2. Related Work",
          "is_title": true,
          "page": 2,
          "text_length": 15
        },
        {
          "text": "Information retrieval (IR) aims to efficiently retrieve rel-\nevant information from user queries, generally expressed\nin text form [8, 18, 35–37].\nWith the advent of LLMs,\nIR pipelines are composed of four components: Query\nRewriter, Retriever, Reranker, and Reader [50]. The Query\nRewriter refines the initial user query to better capture\ninformation needs.\nRetrievers project queries into high-\ndimensional vector spaces and compute relevance via in-\nner product scoring. Rerankers apply matching methods\nand domain-specific objectives to refine candidate sets to\nimprove result quality. Optionally, Readers process with\nLLMs retrieved information as an answer in natural lan-\nguage. LLMs can support different stages of the pipeline\nto better capture user intents and behavior [1]. We pro-\npose a dialogue-based framework for the Query Rewriter\nthat leverages LLMs to generate context-aware questions.\nThese questions are conditioned on the history of previous\nuser answers and guide users through a dialogue to itera-\ntively refine and improve search results.\nLLMs as Query Rewriters. Query retrieval often suffers\nfrom ambiguity or incompleteness in user queries.\nTwo\ncommon strategies to address this are query expansion and\nconversational query rewriting. Query expansion, enriches\nthe original query with semantically related terms [16, 43].\nConversational Query Rewriters addresses user query am-\nbiguity through iterative, multi-turn dialogues between the\nIR system and user [45, 48]. Traditional methods struggle\nwith diverse or long-tailed dialogue sessions [7], whereas\nLLMs’ contextual understanding allows them to manage\ncomplex interactions and generate versatile queries in for-\nmats such as questions or answer-incorporated passages\n[28].\nMoreover, RAG-based LLMs integrate explicit re-\ntrieval modules that allow the model to access external\nknowledge to support query generation [3, 10, 15, 27, 41].\nBuilding on these advances, our approach employs a Query\nRewriter LLM that formulates retrieval queries through di-\nalogue by generating questions and incorporating binary\n(yes/no) answers from the user’s conversation history.\nReinforcement Learning.\nQuery rewriters operate be-\ntween users and retrieval modules, but this intermediary\nrole often leaves them without a dedicated loss function\nfor optimization [50]. This limitation motivates the use of\nRL to better align query generation with downstream re-\ntrieval objectives. RL incorporates feedback from multi-\nple sources.\nRanking models provide preference signals\nthrough good-bad pairs [29, 34]. Offline scoring mecha-\nnisms, such as BEQUE [31], estimate query utility based\non retrieved items. Retrieval-metric rewards, inspired by\nDeepSeek-R1 [9] and adopted in DeepRetrieval [17], en-\nable optimization even without supervised data.\nCollec-\ntively, these strategies allow query rewriters to learn policies\nthat more effectively improve retrieval.",
          "is_title": false,
          "page": 2,
          "text_length": 2927
        },
        {
          "text": "2",
          "is_title": false,
          "page": 2,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 2,
      "page": 3,
      "content": "Questioner \n(Pretrained LLM)\n\nRL\nFine-Tuning\n\nTabular\nRetriever\n\nImage\nRetriever\n\nIs the target male?\n\nNo.\n\nYes.\n\nDataset D\n\nAsk\n\nCSV\n\nTarget c*\n\nIs her hair blonde?\n\nAgent\nUser\n\nHelp me find a picture of a \ncelebrity whose name I can \nnot recall.\n\nIs she smiling?\n\nht\n\nqt\n\ngt\n\nFigure 2. The overview of the pipeline. An LLM fine-tuned using\nreinforcement learning acts as a Questioner to ask questions, while\nthe User answers questions as feedback.\n\nUnlike prior work, we utilize an LLM-based Query\nRewriter specifically designed as a Questioner, which we\nfine-tune using reinforcement learning from human feed-\nback.\nConsequently, the Query Rewriter learns to gen-\nerate the most relevant questions based on the conversa-\ntion history and prompts the user with specific keyword at-\ntributes that are essential for successful retrieval. Overall,\nthis framework enables the user to have only a vague idea\nof the query, while the LLM facilitates the interaction with\nthe retriever to efficiently identify the target.\n\n3. Method\n\nSherlockLLM consists of an interactive agent designed to\nretrieve a target item c∗from a dataset D through a multi-\nturn dialogue with a user. The agent’s architecture is com-\nposed of two primary modules: a Retriever and a Ques-\ntioner (Fig. 2). These modules operate sequentially to it-\neratively refine the search space and identify the target. At\nthe beginning of an episode, a user has a target c∗with-\nout a full description. The user initiates the interaction with\na free-form request. At dialogue turn t, the Retriever takes\nthe current dialogue history ht as input and queries D to ob-\ntain results gt. Subsequently, the Questioner receives both\nthe history ht and the results gt. It then generates the next\nyes/no question qt conditioned on both gt and ht.\nA key feature of SherlockLLM is its modularity. The\nlearning mechanism of the Questioner is domain-agnostic,\nwhile the Retriever is domain-specific. This enables seam-\nless adaptation to different retrieval modalities by replacing\nthe retrieval and ranking components.\n\n3.1. Questioner\n\nA common strategy in dialogue-driven retrieval is to prepare\na finite question pool and select a question via heuristics or\n\nAlgorithm 1 Keyword-Conditioned CLIP Retriever\nInputs: ht (dialogue history), I = {I1, . . . , In} (images);\nhyperparameters µ (threshold), β (steepness), d0 ∈(0, 1)\n(base discount)\nOutput: Ranked image list by score S\n\n1: (K+, K−) ←KEYWORDPARSER(ht)\n▷Extract\npositive keywords K+ and negative keywords K−\n\n2: if |K+| > 0 then\n3:\nt+ ←JOIN(K+, “,”) ▷Join positive keywords to a\nquery string\n4:\nS+ ←CLIP(I, t+)\n5: else\n6:\nS+ ←1N\n7: end if\n8: D ←1N\n▷per-image discount vector\n9: for all k−∈K−do\n10:\nS−←CLIP(I, k−)\n11:\nψ ←1−(1−d0)·σ\n\u0000\nβ (S−−µ)\n\u0001\n▷σ(x) =\n1\n1+e−x\n12:\nD ←D ⊙ψ\n▷elementwise product\n\n13: end for\n14: S ←S+ ⊙D\n▷final score per image\n15: return SORT↓(S)\n▷descending list by S\n\na learned network [12, 19, 44]. While effective in narrow\nsettings, this design constrains the agent to a fixed hypoth-\nesis space: it cannot compose novel queries tailored to the\nunfolding dialogue, and transfers poorly across domains.\nWe therefore adopt a pretrained LLM as a dynamic\nQuestioner. By leveraging broad world knowledge and gen-\nerative capacity, the LLM can formulate coherent, strategic\nquestions on the fly. We model the Questioner as a pol-\nicy over questions conditioned on an augmented dialogue\nhistory that fuses user feedback with retrieval signals. We\ndefine the dialogue history up to turn t as a sequence of\nquestion–answer pairs:\n\nht =\n\b\n(q0, a0), . . . , (qt−1, at−1)\n(1)\n\nqt is a question produced by the agent, and at is the user’s\nanswer. To expose the Questioner to both the user’s re-\nsponse and the Retriever’s result, we define an augmented\nanswer ˜at:\n\n˜at = at ⊕ρ (gt)\n(2)\n\nThe function ρ(·) renders the retrieval output gt into text\n(e.g., summarizing attribute distributions from tabular data\nor top-k retrieved images). The operator ⊕denotes string\nconcatenation.\nThe Questioner is a policy π that generates the next ques-\ntion qt given the augmented dialogue history ˜ht:\n\nqt ∼π\n\u0010\n· | ˜ht\n\u0011\n,\n(3)\n\n3",
      "content_length": 4129,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "Questioner \n(Pretrained LLM)",
          "is_title": false,
          "page": 3,
          "text_length": 28
        },
        {
          "text": "RL\nFine-Tuning",
          "is_title": false,
          "page": 3,
          "text_length": 14
        },
        {
          "text": "Tabular\nRetriever",
          "is_title": false,
          "page": 3,
          "text_length": 17
        },
        {
          "text": "Image\nRetriever",
          "is_title": false,
          "page": 3,
          "text_length": 15
        },
        {
          "text": "Is the target male?",
          "is_title": false,
          "page": 3,
          "text_length": 19
        },
        {
          "text": "No.",
          "is_title": false,
          "page": 3,
          "text_length": 3
        },
        {
          "text": "Yes.",
          "is_title": false,
          "page": 3,
          "text_length": 4
        },
        {
          "text": "Dataset D",
          "is_title": false,
          "page": 3,
          "text_length": 9
        },
        {
          "text": "Ask",
          "is_title": false,
          "page": 3,
          "text_length": 3
        },
        {
          "text": "CSV",
          "is_title": false,
          "page": 3,
          "text_length": 3
        },
        {
          "text": "Target c*",
          "is_title": false,
          "page": 3,
          "text_length": 9
        },
        {
          "text": "Is her hair blonde?",
          "is_title": false,
          "page": 3,
          "text_length": 19
        },
        {
          "text": "Agent\nUser",
          "is_title": false,
          "page": 3,
          "text_length": 10
        },
        {
          "text": "Help me find a picture of a \ncelebrity whose name I can \nnot recall.",
          "is_title": false,
          "page": 3,
          "text_length": 68
        },
        {
          "text": "Is she smiling?",
          "is_title": false,
          "page": 3,
          "text_length": 15
        },
        {
          "text": "ht",
          "is_title": false,
          "page": 3,
          "text_length": 2
        },
        {
          "text": "qt",
          "is_title": false,
          "page": 3,
          "text_length": 2
        },
        {
          "text": "gt",
          "is_title": false,
          "page": 3,
          "text_length": 2
        },
        {
          "text": "Figure 2. The overview of the pipeline. An LLM fine-tuned using\nreinforcement learning acts as a Questioner to ask questions, while\nthe User answers questions as feedback.",
          "is_title": false,
          "page": 3,
          "text_length": 171
        },
        {
          "text": "Unlike prior work, we utilize an LLM-based Query\nRewriter specifically designed as a Questioner, which we\nfine-tune using reinforcement learning from human feed-\nback.\nConsequently, the Query Rewriter learns to gen-\nerate the most relevant questions based on the conversa-\ntion history and prompts the user with specific keyword at-\ntributes that are essential for successful retrieval. Overall,\nthis framework enables the user to have only a vague idea\nof the query, while the LLM facilitates the interaction with\nthe retriever to efficiently identify the target.",
          "is_title": false,
          "page": 3,
          "text_length": 564
        },
        {
          "text": "3. Method",
          "is_title": true,
          "page": 3,
          "text_length": 9
        },
        {
          "text": "SherlockLLM consists of an interactive agent designed to\nretrieve a target item c∗from a dataset D through a multi-\nturn dialogue with a user. The agent’s architecture is com-\nposed of two primary modules: a Retriever and a Ques-\ntioner (Fig. 2). These modules operate sequentially to it-\neratively refine the search space and identify the target. At\nthe beginning of an episode, a user has a target c∗with-\nout a full description. The user initiates the interaction with\na free-form request. At dialogue turn t, the Retriever takes\nthe current dialogue history ht as input and queries D to ob-\ntain results gt. Subsequently, the Questioner receives both\nthe history ht and the results gt. It then generates the next\nyes/no question qt conditioned on both gt and ht.\nA key feature of SherlockLLM is its modularity. The\nlearning mechanism of the Questioner is domain-agnostic,\nwhile the Retriever is domain-specific. This enables seam-\nless adaptation to different retrieval modalities by replacing\nthe retrieval and ranking components.",
          "is_title": false,
          "page": 3,
          "text_length": 1035
        },
        {
          "text": "3.1. Questioner",
          "is_title": true,
          "page": 3,
          "text_length": 15
        },
        {
          "text": "A common strategy in dialogue-driven retrieval is to prepare\na finite question pool and select a question via heuristics or",
          "is_title": false,
          "page": 3,
          "text_length": 123
        },
        {
          "text": "Algorithm 1 Keyword-Conditioned CLIP Retriever\nInputs: ht (dialogue history), I = {I1, . . . , In} (images);\nhyperparameters µ (threshold), β (steepness), d0 ∈(0, 1)\n(base discount)\nOutput: Ranked image list by score S",
          "is_title": false,
          "page": 3,
          "text_length": 218
        },
        {
          "text": "1: (K+, K−) ←KEYWORDPARSER(ht)\n▷Extract\npositive keywords K+ and negative keywords K−",
          "is_title": false,
          "page": 3,
          "text_length": 85
        },
        {
          "text": "2: if |K+| > 0 then\n3:\nt+ ←JOIN(K+, “,”) ▷Join positive keywords to a\nquery string\n4:\nS+ ←CLIP(I, t+)\n5: else\n6:\nS+ ←1N\n7: end if\n8: D ←1N\n▷per-image discount vector\n9: for all k−∈K−do\n10:\nS−←CLIP(I, k−)\n11:\nψ ←1−(1−d0)·σ\n\u0000\nβ (S−−µ)\n\u0001\n▷σ(x) =\n1\n1+e−x\n12:\nD ←D ⊙ψ\n▷elementwise product",
          "is_title": false,
          "page": 3,
          "text_length": 283
        },
        {
          "text": "13: end for\n14: S ←S+ ⊙D\n▷final score per image\n15: return SORT↓(S)\n▷descending list by S",
          "is_title": false,
          "page": 3,
          "text_length": 89
        },
        {
          "text": "a learned network [12, 19, 44]. While effective in narrow\nsettings, this design constrains the agent to a fixed hypoth-\nesis space: it cannot compose novel queries tailored to the\nunfolding dialogue, and transfers poorly across domains.\nWe therefore adopt a pretrained LLM as a dynamic\nQuestioner. By leveraging broad world knowledge and gen-\nerative capacity, the LLM can formulate coherent, strategic\nquestions on the fly. We model the Questioner as a pol-\nicy over questions conditioned on an augmented dialogue\nhistory that fuses user feedback with retrieval signals. We\ndefine the dialogue history up to turn t as a sequence of\nquestion–answer pairs:",
          "is_title": false,
          "page": 3,
          "text_length": 655
        },
        {
          "text": "ht =\n\b\n(q0, a0), . . . , (qt−1, at−1)\n(1)",
          "is_title": false,
          "page": 3,
          "text_length": 41
        },
        {
          "text": "qt is a question produced by the agent, and at is the user’s\nanswer. To expose the Questioner to both the user’s re-\nsponse and the Retriever’s result, we define an augmented\nanswer ˜at:",
          "is_title": false,
          "page": 3,
          "text_length": 186
        },
        {
          "text": "˜at = at ⊕ρ (gt)\n(2)",
          "is_title": false,
          "page": 3,
          "text_length": 20
        },
        {
          "text": "The function ρ(·) renders the retrieval output gt into text\n(e.g., summarizing attribute distributions from tabular data\nor top-k retrieved images). The operator ⊕denotes string\nconcatenation.\nThe Questioner is a policy π that generates the next ques-\ntion qt given the augmented dialogue history ˜ht:",
          "is_title": false,
          "page": 3,
          "text_length": 301
        },
        {
          "text": "qt ∼π\n\u0010\n· | ˜ht\n\u0011\n,\n(3)",
          "is_title": false,
          "page": 3,
          "text_length": 23
        },
        {
          "text": "3",
          "is_title": false,
          "page": 3,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 3,
      "page": 4,
      "content": "The augmented history replaces each standard answer\nwith its augmented counterpart:\n\n˜ht =\n\b\n(q0, ˜a0), . . . , (qt−1, ˜at−1)\n.\n(4)\n\n3.2. Retriever\n\nThe retriever serves as the interface between the dialogue\nand the underlying dataset. Its role is to interpret ht, update\nthe agent’s belief over candidates, and return the feedback\nto the questioner. The mechanism is similar to Retrieval-\nAugmented Generation (RAG). The concrete implementa-\ntion is domain-specific to handle the unique characteristics\nof each data modality.\nTabular Retriever. For structured tabular datasets, the re-\ntriever operates deterministic logical filtering that contracts\nthe hypothesis space at every turn. The agent’s question qt\nis first parsed to extract attribute–value pair(s). For instance,\nfrom the question “Is the target’s hair blonde?” the parser\nyields {hair color:\nblonde}. This can be imple-\nmented using either a rule-based system or a lightweight\nLLM. Given the attribute–value pair(s) and the user’s an-\nswer at, the current candidate set Ct is filtered. The retriever\noutputs a new, strictly smaller candidate set |Ct+1| < |Ct|.\nThe distribution of Ct+1 is provided as feedback to the ques-\ntioner. In addition, this pruned set is then used to calculate\nthe EIG for the reward function.\nImage Retriever. We formalize the image retriever as a\nsimilarity-based ranker that integrates positive and nega-\ntive attributes to handle negation robustly.\nThe retriever\nprojects the user’s queries and all candidate images I =\n{I1,\n. . . ,\nIn} into a shared embedding space and re-\nturns a similarity score per image. Vision–language models\n(VLMs) such as CLIP [33] perform well on general retrieval\ntasks but often degrade on queries with negation (e.g., “a\nman without glasses”). To address this limitation, we pro-\npose a multi-stage retrieval process that explicitly models\nboth positive and negative attributes.\nThe retriever first computes positive queries for every\nimage. As detailed in Algorithm 1, our process begins by\nparsing the dialogue history ht to obtain two sets of key-\nwords: positives K+ and negatives K−. We concatenate\npositive keywords into a single text query t+, and compute\nthe vector of positive similarities S+ ∈Rn, the i-th entry\ngives the CLIP similarity between t+ and image Ii.\nThe retriever then converts each negative keyword into\na vector of discounts D that penalize images similar to that\nkeyword. For every k−∈K−, we compute a similarity\nvector S−(k−) ∈Rn whose i-th entry is the CLIP similar-\nity between negative keyword k−and Ii. We transform this\nvector element-wise through a sigmoid gate σ(·) to obtain a\ndiscount factor ψ(S−(k−)) ∈[d0, 1]:\n\nψ(x) = 1 −(1 −d0) · σ\n\u0000\nβ(x −µ)\n\u0001\n(5)\n\nwhere d0 ∈(0, 1) is a base discount, β > 0 defines the\nsteepness, and µ ∈R is a similarity threshold. Intuitively,\nif an image has high similarity to a negative keyword, its\ncorresponding discount factor ψ(·) approaches d0, thereby\napplying a bounded penalty. For images with low similarity,\nψ(·) remains close to 1, imposing a negligible penalty.\nWe aggregate negative evidence by an element-wise\nproduct over all discount vectors. Let ⊙denote the element-\nwise product. The aggregate discount vector D is\n\nD =\nK\n\nk−∈K−\nψ\n\u0000\nS−(k−)\n\u0001\n, D ∈[d0, 1]n\n(6)\n\nFinally, the initial positive score S+ for each image is\nmultiplied by its aggregate discount factor D to compute\nthe final score S. The retriever outputs a ranked list of all\nimages, sorted in descending order based on S.\n\n3.3. User\n\nWe use an LLM to simulate a user answering questions\nduring training.\nSince our agent generates questions in\nan open-ended manner, a simple rule-based system for the\nsimulator would be insufficient. The simulator workflow\nis as follows: First, it receives qt and the complete at-\ntribute information of c∗, which is dynamically format-\nted into a prompt.\nFor instance, the prompt will con-\ntain the target’s data (e.g., {\"gender\":\n\"female\",\n\"hair color\":\n\"blonde\", ...}) and the specific\nquestion posed by the agent. The prompt also includes a\ndirective instructing the LLM to act as a truthful user, con-\nstraining its response to only: “Yes”, “No”, or “I can’t an-\nswer”, based strictly on the provided facts. This prompt is\nthen sent to the simulator, which generates an answer at.\n\n3.4. Reinforcement Learning\n\nOur goal is to develop a dialogue agent that can efficiently\nidentify a target item from a candidate set through a se-\nquence of clarifying questions. We formulate this task as\na finite Markov Decision Process (MDP), wherein a policy\nlearns to generate questions that maximally reduce uncer-\ntainty about a hidden target.\nLet C = {c1, c2, . . . , cn} be a finite set of N candidate\nitems. At the start of each dialogue episode, a single target\nitem c∗∈C is selected, which remains unknown to the\nagent. The agent’s goal is to identify c∗by engaging in a\nmulti-turn dialogue. The components of our RL framework\nare defined as follows:\nState. At turn t, the state st is represented by the dialogue\nhistory:\nst = ˜ht\n(7)\n\nwhere ˜ht is the history as defined in Eq. (4).\nAction. The action is the generation of the next question qt.\nThis question is sampled from the policy πθ(qt|st). In our\nsetting, the policy πθ is implemented as an LLM.\n\n4",
      "content_length": 5255,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "The augmented history replaces each standard answer\nwith its augmented counterpart:",
          "is_title": false,
          "page": 4,
          "text_length": 83
        },
        {
          "text": "˜ht =\n\b\n(q0, ˜a0), . . . , (qt−1, ˜at−1)\n.\n(4)",
          "is_title": false,
          "page": 4,
          "text_length": 46
        },
        {
          "text": "3.2. Retriever",
          "is_title": true,
          "page": 4,
          "text_length": 14
        },
        {
          "text": "The retriever serves as the interface between the dialogue\nand the underlying dataset. Its role is to interpret ht, update\nthe agent’s belief over candidates, and return the feedback\nto the questioner. The mechanism is similar to Retrieval-\nAugmented Generation (RAG). The concrete implementa-\ntion is domain-specific to handle the unique characteristics\nof each data modality.\nTabular Retriever. For structured tabular datasets, the re-\ntriever operates deterministic logical filtering that contracts\nthe hypothesis space at every turn. The agent’s question qt\nis first parsed to extract attribute–value pair(s). For instance,\nfrom the question “Is the target’s hair blonde?” the parser\nyields {hair color:\nblonde}. This can be imple-\nmented using either a rule-based system or a lightweight\nLLM. Given the attribute–value pair(s) and the user’s an-\nswer at, the current candidate set Ct is filtered. The retriever\noutputs a new, strictly smaller candidate set |Ct+1| < |Ct|.\nThe distribution of Ct+1 is provided as feedback to the ques-\ntioner. In addition, this pruned set is then used to calculate\nthe EIG for the reward function.\nImage Retriever. We formalize the image retriever as a\nsimilarity-based ranker that integrates positive and nega-\ntive attributes to handle negation robustly.\nThe retriever\nprojects the user’s queries and all candidate images I =\n{I1,\n. . . ,\nIn} into a shared embedding space and re-\nturns a similarity score per image. Vision–language models\n(VLMs) such as CLIP [33] perform well on general retrieval\ntasks but often degrade on queries with negation (e.g., “a\nman without glasses”). To address this limitation, we pro-\npose a multi-stage retrieval process that explicitly models\nboth positive and negative attributes.\nThe retriever first computes positive queries for every\nimage. As detailed in Algorithm 1, our process begins by\nparsing the dialogue history ht to obtain two sets of key-\nwords: positives K+ and negatives K−. We concatenate\npositive keywords into a single text query t+, and compute\nthe vector of positive similarities S+ ∈Rn, the i-th entry\ngives the CLIP similarity between t+ and image Ii.\nThe retriever then converts each negative keyword into\na vector of discounts D that penalize images similar to that\nkeyword. For every k−∈K−, we compute a similarity\nvector S−(k−) ∈Rn whose i-th entry is the CLIP similar-\nity between negative keyword k−and Ii. We transform this\nvector element-wise through a sigmoid gate σ(·) to obtain a\ndiscount factor ψ(S−(k−)) ∈[d0, 1]:",
          "is_title": false,
          "page": 4,
          "text_length": 2523
        },
        {
          "text": "ψ(x) = 1 −(1 −d0) · σ\n\u0000\nβ(x −µ)\n\u0001\n(5)",
          "is_title": false,
          "page": 4,
          "text_length": 37
        },
        {
          "text": "where d0 ∈(0, 1) is a base discount, β > 0 defines the\nsteepness, and µ ∈R is a similarity threshold. Intuitively,\nif an image has high similarity to a negative keyword, its\ncorresponding discount factor ψ(·) approaches d0, thereby\napplying a bounded penalty. For images with low similarity,\nψ(·) remains close to 1, imposing a negligible penalty.\nWe aggregate negative evidence by an element-wise\nproduct over all discount vectors. Let ⊙denote the element-\nwise product. The aggregate discount vector D is",
          "is_title": false,
          "page": 4,
          "text_length": 506
        },
        {
          "text": "D =\nK",
          "is_title": false,
          "page": 4,
          "text_length": 5
        },
        {
          "text": "k−∈K−\nψ\n\u0000\nS−(k−)\n\u0001\n, D ∈[d0, 1]n\n(6)",
          "is_title": false,
          "page": 4,
          "text_length": 36
        },
        {
          "text": "Finally, the initial positive score S+ for each image is\nmultiplied by its aggregate discount factor D to compute\nthe final score S. The retriever outputs a ranked list of all\nimages, sorted in descending order based on S.",
          "is_title": false,
          "page": 4,
          "text_length": 222
        },
        {
          "text": "3.3. User",
          "is_title": true,
          "page": 4,
          "text_length": 9
        },
        {
          "text": "We use an LLM to simulate a user answering questions\nduring training.\nSince our agent generates questions in\nan open-ended manner, a simple rule-based system for the\nsimulator would be insufficient. The simulator workflow\nis as follows: First, it receives qt and the complete at-\ntribute information of c∗, which is dynamically format-\nted into a prompt.\nFor instance, the prompt will con-\ntain the target’s data (e.g., {\"gender\":\n\"female\",\n\"hair color\":\n\"blonde\", ...}) and the specific\nquestion posed by the agent. The prompt also includes a\ndirective instructing the LLM to act as a truthful user, con-\nstraining its response to only: “Yes”, “No”, or “I can’t an-\nswer”, based strictly on the provided facts. This prompt is\nthen sent to the simulator, which generates an answer at.",
          "is_title": false,
          "page": 4,
          "text_length": 784
        },
        {
          "text": "3.4. Reinforcement Learning",
          "is_title": true,
          "page": 4,
          "text_length": 27
        },
        {
          "text": "Our goal is to develop a dialogue agent that can efficiently\nidentify a target item from a candidate set through a se-\nquence of clarifying questions. We formulate this task as\na finite Markov Decision Process (MDP), wherein a policy\nlearns to generate questions that maximally reduce uncer-\ntainty about a hidden target.\nLet C = {c1, c2, . . . , cn} be a finite set of N candidate\nitems. At the start of each dialogue episode, a single target\nitem c∗∈C is selected, which remains unknown to the\nagent. The agent’s goal is to identify c∗by engaging in a\nmulti-turn dialogue. The components of our RL framework\nare defined as follows:\nState. At turn t, the state st is represented by the dialogue\nhistory:\nst = ˜ht\n(7)",
          "is_title": false,
          "page": 4,
          "text_length": 717
        },
        {
          "text": "where ˜ht is the history as defined in Eq. (4).\nAction. The action is the generation of the next question qt.\nThis question is sampled from the policy πθ(qt|st). In our\nsetting, the policy πθ is implemented as an LLM.",
          "is_title": false,
          "page": 4,
          "text_length": 217
        },
        {
          "text": "4",
          "is_title": false,
          "page": 4,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 4,
      "page": 5,
      "content": "Environment. The environment consists of a user simu-\nlator and a retriever module, which interact with the agent\nto generate the dialogue trajectory. An episode terminates\nwhen the agent correctly identifies the target c∗or after a\nmaximum of Tmax turns. The environment is also responsi-\nble for storing dialogue trajectories and providing a reward\n(Sec. 3.5) for each completed trajectory.\nLearning Algorithm.\nTo train the agent, we employ\nGroup Relative Policy Optimization (GRPO) [39]. GRPO\nestimates the baseline using intra-group relative rewards,\nthereby avoiding the need for a separate value function\nmodel. This approach significantly reduces memory and\ncomputational resource consumption [39].\nDuring each\ntraining iteration, we sample i targets. For each target c∗,\nwe execute j stochastic rollouts, resulting in i × j trajecto-\nries, denoted as {τx,y}x=1..i, y=1..j. For each τ, the envi-\nronment provides a trajectory-level reward rτ. We then use\nthese collected trajectories to estimate the policy gradient\nand update the policy parameters θ. This iterative process\nrefines the agent’s questioning strategy, making it more ef-\nficient and strategic over time.\n\n3.5. Reward Functions\n\nThe reward function is a principal contributing factor affect-\ning the performance of RL agents and is also challenging to\ndesign [4]. Because GRPO optimizes trajectory-level re-\nwards, we assign each episode a single scalar reward that\nsummarizes (i) a success bonus, (ii) the average step score,\nand (iii) a step-length penalty. Formally, let T be the num-\nber of turns in the episode (1 ≤T ≤Tmax) and let ϵt\ndenote the step score at turn t. We define\n\n¯ϵ = 1\n\nT\n\nT\nX\n\nt=1\nϵt,\n(8)\n\nand the trajectory reward:\n\nR(τ) =\n\n(\nκ + ¯ϵ −Penalty(T),\nsuccess,\n−κ,\notherwise,\n(9)\n\nwhere κ > 0 is a success constant. The step-length penalty\nscales linearly with the episode length:\n\nPenalty(T) = α ·\nT\nTmax\n,\nα ≥0,\n(10)\n\nWe define the step score ϵt differently for tabular and\nimage settings.\nStep Score for Tabular Data. The tabular setting is a struc-\ntured, deterministic environment. Each candidate in the cur-\nrent pool is described by discrete attributes. Any attribute\nquery deterministically partitions the current candidate set\ninto two disjoint subsets, enabling an exact accounting of\nuncertainty reduction.\n\nTable 1. Comparison to baseline for Guess Number task and Guess\nWho task. Oracle is the optimal baseline.\n\nGuess Number Guess Who\n\nModel\nParameters SR↑\nMT↓\nSR↑MT↓\n\nOracle\n-\n1.0\n6.64\n1.0\n5.22\n\nDeepSeek-V3.1\n671B\n1.0\n8.02\n1.0\n6.18\n\nQwen2.5\n\n0.5B\n0.0\n16\n0.0\n16\n1.5B\n0.07\n15.61\n0.0\n16\n7B\n0.68\n12.43\n0.46 13.26\n32B\n0.92\n9.39\n0.56 11.44\n\nQwen2.5 + SFT\n7B\n0.88\n10.38\n0.48 11.66\n\nQwen2.5 + GRPO\n7B\n0.99\n7.62\n1.0\n6.25\n\nWe measure uncertainty using Shannon entropy [38].\nThis method is a widely accepted direct reward for IR [13,\n30, 47]. Let the unknown target be a random variable c sup-\nported on the current candidate set Ct = {c1t, . . . , cnt} at\nturn t, with belief pt(x) = Pr(c = x | st) given state st.\nThe conditional entropy is\n\nH(c | st) = −\nX\n\nx∈Ct\npt(x) log2 pt(x)\n(11)\n\nUnder a uniform belief, H(c | st) = log2 nt. Let at ∈\n{yes, no} the user’s answer. The EIG of question qt is:\n\nEIGt = H(c | st) −Eat|st\n\u0002\nH(c | st, at)\n\u0003\n(12)\n\nSince the answer deterministically partitions Ct into Cyes\nt\nand Cno\nt with sizes nyes\nt\n= |Cyes\nt | and nno\nt = |Cno\nt |, the answer\nprobabilities under a uniform prior are\n\npyes\nt\n= nyes\nt\nnt\n,\npno\nt = 1 −pyes\nt\n= nno\nt\nnt\n(13)\n\nThe resulting step score is\n\nϵt = EIGt = log2 nt −\n\u0010\npyes\nt\nlog2 nyes\nt\n+ pno\nt log2 nno\nt\n\u0011\n\n(14)\nϵt is maximized for a balanced split (pyes\nt\n≈0.5), achieving\nat most one bit of information per yes/no question.\nAdditionally, when a question fails the simulator-verified\nlegality check (i.e., it is not a well-formed, answerable En-\nglish interrogative), we bypass the EIG score and instead\nassign a soft penalty ϵt = ˆϵ with −0.5 < ˆϵ < 0.\nStep Score for Image Data. In the image setting, the se-\nmantic space is high-dimensional and unstructured, making\ndirect entropy estimation impractical. Instead, we track the\nagent’s progress via the rank of the ground-truth target un-\nder a dialogue-conditioned retriever. A successful question\nshould move the target closer to the top of the ranked list.\n\n5",
      "content_length": 4277,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "Environment. The environment consists of a user simu-\nlator and a retriever module, which interact with the agent\nto generate the dialogue trajectory. An episode terminates\nwhen the agent correctly identifies the target c∗or after a\nmaximum of Tmax turns. The environment is also responsi-\nble for storing dialogue trajectories and providing a reward\n(Sec. 3.5) for each completed trajectory.\nLearning Algorithm.\nTo train the agent, we employ\nGroup Relative Policy Optimization (GRPO) [39]. GRPO\nestimates the baseline using intra-group relative rewards,\nthereby avoiding the need for a separate value function\nmodel. This approach significantly reduces memory and\ncomputational resource consumption [39].\nDuring each\ntraining iteration, we sample i targets. For each target c∗,\nwe execute j stochastic rollouts, resulting in i × j trajecto-\nries, denoted as {τx,y}x=1..i, y=1..j. For each τ, the envi-\nronment provides a trajectory-level reward rτ. We then use\nthese collected trajectories to estimate the policy gradient\nand update the policy parameters θ. This iterative process\nrefines the agent’s questioning strategy, making it more ef-\nficient and strategic over time.",
          "is_title": false,
          "page": 5,
          "text_length": 1175
        },
        {
          "text": "3.5. Reward Functions",
          "is_title": true,
          "page": 5,
          "text_length": 21
        },
        {
          "text": "The reward function is a principal contributing factor affect-\ning the performance of RL agents and is also challenging to\ndesign [4]. Because GRPO optimizes trajectory-level re-\nwards, we assign each episode a single scalar reward that\nsummarizes (i) a success bonus, (ii) the average step score,\nand (iii) a step-length penalty. Formally, let T be the num-\nber of turns in the episode (1 ≤T ≤Tmax) and let ϵt\ndenote the step score at turn t. We define",
          "is_title": false,
          "page": 5,
          "text_length": 453
        },
        {
          "text": "¯ϵ = 1",
          "is_title": false,
          "page": 5,
          "text_length": 6
        },
        {
          "text": "T",
          "is_title": false,
          "page": 5,
          "text_length": 1
        },
        {
          "text": "T\nX",
          "is_title": false,
          "page": 5,
          "text_length": 3
        },
        {
          "text": "t=1\nϵt,\n(8)",
          "is_title": false,
          "page": 5,
          "text_length": 11
        },
        {
          "text": "and the trajectory reward:",
          "is_title": false,
          "page": 5,
          "text_length": 26
        },
        {
          "text": "R(τ) =",
          "is_title": false,
          "page": 5,
          "text_length": 6
        },
        {
          "text": "(\nκ + ¯ϵ −Penalty(T),\nsuccess,\n−κ,\notherwise,\n(9)",
          "is_title": false,
          "page": 5,
          "text_length": 49
        },
        {
          "text": "where κ > 0 is a success constant. The step-length penalty\nscales linearly with the episode length:",
          "is_title": false,
          "page": 5,
          "text_length": 99
        },
        {
          "text": "Penalty(T) = α ·\nT\nTmax\n,\nα ≥0,\n(10)",
          "is_title": false,
          "page": 5,
          "text_length": 36
        },
        {
          "text": "We define the step score ϵt differently for tabular and\nimage settings.\nStep Score for Tabular Data. The tabular setting is a struc-\ntured, deterministic environment. Each candidate in the cur-\nrent pool is described by discrete attributes. Any attribute\nquery deterministically partitions the current candidate set\ninto two disjoint subsets, enabling an exact accounting of\nuncertainty reduction.",
          "is_title": false,
          "page": 5,
          "text_length": 397
        },
        {
          "text": "Table 1. Comparison to baseline for Guess Number task and Guess\nWho task. Oracle is the optimal baseline.",
          "is_title": false,
          "page": 5,
          "text_length": 105
        },
        {
          "text": "Guess Number Guess Who",
          "is_title": false,
          "page": 5,
          "text_length": 22
        },
        {
          "text": "Model\nParameters SR↑\nMT↓\nSR↑MT↓",
          "is_title": false,
          "page": 5,
          "text_length": 31
        },
        {
          "text": "Oracle\n-\n1.0\n6.64\n1.0\n5.22",
          "is_title": false,
          "page": 5,
          "text_length": 26
        },
        {
          "text": "DeepSeek-V3.1\n671B\n1.0\n8.02\n1.0\n6.18",
          "is_title": false,
          "page": 5,
          "text_length": 36
        },
        {
          "text": "Qwen2.5",
          "is_title": false,
          "page": 5,
          "text_length": 7
        },
        {
          "text": "0.5B\n0.0\n16\n0.0\n16\n1.5B\n0.07\n15.61\n0.0\n16\n7B\n0.68\n12.43\n0.46 13.26\n32B\n0.92\n9.39\n0.56 11.44",
          "is_title": false,
          "page": 5,
          "text_length": 91
        },
        {
          "text": "Qwen2.5 + SFT\n7B\n0.88\n10.38\n0.48 11.66",
          "is_title": false,
          "page": 5,
          "text_length": 38
        },
        {
          "text": "Qwen2.5 + GRPO\n7B\n0.99\n7.62\n1.0\n6.25",
          "is_title": false,
          "page": 5,
          "text_length": 36
        },
        {
          "text": "We measure uncertainty using Shannon entropy [38].\nThis method is a widely accepted direct reward for IR [13,\n30, 47]. Let the unknown target be a random variable c sup-\nported on the current candidate set Ct = {c1t, . . . , cnt} at\nturn t, with belief pt(x) = Pr(c = x | st) given state st.\nThe conditional entropy is",
          "is_title": false,
          "page": 5,
          "text_length": 318
        },
        {
          "text": "H(c | st) = −\nX",
          "is_title": false,
          "page": 5,
          "text_length": 15
        },
        {
          "text": "x∈Ct\npt(x) log2 pt(x)\n(11)",
          "is_title": false,
          "page": 5,
          "text_length": 26
        },
        {
          "text": "Under a uniform belief, H(c | st) = log2 nt. Let at ∈\n{yes, no} the user’s answer. The EIG of question qt is:",
          "is_title": false,
          "page": 5,
          "text_length": 109
        },
        {
          "text": "EIGt = H(c | st) −Eat|st\n\u0002\nH(c | st, at)\n\u0003\n(12)",
          "is_title": false,
          "page": 5,
          "text_length": 47
        },
        {
          "text": "Since the answer deterministically partitions Ct into Cyes\nt\nand Cno\nt with sizes nyes\nt\n= |Cyes\nt | and nno\nt = |Cno\nt |, the answer\nprobabilities under a uniform prior are",
          "is_title": false,
          "page": 5,
          "text_length": 173
        },
        {
          "text": "pyes\nt\n= nyes\nt\nnt\n,\npno\nt = 1 −pyes\nt\n= nno\nt\nnt\n(13)",
          "is_title": false,
          "page": 5,
          "text_length": 54
        },
        {
          "text": "The resulting step score is",
          "is_title": false,
          "page": 5,
          "text_length": 27
        },
        {
          "text": "ϵt = EIGt = log2 nt −\n\u0010\npyes\nt\nlog2 nyes\nt\n+ pno\nt log2 nno\nt\n\u0011",
          "is_title": false,
          "page": 5,
          "text_length": 63
        },
        {
          "text": "(14)\nϵt is maximized for a balanced split (pyes\nt\n≈0.5), achieving\nat most one bit of information per yes/no question.\nAdditionally, when a question fails the simulator-verified\nlegality check (i.e., it is not a well-formed, answerable En-\nglish interrogative), we bypass the EIG score and instead\nassign a soft penalty ϵt = ˆϵ with −0.5 < ˆϵ < 0.\nStep Score for Image Data. In the image setting, the se-\nmantic space is high-dimensional and unstructured, making\ndirect entropy estimation impractical. Instead, we track the\nagent’s progress via the rank of the ground-truth target un-\nder a dialogue-conditioned retriever. A successful question\nshould move the target closer to the top of the ranked list.",
          "is_title": false,
          "page": 5,
          "text_length": 705
        },
        {
          "text": "5",
          "is_title": false,
          "page": 5,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 5,
      "page": 6,
      "content": "Table 2. Comparison to baseline for CelebA image retrieval task.\n\nModel\nParameters\nData Size\nSR/R@5↑\nMT↓\nMedR↓\nMR↓\n\nDeepSeek-V3.1\n671B\n100 / 500\n0.61 / 0.36\n12.82 / 19.96\n5.0 / 22.5\n14.24 / 53.79\n\nQwen2.5\n1.5B\n100 / 500\n0.15 / 0.08\n17.56 / 23.52\n45.5 / 196.0\n44.38 / 217.1\n7B\n100 / 500\n0.36 / 0.23\n15.33 / 21.52\n17.0 / 65.0\n25.75 / 115.30\n32B\n100 / 500\n0.46 / 0.24\n13.90 / 20.89\n8.5 / 49.5\n20.80 / 89.42\n\nQwen2.5 + GRPO\n7B\n100 / 500\n0.90 / 0.69\n6.15 / 12.21\n3.5 / 5.0\n6.35 / 45.11\n\nGiven ht at turn t, the retriever produces similarity scores\nfor each candidate image and induces a total ordering Φt by\nsorting candidates in descending similarity. We focus on the\nrank of the true target c⋆under this ordering:\n\nΦt ∈{1, 2, . . . , n},\nϕt = rankΦt(c⋆),\n(15)\n\nWe measure per-turn improvement by the change in log-\nrank:\nϵt = ∆t = log ϕt−1 −log ϕt\n(16)\n\nwhere ϵt > 0, when the rank improves, ϵt = 0, when it\nis unchanged, and ϵt < 0, when it worsens. Similar to the\ntabular setting, we assign a soft penalty as a step score when\nan illegal question is generated.\n\n4. Experiments\n\nIn this section, we conduct extensive experiments on two\ndata modalities. We first introduce the datasets, and then\npresent the experimental results and analysis.\n\n4.1. Datasets\n\nWe evaluate SherlockLLM on two data modalities across\nthree tasks. In all settings, an episode ends when the target\nis identified or the turn budget is exhausted.\nGuess Number. This is a logical deduction task on a tabular\ndataset. In each game, a set of 100 consecutive integers is\nrandomly chosen from a range of [0, 1000], and a single\nnumber is selected as the target.\nGuess Who. Guess Who is a multi-attribute reasoning task\non a tabular dataset. We use a dataset of 36 unique char-\nacters, each defined by 9 distinct attributes (more details in\nSec. C).\nCelebA Image Retrieval. This is a semantic retrieval task\non an image dataset. CelebA [25] is a celebrity face at-\ntributes dataset containing 202,599 images annotated with\n40 binary attributes. We remove blurred images and form\ntwo sets of 100 and 500 images to assess performance un-\nder small and larger search spaces. To create rich textual\ncaptions, we first use the binary attributes to create base\ncaptions and then rewrite them with GPT-4o [14] to im-\nprove fluency while preserving the encoded attributes. An\nepisode is counted as successful when the target image is\nranked within the top 5 (rank ≤5) within Tmax rounds.\n\n4.2. Implementation Details\n\nModel. We use Qwen2.5-7B-Instruct [42] as the question\npolicy.\nTo accelerate training and reduce computational\noverhead, we use a 4-bit quantized version of the model.\nUser Simulator. We use DeepSeek-V3.1 [24] to serve as\nthe user simulator, responsible for answering questions.\nRetriever. For our retriever, we set the hyperparameters\nin Eq. (5) to µ = 0.15, β = 20, and d0 = 0.9. These values\nwere empirically determined to yield optimal performance.\nPolicy Learning. We fine-tune Qwen2.5-7B-Instruct using\nLoRA [11] with a rank of 16 and an alpha of 32. The model\nis trained with the AdamW [26] optimizer with a learning\nrate of 2e−6. For the reward function, we set κ to 2 and\nthe step penalty coefficient to 0.7. We generate 25 dialogue\ntrajectories for each possible target within every task. The\nmaximum number of turns Tmax is task-dependent: for tab-\nular tasks, Tmax is 16. For image retrieval tasks, Tmax is set\nto 20 for a dataset of 100 images and 25 for 500 images. All\ntasks are trained for 120 steps, except for the 500-image re-\ntrieval task, which is trained for 200 steps. For each task,\nwe select the model that achieves the highest reward.\n\n4.3. Baselines\n\nState-of-the-Art LLM (Zero-Shot).\nWe compare Sher-\nlockLLM against DeepSeek-V3.1 (671B parameters). This\nmodel is prompted in a zero-shot manner, using the same\nprompt and feedback format as our agent. For a fair com-\nparison to other models, we do not use the “thinking mode”.\nFoundation Models (Zero-Shot). We also compare Sher-\nlockLLM against its underlying models without any train-\ning in a zero-shot setting. We evaluate pretrained Qwen2.5-\nInstruct models of varying scales (0.5B, 1.5B, 7B and 32B).\nSupervised Fine-Tuning Method. We include a baseline\ndeveloped using Supervised Fine-Tuning (SFT) on tabular\ntasks. It directly trains a model to imitate expert behavior\nby using a synthetic dialogue dataset. Training details are\nprovided in Section D.1 and Section D.2.\nOracle.\nWe define an Oracle baseline to represent the\ntheoretical upper bound performance of our tabular tasks.\nThe Oracle simulates a perfect decision-making process.\nAt each round, it exhaustively searches the entire pool of\nsingle-attribute questions and selects the one that yields the\n\n6",
      "content_length": 4725,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "Table 2. Comparison to baseline for CelebA image retrieval task.",
          "is_title": false,
          "page": 6,
          "text_length": 64
        },
        {
          "text": "Model\nParameters\nData Size\nSR/R@5↑\nMT↓\nMedR↓\nMR↓",
          "is_title": false,
          "page": 6,
          "text_length": 48
        },
        {
          "text": "DeepSeek-V3.1\n671B\n100 / 500\n0.61 / 0.36\n12.82 / 19.96\n5.0 / 22.5\n14.24 / 53.79",
          "is_title": false,
          "page": 6,
          "text_length": 79
        },
        {
          "text": "Qwen2.5\n1.5B\n100 / 500\n0.15 / 0.08\n17.56 / 23.52\n45.5 / 196.0\n44.38 / 217.1\n7B\n100 / 500\n0.36 / 0.23\n15.33 / 21.52\n17.0 / 65.0\n25.75 / 115.30\n32B\n100 / 500\n0.46 / 0.24\n13.90 / 20.89\n8.5 / 49.5\n20.80 / 89.42",
          "is_title": false,
          "page": 6,
          "text_length": 206
        },
        {
          "text": "Qwen2.5 + GRPO\n7B\n100 / 500\n0.90 / 0.69\n6.15 / 12.21\n3.5 / 5.0\n6.35 / 45.11",
          "is_title": false,
          "page": 6,
          "text_length": 75
        },
        {
          "text": "Given ht at turn t, the retriever produces similarity scores\nfor each candidate image and induces a total ordering Φt by\nsorting candidates in descending similarity. We focus on the\nrank of the true target c⋆under this ordering:",
          "is_title": false,
          "page": 6,
          "text_length": 228
        },
        {
          "text": "Φt ∈{1, 2, . . . , n},\nϕt = rankΦt(c⋆),\n(15)",
          "is_title": false,
          "page": 6,
          "text_length": 44
        },
        {
          "text": "We measure per-turn improvement by the change in log-\nrank:\nϵt = ∆t = log ϕt−1 −log ϕt\n(16)",
          "is_title": false,
          "page": 6,
          "text_length": 91
        },
        {
          "text": "where ϵt > 0, when the rank improves, ϵt = 0, when it\nis unchanged, and ϵt < 0, when it worsens. Similar to the\ntabular setting, we assign a soft penalty as a step score when\nan illegal question is generated.",
          "is_title": false,
          "page": 6,
          "text_length": 208
        },
        {
          "text": "4. Experiments",
          "is_title": true,
          "page": 6,
          "text_length": 14
        },
        {
          "text": "In this section, we conduct extensive experiments on two\ndata modalities. We first introduce the datasets, and then\npresent the experimental results and analysis.",
          "is_title": false,
          "page": 6,
          "text_length": 162
        },
        {
          "text": "4.1. Datasets",
          "is_title": true,
          "page": 6,
          "text_length": 13
        },
        {
          "text": "We evaluate SherlockLLM on two data modalities across\nthree tasks. In all settings, an episode ends when the target\nis identified or the turn budget is exhausted.\nGuess Number. This is a logical deduction task on a tabular\ndataset. In each game, a set of 100 consecutive integers is\nrandomly chosen from a range of [0, 1000], and a single\nnumber is selected as the target.\nGuess Who. Guess Who is a multi-attribute reasoning task\non a tabular dataset. We use a dataset of 36 unique char-\nacters, each defined by 9 distinct attributes (more details in\nSec. C).\nCelebA Image Retrieval. This is a semantic retrieval task\non an image dataset. CelebA [25] is a celebrity face at-\ntributes dataset containing 202,599 images annotated with\n40 binary attributes. We remove blurred images and form\ntwo sets of 100 and 500 images to assess performance un-\nder small and larger search spaces. To create rich textual\ncaptions, we first use the binary attributes to create base\ncaptions and then rewrite them with GPT-4o [14] to im-\nprove fluency while preserving the encoded attributes. An\nepisode is counted as successful when the target image is\nranked within the top 5 (rank ≤5) within Tmax rounds.",
          "is_title": false,
          "page": 6,
          "text_length": 1189
        },
        {
          "text": "4.2. Implementation Details",
          "is_title": true,
          "page": 6,
          "text_length": 27
        },
        {
          "text": "Model. We use Qwen2.5-7B-Instruct [42] as the question\npolicy.\nTo accelerate training and reduce computational\noverhead, we use a 4-bit quantized version of the model.\nUser Simulator. We use DeepSeek-V3.1 [24] to serve as\nthe user simulator, responsible for answering questions.\nRetriever. For our retriever, we set the hyperparameters\nin Eq. (5) to µ = 0.15, β = 20, and d0 = 0.9. These values\nwere empirically determined to yield optimal performance.\nPolicy Learning. We fine-tune Qwen2.5-7B-Instruct using\nLoRA [11] with a rank of 16 and an alpha of 32. The model\nis trained with the AdamW [26] optimizer with a learning\nrate of 2e−6. For the reward function, we set κ to 2 and\nthe step penalty coefficient to 0.7. We generate 25 dialogue\ntrajectories for each possible target within every task. The\nmaximum number of turns Tmax is task-dependent: for tab-\nular tasks, Tmax is 16. For image retrieval tasks, Tmax is set\nto 20 for a dataset of 100 images and 25 for 500 images. All\ntasks are trained for 120 steps, except for the 500-image re-\ntrieval task, which is trained for 200 steps. For each task,\nwe select the model that achieves the highest reward.",
          "is_title": false,
          "page": 6,
          "text_length": 1160
        },
        {
          "text": "4.3. Baselines",
          "is_title": true,
          "page": 6,
          "text_length": 14
        },
        {
          "text": "State-of-the-Art LLM (Zero-Shot).\nWe compare Sher-\nlockLLM against DeepSeek-V3.1 (671B parameters). This\nmodel is prompted in a zero-shot manner, using the same\nprompt and feedback format as our agent. For a fair com-\nparison to other models, we do not use the “thinking mode”.\nFoundation Models (Zero-Shot). We also compare Sher-\nlockLLM against its underlying models without any train-\ning in a zero-shot setting. We evaluate pretrained Qwen2.5-\nInstruct models of varying scales (0.5B, 1.5B, 7B and 32B).\nSupervised Fine-Tuning Method. We include a baseline\ndeveloped using Supervised Fine-Tuning (SFT) on tabular\ntasks. It directly trains a model to imitate expert behavior\nby using a synthetic dialogue dataset. Training details are\nprovided in Section D.1 and Section D.2.\nOracle.\nWe define an Oracle baseline to represent the\ntheoretical upper bound performance of our tabular tasks.\nThe Oracle simulates a perfect decision-making process.\nAt each round, it exhaustively searches the entire pool of\nsingle-attribute questions and selects the one that yields the",
          "is_title": false,
          "page": 6,
          "text_length": 1068
        },
        {
          "text": "6",
          "is_title": false,
          "page": 6,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 6,
      "page": 7,
      "content": "Task: find 143 from [86, 87, …, 184, 185]\n\nNo. Question\nAnswer\nEIG\n1\nIs the target number less than or equal to 135?\nNo\n1.0\n2\nIs the target number less than or equal to 160?\nYes\n1.0\n3\nIs the target number less than or equal to 147?\nYes\n1.0\n4\nIs the target number less than or equal to 141?\nNo\n1.0\n5\nIs the target number less than or equal to 144?\nYes\n1.0\n6\nIs the target number 144?\nNo\n0.92\n7\nIs the target number 143?\nYes\n1.0\n(a) An example of the Guess Number task.\n\nTask: find C33 among 36 characters.\n\nNo. Question\nAnswer\nEIG\n1\nIs the target candidate a male?\nYes\n1.0\n2\nIs the target candidate's hair color blonde?\nNo\n0.85\n3\nIs the target candidate's hairstyle bald?\nNo\n1.0\n4\nIs the target candidate's hair color red?\nNo\n0.92\n5\nIs the target candidate's hair color black?\nNo\n1.0\n\n6\nIs the target candidate's hair color white and \nlong hairstyle?\nNo\n1.0\n\n7\nIs the target candidate C33?\nYes\n0.0\n\n(b) An example of the Guess Who task.\n\nFigure 3. Examples from the Guess Number and Guess Who\ntasks. (a) illustrates the agent’s strategy in Guess Number, while\n(b) shows its approach in Guess Who.\n\nmaximum EIG. This question is the most informative one,\nand ideally, it halves the remaining candidate set. This pro-\ncess continues until the target’s probability becomes 1.\n\n4.4. Metrics\n\nFor all tasks, we report the Success Rate (SR), which is\nthe percentage of dialogues that successfully meet the task-\nspecific completion criteria, and the Mean Turns (MT),\nwhich measures the mean number of turns for all dialogues.\nFor the CelebA Image Retrieval task, we also report com-\nmon retrieval metrics: Recall@K (R@K) for K=5, repre-\nsenting the percentage of trials where the target is ranked\nwithin the top 5. By this definition, R@5 is equivalent to\nSR. Finally, we report the Median Rank (MedR) and the\nMean Rank (MR) of the target image across all trials.\n\n4.5. Results and Analysis\n\nTabular Retrieval Tasks.\nTable 1 presents a per-\nformance comparison of SherlockLLM (Qwen2.5-7B +\nGRPO) against baseline models on tabular tasks. On Guess\nNumber, the Oracle achieves a perfect SR in an optimal\nnumber of turns.\nSherlockLLM, while maintaining an\nalmost perfect SR, yields an MT of 7.6.\nCompared to\nDeepSeek, SherlockLLM reduces MT by 5.0%.\nOn the\nGuess Who task, both SherlockLLM and DeepSeek attain a\n100% SR. Our policy requires an average of 6.25 turns per\ndialogue, a result that is on par with DeepSeek and one step\n\nTarget\nNo. Question\nAnswer\nRank\nEIG EIG Rank\n1\nIs the target a man?\nNo\n53\n0.98\n1\n2\nDoes the target have bangs?\nNo\n10\n0.88\n2\n3\nDoes the target have earrings?\nYes\n3\n0.92\n1\n\n(a) An example of progressive refinement in the CelebA image retrieval\ntask, where each question improves the target’s rank.\n\nTarget\nNo. Question\nAnswer\nRank\nEIG\nEIG Rank\n1\nIs the target a man?\nYes\n25\n0.98\n1\n2\nDoes the target have a goatee?\nNo\n27\n0.97\n2\n3\nDoes the target have black hair?\nNo\n32\n0.54\n4\n4\nDoes the target have brown hair?\nYes\n7\n0.76\n3\n5\nDoes the target wear eyeglasses?\nYes\n3\n0.72\n2\n\n(b) A more challenging retrieval task. The agent demonstrates resilience\nby recovering from an initial rank degradation caused by noisy similarity.\n\nFigure 4. Examples from the image retrieval task. For each turn,\nwe show the target’s rank, EIG of the chosen question, and its EIG\nRank among all possible questions for the top-10 candidates.\n\nmore than the Oracle. The Qwen2.5 models without fine-\ntuning underperform. The smallest variants (0.5B and 1.5B)\nfail to complete the tasks, indicating insufficient reasoning\nand instruction-following ability at this scale. Even the 32B\nvariant stops at SR = 0.92/0.56 with MT = 9.39/11.44 on the\ntwo tasks. We compare the Qwen2.5-7B backbone against\nits SFT and GRPO counterparts. On Guess Number, SFT\nmethod increases SR by 29% and reduces MT by 16.5%.\nGRPO-tuned agent improves SR by 45.6% and reduces MT\nby 38.7%. On Guess Who, SFT provides minimal improve-\nment to the baseline model’s SR and MT, showing improve-\nments of 5.0% and 12.1%, respectively.\nFor the GRPO\nmethod, improvements are even more striking: SR more\nthan doubles from 0.46 to 1.0, with MT dropping by 52.9%.\nCelebA Image Retrieval.\nSherlockLLM achieves state-\nof-the-art performance on the CelebA image retrieval task,\noutperforming all baseline models (Tab. 2). Compared to\nDeepSeek-V3.1, SherlockLLM shows a substantial advan-\ntage in dialogue efficiency, reducing MT by approximately\n52% and 38.8% on the two test sets. In addition, its SR is\nsignificantly higher, showing improvements of 47.5% and\n91.7% over DeepSeek-V3.1. Compared to the Qwen2.5-7B\nbase model, the GRPO training yields remarkable gains: SR\nincreases by 150% and 200% on the 100- and 500-image\nsets, respectively. The training also improves ranking per-\nformance, with MedR decreasing by 79.4% and 92.3%, and\nMR decreasing by 75.3% and 60.9%. Concurrently, dia-\nlogue efficiency is enhanced, as evidenced by a reduction in\nMT of 59.9% and 43.3% on the respective sets.\nQualitative examples. In Guess Number and Guess Who\n(Fig. 3), SherlockLLM learns near-optimal questioning\nstrategies.\nFor instance, in Guess Number, its approach\nis analogous to a binary search, consistently halving the\ncandidate space to achieve a maximal EIG of 1.0. Sim-\nilarly, in Guess Who, SherlockLLM learns to prioritize\n\n7",
      "content_length": 5282,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "Task: find 143 from [86, 87, …, 184, 185]",
          "is_title": false,
          "page": 7,
          "text_length": 41
        },
        {
          "text": "No. Question\nAnswer\nEIG\n1\nIs the target number less than or equal to 135?\nNo\n1.0\n2\nIs the target number less than or equal to 160?\nYes\n1.0\n3\nIs the target number less than or equal to 147?\nYes\n1.0\n4\nIs the target number less than or equal to 141?\nNo\n1.0\n5\nIs the target number less than or equal to 144?\nYes\n1.0\n6\nIs the target number 144?\nNo\n0.92\n7\nIs the target number 143?\nYes\n1.0\n(a) An example of the Guess Number task.",
          "is_title": false,
          "page": 7,
          "text_length": 424
        },
        {
          "text": "Task: find C33 among 36 characters.",
          "is_title": false,
          "page": 7,
          "text_length": 35
        },
        {
          "text": "No. Question\nAnswer\nEIG\n1\nIs the target candidate a male?\nYes\n1.0\n2\nIs the target candidate's hair color blonde?\nNo\n0.85\n3\nIs the target candidate's hairstyle bald?\nNo\n1.0\n4\nIs the target candidate's hair color red?\nNo\n0.92\n5\nIs the target candidate's hair color black?\nNo\n1.0",
          "is_title": false,
          "page": 7,
          "text_length": 276
        },
        {
          "text": "6\nIs the target candidate's hair color white and \nlong hairstyle?\nNo\n1.0",
          "is_title": true,
          "page": 7,
          "text_length": 72
        },
        {
          "text": "7\nIs the target candidate C33?\nYes\n0.0",
          "is_title": true,
          "page": 7,
          "text_length": 38
        },
        {
          "text": "(b) An example of the Guess Who task.",
          "is_title": false,
          "page": 7,
          "text_length": 37
        },
        {
          "text": "Figure 3. Examples from the Guess Number and Guess Who\ntasks. (a) illustrates the agent’s strategy in Guess Number, while\n(b) shows its approach in Guess Who.",
          "is_title": false,
          "page": 7,
          "text_length": 158
        },
        {
          "text": "maximum EIG. This question is the most informative one,\nand ideally, it halves the remaining candidate set. This pro-\ncess continues until the target’s probability becomes 1.",
          "is_title": false,
          "page": 7,
          "text_length": 174
        },
        {
          "text": "4.4. Metrics",
          "is_title": true,
          "page": 7,
          "text_length": 12
        },
        {
          "text": "For all tasks, we report the Success Rate (SR), which is\nthe percentage of dialogues that successfully meet the task-\nspecific completion criteria, and the Mean Turns (MT),\nwhich measures the mean number of turns for all dialogues.\nFor the CelebA Image Retrieval task, we also report com-\nmon retrieval metrics: Recall@K (R@K) for K=5, repre-\nsenting the percentage of trials where the target is ranked\nwithin the top 5. By this definition, R@5 is equivalent to\nSR. Finally, we report the Median Rank (MedR) and the\nMean Rank (MR) of the target image across all trials.",
          "is_title": false,
          "page": 7,
          "text_length": 569
        },
        {
          "text": "4.5. Results and Analysis",
          "is_title": true,
          "page": 7,
          "text_length": 25
        },
        {
          "text": "Tabular Retrieval Tasks.\nTable 1 presents a per-\nformance comparison of SherlockLLM (Qwen2.5-7B +\nGRPO) against baseline models on tabular tasks. On Guess\nNumber, the Oracle achieves a perfect SR in an optimal\nnumber of turns.\nSherlockLLM, while maintaining an\nalmost perfect SR, yields an MT of 7.6.\nCompared to\nDeepSeek, SherlockLLM reduces MT by 5.0%.\nOn the\nGuess Who task, both SherlockLLM and DeepSeek attain a\n100% SR. Our policy requires an average of 6.25 turns per\ndialogue, a result that is on par with DeepSeek and one step",
          "is_title": false,
          "page": 7,
          "text_length": 535
        },
        {
          "text": "Target\nNo. Question\nAnswer\nRank\nEIG EIG Rank\n1\nIs the target a man?\nNo\n53\n0.98\n1\n2\nDoes the target have bangs?\nNo\n10\n0.88\n2\n3\nDoes the target have earrings?\nYes\n3\n0.92\n1",
          "is_title": false,
          "page": 7,
          "text_length": 169
        },
        {
          "text": "(a) An example of progressive refinement in the CelebA image retrieval\ntask, where each question improves the target’s rank.",
          "is_title": false,
          "page": 7,
          "text_length": 124
        },
        {
          "text": "Target\nNo. Question\nAnswer\nRank\nEIG\nEIG Rank\n1\nIs the target a man?\nYes\n25\n0.98\n1\n2\nDoes the target have a goatee?\nNo\n27\n0.97\n2\n3\nDoes the target have black hair?\nNo\n32\n0.54\n4\n4\nDoes the target have brown hair?\nYes\n7\n0.76\n3\n5\nDoes the target wear eyeglasses?\nYes\n3\n0.72\n2",
          "is_title": false,
          "page": 7,
          "text_length": 271
        },
        {
          "text": "(b) A more challenging retrieval task. The agent demonstrates resilience\nby recovering from an initial rank degradation caused by noisy similarity.",
          "is_title": false,
          "page": 7,
          "text_length": 147
        },
        {
          "text": "Figure 4. Examples from the image retrieval task. For each turn,\nwe show the target’s rank, EIG of the chosen question, and its EIG\nRank among all possible questions for the top-10 candidates.",
          "is_title": false,
          "page": 7,
          "text_length": 192
        },
        {
          "text": "more than the Oracle. The Qwen2.5 models without fine-\ntuning underperform. The smallest variants (0.5B and 1.5B)\nfail to complete the tasks, indicating insufficient reasoning\nand instruction-following ability at this scale. Even the 32B\nvariant stops at SR = 0.92/0.56 with MT = 9.39/11.44 on the\ntwo tasks. We compare the Qwen2.5-7B backbone against\nits SFT and GRPO counterparts. On Guess Number, SFT\nmethod increases SR by 29% and reduces MT by 16.5%.\nGRPO-tuned agent improves SR by 45.6% and reduces MT\nby 38.7%. On Guess Who, SFT provides minimal improve-\nment to the baseline model’s SR and MT, showing improve-\nments of 5.0% and 12.1%, respectively.\nFor the GRPO\nmethod, improvements are even more striking: SR more\nthan doubles from 0.46 to 1.0, with MT dropping by 52.9%.\nCelebA Image Retrieval.\nSherlockLLM achieves state-\nof-the-art performance on the CelebA image retrieval task,\noutperforming all baseline models (Tab. 2). Compared to\nDeepSeek-V3.1, SherlockLLM shows a substantial advan-\ntage in dialogue efficiency, reducing MT by approximately\n52% and 38.8% on the two test sets. In addition, its SR is\nsignificantly higher, showing improvements of 47.5% and\n91.7% over DeepSeek-V3.1. Compared to the Qwen2.5-7B\nbase model, the GRPO training yields remarkable gains: SR\nincreases by 150% and 200% on the 100- and 500-image\nsets, respectively. The training also improves ranking per-\nformance, with MedR decreasing by 79.4% and 92.3%, and\nMR decreasing by 75.3% and 60.9%. Concurrently, dia-\nlogue efficiency is enhanced, as evidenced by a reduction in\nMT of 59.9% and 43.3% on the respective sets.\nQualitative examples. In Guess Number and Guess Who\n(Fig. 3), SherlockLLM learns near-optimal questioning\nstrategies.\nFor instance, in Guess Number, its approach\nis analogous to a binary search, consistently halving the\ncandidate space to achieve a maximal EIG of 1.0. Sim-\nilarly, in Guess Who, SherlockLLM learns to prioritize",
          "is_title": false,
          "page": 7,
          "text_length": 1944
        },
        {
          "text": "7",
          "is_title": false,
          "page": 7,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 7,
      "page": 8,
      "content": "Table 3. Ablation study on different reward functions.\n\nReward Function\nSR↑\nMT↓\n\nEIG Only\n0.99\n7.55\nStep Penalty Only\n0.99\n6.19\nEIG + Step Penalty\n1.0\n6.25\n\nbroad, discriminative questions about high-level attributes\nsuch as gender.\nThese cases demonstrate that in well-\ndefined problem spaces, our framework effectively guides\nthe agent toward mathematically efficient policies. In the\nimage retrieval, Figure 4a shows a progressive refinement,\nwhere each subsequent question steadily improves the tar-\nget’s rank. Fig. 4b reveals SherlockLLM’s ability to handle\nnon-monotonic progression. In this challenging example, a\nnegative answer temporarily worsens the target’s rank from\n25 to 32.\nThis is a likely consequence of the complex-\nities within the visual-semantic embedding space. How-\never, SherlockLLM recovers from this setback, ultimately\nimproving the target’s rank to 3. This self-correction capa-\nbility is a key advantage of our interactive approach over\ntraditional one-shot query retrieval systems.\nAblation Study on Reward Components. To ascertain\nthe individual contribution of each component within our\nreward function (Sec. 3.5), we conduct an ablation study\nfor the Guess Who task. We use three experimental con-\nditions: 1) an agent rewarded solely based on EIG, 2) an\nagent guided by a terminal reward and a step penalty, and\n3) an agent trained with the complete reward function. The\nsuccess-contingent constant term is retained across all con-\nfigurations. The results are presented in Tab. 3. Sherlock-\nLLM trained with only the EIG reward learns to ask highly\ninformative questions. However, this leads to a deliberate\nbut slow strategy. SherlockLLM guided exclusively by a\nstep penalty becomes highly efficient (6.19 MT), but opti-\nmized purely for speed, may not always be the most robust\napproach. The combination of both components in the re-\nward yields a more robust policy. This agent achieves a\nperfect success rate while maintaining a highly efficient av-\nerage of 6.25 turns. The per-step guidance from the EIG re-\nward teaches the agent what constitutes an effective query,\nwhile the global step penalty incentivizes it to reach the goal\nexpeditiously. This combination compels the agent to learn\na strategy that is both effective and temporally efficient.\nImpact of Retrieval Feedback. Table 4 isolates the effect\nof retrieval feedback. For Guess Who, injecting attribute-\ndistribution feedback shortens the dialogue while maintain-\ning ceiling success. This indicates that the agent already\nsolves the task reliably, and feedback mainly helps it ask\nmore informative follow-ups. For the image retrieval, the\nchoice of top-K feedback is decisive. Moving from top-5 to\ntop-10 retrieved images boosts SR by 36.4%, and cuts MR\n\nTable 4. Impact of retrieval feedback. For the Guess Who, we\ncompare a model with no feedback against one that receives the\ncurrent attribute distribution. For the image retrieval, we compare\nthe models using the top-5 or top-10 retrieved images.\n\nTask\nFeedback\nSR↑MT↓MedR↓MR↓\n\nGuess Who\nNo Feedback 1.0\n6.98\n-\n-\nFeedback\n1.0\n6.25\n-\n-\n\nImage Retrieval Top-5\n0.66 9.92\n4.0\n16.7\nTop-10\n0.90 6.15\n3.5\n6.35\n\nTable 5. Comparison to different image retrieval backbones.\n\nModels\nData Size\nSR/R@5↑\nMT↓\nMedR↓\nMR↓\n\nCLIP\n100 / 500\n0.90 / 0.69\n6.15 / 12.21\n3.5 / 5.0\n6.35 / 45.11\nBLIP\n100 / 500\n1.0 / 0.27\n5.22 / 20.44\n2.0 / 64.0\n2.36 / 112.11\n\nby 62.0%. Shrinking K from 10 to 5 amplifies the noise and\nbias in the model’s estimation of image feature distribution\nbased on top-K captions, leading to inaccurate expected\ninformation gain. Increasing K can stabilize this distribu-\ntion estimation, enabling the strategy to propose clearer and\nmore valuable queries.\n\nImpact of Retrieval Backbone. We investigate the im-\npact of the retrieval backbone by comparing a training-\nfree CLIP retriever and a fine-tuned BLIP [22] counter-\npart. BLIP is first fine-tuned on a synthetic dialogue dataset\n(Sec. D.3). The experiments are conducted on 100-image\nand 500-image set while holding the Questioner module\nand RL framework constant. As shown in Tab. 5, the BLIP\nbackbone achieves a higher ranking quality for the 100-\nimage set. However, this trend reversed in the 500-image\nsetting. While CLIP’s success rate remains stable at 0.69,\nBLIP’s performance degrades to 0.27. A larger dataset de-\nmands longer dialogues and more compositional questions.\nThe end-to-end BLIP does not generalize well, whereas the\nkeyword-based CLIP demonstrates more robust scalability.\n\n5. Conclusions\n\nWe propose a dialogue-driven information retrieval frame-\nwork trained with reinforcement learning.\nOur method\ntrains an agent to ask a sequence of optimally informative\nquestions to efficiently narrow the search space. This ap-\nproach aims to improve the retrieval accuracy and reduce\nthe interaction turns. To validate our framework, we de-\nsign three benchmark tasks across two data modalities: two\nstructured data retrieval games and an image retrieval task.\nExperimental results demonstrate that our method signifi-\ncantly outperforms strong baselines across all tasks.\n\n8",
      "content_length": 5107,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "Table 3. Ablation study on different reward functions.",
          "is_title": false,
          "page": 8,
          "text_length": 54
        },
        {
          "text": "Reward Function\nSR↑\nMT↓",
          "is_title": false,
          "page": 8,
          "text_length": 23
        },
        {
          "text": "EIG Only\n0.99\n7.55\nStep Penalty Only\n0.99\n6.19\nEIG + Step Penalty\n1.0\n6.25",
          "is_title": false,
          "page": 8,
          "text_length": 74
        },
        {
          "text": "broad, discriminative questions about high-level attributes\nsuch as gender.\nThese cases demonstrate that in well-\ndefined problem spaces, our framework effectively guides\nthe agent toward mathematically efficient policies. In the\nimage retrieval, Figure 4a shows a progressive refinement,\nwhere each subsequent question steadily improves the tar-\nget’s rank. Fig. 4b reveals SherlockLLM’s ability to handle\nnon-monotonic progression. In this challenging example, a\nnegative answer temporarily worsens the target’s rank from\n25 to 32.\nThis is a likely consequence of the complex-\nities within the visual-semantic embedding space. How-\never, SherlockLLM recovers from this setback, ultimately\nimproving the target’s rank to 3. This self-correction capa-\nbility is a key advantage of our interactive approach over\ntraditional one-shot query retrieval systems.\nAblation Study on Reward Components. To ascertain\nthe individual contribution of each component within our\nreward function (Sec. 3.5), we conduct an ablation study\nfor the Guess Who task. We use three experimental con-\nditions: 1) an agent rewarded solely based on EIG, 2) an\nagent guided by a terminal reward and a step penalty, and\n3) an agent trained with the complete reward function. The\nsuccess-contingent constant term is retained across all con-\nfigurations. The results are presented in Tab. 3. Sherlock-\nLLM trained with only the EIG reward learns to ask highly\ninformative questions. However, this leads to a deliberate\nbut slow strategy. SherlockLLM guided exclusively by a\nstep penalty becomes highly efficient (6.19 MT), but opti-\nmized purely for speed, may not always be the most robust\napproach. The combination of both components in the re-\nward yields a more robust policy. This agent achieves a\nperfect success rate while maintaining a highly efficient av-\nerage of 6.25 turns. The per-step guidance from the EIG re-\nward teaches the agent what constitutes an effective query,\nwhile the global step penalty incentivizes it to reach the goal\nexpeditiously. This combination compels the agent to learn\na strategy that is both effective and temporally efficient.\nImpact of Retrieval Feedback. Table 4 isolates the effect\nof retrieval feedback. For Guess Who, injecting attribute-\ndistribution feedback shortens the dialogue while maintain-\ning ceiling success. This indicates that the agent already\nsolves the task reliably, and feedback mainly helps it ask\nmore informative follow-ups. For the image retrieval, the\nchoice of top-K feedback is decisive. Moving from top-5 to\ntop-10 retrieved images boosts SR by 36.4%, and cuts MR",
          "is_title": false,
          "page": 8,
          "text_length": 2604
        },
        {
          "text": "Table 4. Impact of retrieval feedback. For the Guess Who, we\ncompare a model with no feedback against one that receives the\ncurrent attribute distribution. For the image retrieval, we compare\nthe models using the top-5 or top-10 retrieved images.",
          "is_title": false,
          "page": 8,
          "text_length": 246
        },
        {
          "text": "Task\nFeedback\nSR↑MT↓MedR↓MR↓",
          "is_title": false,
          "page": 8,
          "text_length": 28
        },
        {
          "text": "Guess Who\nNo Feedback 1.0\n6.98\n-\n-\nFeedback\n1.0\n6.25\n-\n-",
          "is_title": false,
          "page": 8,
          "text_length": 56
        },
        {
          "text": "Image Retrieval Top-5\n0.66 9.92\n4.0\n16.7\nTop-10\n0.90 6.15\n3.5\n6.35",
          "is_title": false,
          "page": 8,
          "text_length": 66
        },
        {
          "text": "Table 5. Comparison to different image retrieval backbones.",
          "is_title": false,
          "page": 8,
          "text_length": 59
        },
        {
          "text": "Models\nData Size\nSR/R@5↑\nMT↓\nMedR↓\nMR↓",
          "is_title": false,
          "page": 8,
          "text_length": 38
        },
        {
          "text": "CLIP\n100 / 500\n0.90 / 0.69\n6.15 / 12.21\n3.5 / 5.0\n6.35 / 45.11\nBLIP\n100 / 500\n1.0 / 0.27\n5.22 / 20.44\n2.0 / 64.0\n2.36 / 112.11",
          "is_title": false,
          "page": 8,
          "text_length": 126
        },
        {
          "text": "by 62.0%. Shrinking K from 10 to 5 amplifies the noise and\nbias in the model’s estimation of image feature distribution\nbased on top-K captions, leading to inaccurate expected\ninformation gain. Increasing K can stabilize this distribu-\ntion estimation, enabling the strategy to propose clearer and\nmore valuable queries.",
          "is_title": false,
          "page": 8,
          "text_length": 320
        },
        {
          "text": "Impact of Retrieval Backbone. We investigate the im-\npact of the retrieval backbone by comparing a training-\nfree CLIP retriever and a fine-tuned BLIP [22] counter-\npart. BLIP is first fine-tuned on a synthetic dialogue dataset\n(Sec. D.3). The experiments are conducted on 100-image\nand 500-image set while holding the Questioner module\nand RL framework constant. As shown in Tab. 5, the BLIP\nbackbone achieves a higher ranking quality for the 100-\nimage set. However, this trend reversed in the 500-image\nsetting. While CLIP’s success rate remains stable at 0.69,\nBLIP’s performance degrades to 0.27. A larger dataset de-\nmands longer dialogues and more compositional questions.\nThe end-to-end BLIP does not generalize well, whereas the\nkeyword-based CLIP demonstrates more robust scalability.",
          "is_title": false,
          "page": 8,
          "text_length": 794
        },
        {
          "text": "5. Conclusions",
          "is_title": true,
          "page": 8,
          "text_length": 14
        },
        {
          "text": "We propose a dialogue-driven information retrieval frame-\nwork trained with reinforcement learning.\nOur method\ntrains an agent to ask a sequence of optimally informative\nquestions to efficiently narrow the search space. This ap-\nproach aims to improve the retrieval accuracy and reduce\nthe interaction turns. To validate our framework, we de-\nsign three benchmark tasks across two data modalities: two\nstructured data retrieval games and an image retrieval task.\nExperimental results demonstrate that our method signifi-\ncantly outperforms strong baselines across all tasks.",
          "is_title": false,
          "page": 8,
          "text_length": 574
        },
        {
          "text": "8",
          "is_title": false,
          "page": 8,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 8,
      "page": 9,
      "content": "References\n\n[1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen,\nZhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng\nDou, Fuli Feng, et al. Information retrieval meets large lan-\nguage models: a strategic report from chinese ir community.\nAI open, 2023. 2\n[2] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani,\nand W Bruce Croft. Asking clarifying questions in open-\ndomain information-seeking conversations. In SIGIR, 2019.\n2\n[3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George Bm\nVan Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc,\nAidan Clark, et al. Improving language models by retriev-\ning from trillions of tokens. In International conference on\nmachine learning, pages 2206–2240. PMLR, 2022. 2\n[4] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen,\nGuolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, and\nYun Li.\nSurvey on large language model-enhanced rein-\nforcement learning: Concept, taxonomy, and methods. In\nIEEE Transactions on Neural Networks and Learning Sys-\ntems, 2025. 5\n[5] Peiyuan Chen, Zecheng Zhang, Yiping Dong, Li Zhou, and\nHan Wang. Enhancing visual question answering through\nranking-based hybrid training and multimodal fusion. arXiv\npreprint arXiv:2408.07303, 2024. 1\n[6] Gordon V Cormack,\nCharles LA Clarke,\nand Stefan\nBuettcher.\nReciprocal rank fusion outperforms condorcet\nand individual rank learning methods. In SIGIR, 2009. 1\n[7] Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao, Aida\nAmini, Qazi Mamunur Rashid, Mike Green, and Kelvin\nGuu. Dialog inpainting: Turning documents into dialogs. In\nInternational conference on machine learning, pages 4558–\n4586. PMLR, 2022. 2\n[8] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang. Im-\nage retrieval: Ideas, influences, and trends of the new age. In\nACM Comput. Surv., 2008. 2\n[9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning. arXiv, 2025. 2\n[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and\nMingwei Chang. Retrieval augmented language model pre-\ntraining. In International conference on machine learning,\npages 3929–3938. PMLR, 2020. 2\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.\nLora: Low-rank adaptation of large language models.\nIn\nICLR, 2022. 6\n[12] Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao,\nCan Xu, Wei Wu, and Zhan Chen. Playing 20 question game\nwith policy-based reinforcement learning. In EMNLP, 2018.\n3\n[13] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-\nKiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh,\nand Bryan Hooi.\nUncertainty of Thoughts: Uncertainty-\n\nAware Planning Enhances Information Seeking in LLMs. In\nNeurIPS, 2024. 2, 5\n[14] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-\nman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-\nhinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.\nIn arXiv preprint arXiv:2410.21276, 2024. 6\n[15] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hos-\nseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Ar-\nmand Joulin, Sebastian Riedel, and Edouard Grave. Atlas:\nFew-shot learning with retrieval augmented language mod-\nels. Journal of Machine Learning Research, 24(251):1–43,\n2023. 2\n[16] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang,\nand Michael Bendersky.\nQuery expansion by prompting\nlarge language models.\narXiv preprint arXiv:2305.03653,\n2023. 2\n[17] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian,\nSeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han.\nDeepretrieval:\nHacking real search engines and retriev-\ners with large language models via reinforcement learning.\nCoRR, 2025. 2\n[18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu,\nSergey Edunov, Danqi Chen, and Wen-tau Yih.\nDense\npassage retrieval for open-domain question answering. In\nEMNLP, 2020. 2\n[19] Sedrick Keh, Justin T Chiu, and Daniel Fried. Asking more\ninformative questions for grounded retrieval.\nIn Findings\nof the Association for Computational Linguistics: NAACL\n2024, 2023. 3\n[20] Saehyung Lee, Sangwon Yu, Junsung Park, Jihun Yi, and\nSungroh Yoon. Interactive text-to-image retrieval with large\nlanguage models: A plug-and-play approach. In 62nd An-\nnual Meeting of the Association for Computational Linguis-\ntics, 2024. 2\n[21] Matan Levy, Rami Ben-Ari, Nir Darshan, and Dani Lischin-\nski. Chatting makes perfect: Chat-based image retrieval. In\nNeurIPS, 2023. 2\n[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn PMLR,\n2022. 8\n[23] Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan\nIlgen, Emma Pierson, Pang Wei W Koh, and Yulia Tsvetkov.\nMediq: Question-asking llms and a benchmark for reliable\ninteractive clinical reasoning. In NeurIPS, 2024. 2\n[24] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao\nWu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, et al. Deepseek-v3 technical report.\narXiv preprint arXiv:2412.19437, 2024. 2, 6\n[25] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In ICCV, 2015.\n6\n[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In arXiv preprint arXiv:1711.05101, 2017. 6\n[27] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan\nDuan. Query rewriting in retrieval-augmented large language\nmodels. In EMNLP, 2023. 2\n\n9",
      "content_length": 5576,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "References",
          "is_title": false,
          "page": 9,
          "text_length": 10
        },
        {
          "text": "[1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen,\nZhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng\nDou, Fuli Feng, et al. Information retrieval meets large lan-\nguage models: a strategic report from chinese ir community.\nAI open, 2023. 2\n[2] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani,\nand W Bruce Croft. Asking clarifying questions in open-\ndomain information-seeking conversations. In SIGIR, 2019.\n2\n[3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George Bm\nVan Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc,\nAidan Clark, et al. Improving language models by retriev-\ning from trillions of tokens. In International conference on\nmachine learning, pages 2206–2240. PMLR, 2022. 2\n[4] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen,\nGuolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, and\nYun Li.\nSurvey on large language model-enhanced rein-\nforcement learning: Concept, taxonomy, and methods. In\nIEEE Transactions on Neural Networks and Learning Sys-\ntems, 2025. 5\n[5] Peiyuan Chen, Zecheng Zhang, Yiping Dong, Li Zhou, and\nHan Wang. Enhancing visual question answering through\nranking-based hybrid training and multimodal fusion. arXiv\npreprint arXiv:2408.07303, 2024. 1\n[6] Gordon V Cormack,\nCharles LA Clarke,\nand Stefan\nBuettcher.\nReciprocal rank fusion outperforms condorcet\nand individual rank learning methods. In SIGIR, 2009. 1\n[7] Zhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao, Aida\nAmini, Qazi Mamunur Rashid, Mike Green, and Kelvin\nGuu. Dialog inpainting: Turning documents into dialogs. In\nInternational conference on machine learning, pages 4558–\n4586. PMLR, 2022. 2\n[8] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang. Im-\nage retrieval: Ideas, influences, and trends of the new age. In\nACM Comput. Surv., 2008. 2\n[9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning. arXiv, 2025. 2\n[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and\nMingwei Chang. Retrieval augmented language model pre-\ntraining. In International conference on machine learning,\npages 3929–3938. PMLR, 2020. 2\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.\nLora: Low-rank adaptation of large language models.\nIn\nICLR, 2022. 6\n[12] Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao,\nCan Xu, Wei Wu, and Zhan Chen. Playing 20 question game\nwith policy-based reinforcement learning. In EMNLP, 2018.\n3\n[13] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-\nKiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh,\nand Bryan Hooi.\nUncertainty of Thoughts: Uncertainty-",
          "is_title": false,
          "page": 9,
          "text_length": 2783
        },
        {
          "text": "Aware Planning Enhances Information Seeking in LLMs. In\nNeurIPS, 2024. 2, 5\n[14] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-\nman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-\nhinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.\nIn arXiv preprint arXiv:2410.21276, 2024. 6\n[15] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hos-\nseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Ar-\nmand Joulin, Sebastian Riedel, and Edouard Grave. Atlas:\nFew-shot learning with retrieval augmented language mod-\nels. Journal of Machine Learning Research, 24(251):1–43,\n2023. 2\n[16] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang,\nand Michael Bendersky.\nQuery expansion by prompting\nlarge language models.\narXiv preprint arXiv:2305.03653,\n2023. 2\n[17] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian,\nSeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han.\nDeepretrieval:\nHacking real search engines and retriev-\ners with large language models via reinforcement learning.\nCoRR, 2025. 2\n[18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu,\nSergey Edunov, Danqi Chen, and Wen-tau Yih.\nDense\npassage retrieval for open-domain question answering. In\nEMNLP, 2020. 2\n[19] Sedrick Keh, Justin T Chiu, and Daniel Fried. Asking more\ninformative questions for grounded retrieval.\nIn Findings\nof the Association for Computational Linguistics: NAACL\n2024, 2023. 3\n[20] Saehyung Lee, Sangwon Yu, Junsung Park, Jihun Yi, and\nSungroh Yoon. Interactive text-to-image retrieval with large\nlanguage models: A plug-and-play approach. In 62nd An-\nnual Meeting of the Association for Computational Linguis-\ntics, 2024. 2\n[21] Matan Levy, Rami Ben-Ari, Nir Darshan, and Dani Lischin-\nski. Chatting makes perfect: Chat-based image retrieval. In\nNeurIPS, 2023. 2\n[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn PMLR,\n2022. 8\n[23] Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan\nIlgen, Emma Pierson, Pang Wei W Koh, and Yulia Tsvetkov.\nMediq: Question-asking llms and a benchmark for reliable\ninteractive clinical reasoning. In NeurIPS, 2024. 2\n[24] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao\nWu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, et al. Deepseek-v3 technical report.\narXiv preprint arXiv:2412.19437, 2024. 2, 6\n[25] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In ICCV, 2015.\n6\n[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In arXiv preprint arXiv:1711.05101, 2017. 6\n[27] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan\nDuan. Query rewriting in retrieval-augmented large language\nmodels. In EMNLP, 2023. 2",
          "is_title": false,
          "page": 9,
          "text_length": 2776
        },
        {
          "text": "9",
          "is_title": false,
          "page": 9,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 9,
      "page": 10,
      "content": "[28] Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Hao-\nnan Chen, and Hongjin Qian. Large language models know\nyour contextual search intent: A prompting framework for\nconversational search. In EMNLP, 2023. 2\n[29] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang,\nXinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and\nNingyu Zhang.\nRafe: Ranking feedback improves query\nrewriting for rag. In EMNLP, 2024. 2\n[30] Davide Mazzaccara, Alberto Testoni, and Raffaella Bernardi.\nLearning to ask informative questions: Enhancing LLMs\nwith preference optimization and expected information gain.\nIn Findings of the Association for Computational Linguis-\ntics: EMNLP 2024, 2024. 5\n[31] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou,\nXiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. Large\nlanguage model based long-tail query rewriting in taobao\nsearch. In ACM, 2024. 2\n[32] Zackary Rackauckas. Rag-fusion: a new take on retrieval-\naugmented generation.\narXiv preprint arXiv:2402.03367,\n2024. 1\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 4\n[34] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn. Direct\npreference optimization: Your language model is secretly a\nreward model. In NeurIPS, 2023. 2\n[35] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic\nrelevance framework: Bm25 and beyond. Foundations and\nTrends in Information Retrieval, 2009. 2\n[36] Gerard Salton and Michael J. McGill. Introduction to Mod-\nern Information Retrieval. McGraw-Hill, Inc., USA, 1986.\n[37] Gerard Salton, Anita Wong, and Chung-Shu Yang. A vector\nspace model for automatic indexing. Communications of the\nACM, 1975. 2\n[38] Claude E Shannon. A mathematical theory of communica-\ntion. In The Bell system technical journal, 1948. 5\n[39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao\nSong, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li,\nY. Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models. In arXiv\npreprint arXiv:2402.03300, 2024. 5\n[40] Clemencia Siro, Yifei Yuan, Mohammad Aliannejadi, and\nMaarten de Rijke. Agent-cq: Automatic generation and eval-\nuation of clarifying questions for conversational search with\nllms. In arXiv preprint arXiv:2410.19692, 2024. 2\n[41] Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lin-\ngrui Liao, Luca Bertelli, and Michael Bendersky.\nQuill:\nQuery intent with large language models using retrieval aug-\nmentation and multi-stage distillation.\nIn Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing: Industry Track, 2022. 2\n[42] Qwen Team. Qwen2.5 technical report. In arXiv preprint\narXiv:2412.15115, 2024. 6\n[43] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query\nexpansion with large language models. In EMNLP, 2023. 2\n\n[44] Julia White, Gabriel Poesia, Robert Hawkins, Dorsa Sadigh,\nand Noah Goodman.\nOpen-domain clarification question\ngeneration without question examples. In EMNLP, 2021. 3\n[45] Raymond Wilson, Chase Carter, and Cole Graham. Con-\ntextualizing search queries In-context learning for con-\nversational rewriting with LLMs.\nIn arXiv preprint\narXiv:2502.15009, 2025. 1, 2\n[46] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\nmaier. From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descrip-\ntions. In Transactions of the Association for Computational\nLinguistics, 2014. 9\n[47] Lili Yu, Howard Chen, Sida I. Wang, Tao Lei, and Yoav\nArtzi. Interactive classification by asking informative ques-\ntions. In 58th Annual Meeting of the Association for Compu-\ntational Linguistics, 2020. 5\n[48] Yifei Yuan, Zahra Abbasiantaeb, Mohammad Aliannejadi,\nand Yang Deng. Query understanding in llm-based conver-\nsational information seeking. In Proceedings of the 48th In-\nternational ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, pages 4098–4101, 2025. 1,\n2\n[49] Hamed Zamani, Bhaskar Mitra, Everest Chen, Gord Lueck,\nFernando Diaz, Paul N Bennett, Nick Craswell, and Susan T\nDumais. Analyzing and learning from user interactions for\nsearch clarification. In SIGIR, 2020. 1\n[50] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu,\nZhicheng Dou, and Ji-Rong Wen. Large language models\nfor information retrieval: A survey. ACM Trans. Inf. Syst.,\n2025. 2\n\n10",
      "content_length": 4601,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "[28] Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Hao-\nnan Chen, and Hongjin Qian. Large language models know\nyour contextual search intent: A prompting framework for\nconversational search. In EMNLP, 2023. 2\n[29] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang,\nXinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and\nNingyu Zhang.\nRafe: Ranking feedback improves query\nrewriting for rag. In EMNLP, 2024. 2\n[30] Davide Mazzaccara, Alberto Testoni, and Raffaella Bernardi.\nLearning to ask informative questions: Enhancing LLMs\nwith preference optimization and expected information gain.\nIn Findings of the Association for Computational Linguis-\ntics: EMNLP 2024, 2024. 5\n[31] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou,\nXiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. Large\nlanguage model based long-tail query rewriting in taobao\nsearch. In ACM, 2024. 2\n[32] Zackary Rackauckas. Rag-fusion: a new take on retrieval-\naugmented generation.\narXiv preprint arXiv:2402.03367,\n2024. 1\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 4\n[34] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn. Direct\npreference optimization: Your language model is secretly a\nreward model. In NeurIPS, 2023. 2\n[35] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic\nrelevance framework: Bm25 and beyond. Foundations and\nTrends in Information Retrieval, 2009. 2\n[36] Gerard Salton and Michael J. McGill. Introduction to Mod-\nern Information Retrieval. McGraw-Hill, Inc., USA, 1986.\n[37] Gerard Salton, Anita Wong, and Chung-Shu Yang. A vector\nspace model for automatic indexing. Communications of the\nACM, 1975. 2\n[38] Claude E Shannon. A mathematical theory of communica-\ntion. In The Bell system technical journal, 1948. 5\n[39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao\nSong, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li,\nY. Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models. In arXiv\npreprint arXiv:2402.03300, 2024. 5\n[40] Clemencia Siro, Yifei Yuan, Mohammad Aliannejadi, and\nMaarten de Rijke. Agent-cq: Automatic generation and eval-\nuation of clarifying questions for conversational search with\nllms. In arXiv preprint arXiv:2410.19692, 2024. 2\n[41] Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lin-\ngrui Liao, Luca Bertelli, and Michael Bendersky.\nQuill:\nQuery intent with large language models using retrieval aug-\nmentation and multi-stage distillation.\nIn Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing: Industry Track, 2022. 2\n[42] Qwen Team. Qwen2.5 technical report. In arXiv preprint\narXiv:2412.15115, 2024. 6\n[43] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query\nexpansion with large language models. In EMNLP, 2023. 2",
          "is_title": false,
          "page": 10,
          "text_length": 3017
        },
        {
          "text": "[44] Julia White, Gabriel Poesia, Robert Hawkins, Dorsa Sadigh,\nand Noah Goodman.\nOpen-domain clarification question\ngeneration without question examples. In EMNLP, 2021. 3\n[45] Raymond Wilson, Chase Carter, and Cole Graham. Con-\ntextualizing search queries In-context learning for con-\nversational rewriting with LLMs.\nIn arXiv preprint\narXiv:2502.15009, 2025. 1, 2\n[46] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\nmaier. From image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event descrip-\ntions. In Transactions of the Association for Computational\nLinguistics, 2014. 9\n[47] Lili Yu, Howard Chen, Sida I. Wang, Tao Lei, and Yoav\nArtzi. Interactive classification by asking informative ques-\ntions. In 58th Annual Meeting of the Association for Compu-\ntational Linguistics, 2020. 5\n[48] Yifei Yuan, Zahra Abbasiantaeb, Mohammad Aliannejadi,\nand Yang Deng. Query understanding in llm-based conver-\nsational information seeking. In Proceedings of the 48th In-\nternational ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, pages 4098–4101, 2025. 1,\n2\n[49] Hamed Zamani, Bhaskar Mitra, Everest Chen, Gord Lueck,\nFernando Diaz, Paul N Bennett, Nick Craswell, and Susan T\nDumais. Analyzing and learning from user interactions for\nsearch clarification. In SIGIR, 2020. 1\n[50] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu,\nZhicheng Dou, and Ji-Rong Wen. Large language models\nfor information retrieval: A survey. ACM Trans. Inf. Syst.,\n2025. 2",
          "is_title": false,
          "page": 10,
          "text_length": 1578
        },
        {
          "text": "10",
          "is_title": false,
          "page": 10,
          "text_length": 2
        }
      ]
    },
    {
      "document_index": 10,
      "page": 11,
      "content": "Sherlock Your Queries: Learning to Ask the Right Questions\nfor Dialogue-Based Retrieval\n\nSupplementary Material\n\nA. Analysis of Alternative Methods\n\nThis section justifies the core design choices of the Sher-\nlockLLM for image retrieval. First, we justify our use of\na rank-based reward function, which we compare to an al-\nternative similarity-based formulation. Second, we evaluate\nour ranking algorithm. This evaluation involves a compari-\nson with other common fusion methods.\n\nA.1. Image Retrieval Reward Signal.\n\nTo optimize the training of our agent for the interactive im-\nage retrieval task, we investigated the efficacy of two differ-\nent reward signal formulations. Intuitively, if the similarity\nscore between the target and the query is higher, then it will\nrank higher. The objective of this experiment was to deter-\nmine whether a direct optimization of the target’s rank is\nmore effective than using an implicit signal based on text-\nimage similarity. All comparative experiments were trained\non the 100-image dataset. Except for the definition of the re-\nward function, all other conditions and training procedures\nremained consistent.\nAs Table 6 shows, both reward signals achieve an identi-\ncal SR of 0.90. This indicates that both formulations are\nfeasible.\nHowever, we observe clear differences in effi-\nciency and ranking quality.\nThe agent trained with the\nRank-based reward demonstrates superior efficiency. It re-\nquires an MT of 6.15, a 6.1% reduction in conversational\nlength compared to the 6.55 turns required by the agent\ntrained on the Similarity score.\nFurthermore, the Rank-\nbased reward achieves a MedR of 3.5, representing a 12.5%\nimprovement for the Similarity-based reward.\nInterestingly, the Similarity-based reward achieves a\nslightly better MR of 5.9. The Rank-based reward’s MR is\napproximately 7.6% worse by comparison. We hypothesize\nthat while the similarity-based agent is less efficient on av-\nerage, it generates more exceptionally descriptive questions\nthat result in a perfect R@1 retrieval. These highly success-\nful instances would significantly pull down the mean rank.\nConversely, the rank-based agent is more consistent, reli-\nably placing the target in the top few positions as suggested\nby its superior median rank, but with fewer perfect R@1\nretrievals. This is substantiated by an analysis of the R@1\nmetric. The agent trained on Similarity achieves an R@1 of\n0.25, whereas the agent trained on Rank only reaches 0.1.\nDespite the superior mean rank achieved with the simi-\nlarity signal, we select the Rank-based reward as the main\nmethod in this work. The significant gains in dialogue effi-\nciency and more reliable typical performance (12.5% better\n\nTable 6. Comparison of retrieval performance on CelebA image\nretrieval task using Rank vs. Similarity as the reward signal.\n\nReward Signal\nSR/R@5↑\nMT↓\nMedR↓\nMR↓\nR@1↑\n\nRank\n0.90\n6.15\n3.5\n6.35\n0.1\nSimilarity\n0.90\n6.55\n4.0\n5.9\n0.25\n\nmedian rank) are of paramount importance for an interac-\ntive retrieval system.\n\nA.2. Ranking Algorithm Comparison\n\nThe ranking algorithm is a critical component of our agent\nfor the image retrieval task. It translates the dialogue his-\ntory into a ranked list of candidate images at each turn. We\njustify our choice of algorithm by comparing our proposed\nmethod against two plausible and computationally efficient\nalternatives.\nOur primary ranking algorithm, detailed in Algorithm 1,\noperates on a cumulative basis. This method is designed for\nhigh precision because it leverages all information gathered\nup to the current turn. The main drawback of this approach\nis its computational cost. Recalculating similarities for the\nentire keyword history at every turn can introduce signifi-\ncant latency. We therefore design two alternative algorithms\nthat improve efficiency by avoiding these historical recalcu-\nlations.\n\nReciprocal Rank Fusion (RRF).\nRRF [6] is an algorithm\nthat combines multiple ranked lists into a single, more op-\ntimal result.\nThis method has seen wide application in\nmodern retrieval systems [5, 32]. The algorithm is entirely\ntraining-free. It operates only on the rank positions of items\nand does not require their underlying relevance scores. The\nRRF score for a candidate is the sum of the reciprocal of\nits rank across all lists. A small constant k = 60 is typi-\ncally added to the denominator. The formula is given by the\nfollowing equation.\n\nScoreRRF (c) =\n\nN\nX\n\ni=1\n\n1\nk + ranki(c)\n(17)\n\nwhere ranki(c) is the rank of candidate c in the i-th list from\na total of N lists.\nIn our implementation of this alternative, we use RRF to\nfuse information sequentially across dialogue turns. At each\nturn t, we apply Algorithm 1 using only the most recent key-\nwords. This process generates a turn-specific ranked list,\n\n1",
      "content_length": 4783,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "Sherlock Your Queries: Learning to Ask the Right Questions\nfor Dialogue-Based Retrieval",
          "is_title": false,
          "page": 11,
          "text_length": 87
        },
        {
          "text": "Supplementary Material",
          "is_title": false,
          "page": 11,
          "text_length": 22
        },
        {
          "text": "A. Analysis of Alternative Methods",
          "is_title": false,
          "page": 11,
          "text_length": 34
        },
        {
          "text": "This section justifies the core design choices of the Sher-\nlockLLM for image retrieval. First, we justify our use of\na rank-based reward function, which we compare to an al-\nternative similarity-based formulation. Second, we evaluate\nour ranking algorithm. This evaluation involves a compari-\nson with other common fusion methods.",
          "is_title": false,
          "page": 11,
          "text_length": 331
        },
        {
          "text": "A.1. Image Retrieval Reward Signal.",
          "is_title": false,
          "page": 11,
          "text_length": 35
        },
        {
          "text": "To optimize the training of our agent for the interactive im-\nage retrieval task, we investigated the efficacy of two differ-\nent reward signal formulations. Intuitively, if the similarity\nscore between the target and the query is higher, then it will\nrank higher. The objective of this experiment was to deter-\nmine whether a direct optimization of the target’s rank is\nmore effective than using an implicit signal based on text-\nimage similarity. All comparative experiments were trained\non the 100-image dataset. Except for the definition of the re-\nward function, all other conditions and training procedures\nremained consistent.\nAs Table 6 shows, both reward signals achieve an identi-\ncal SR of 0.90. This indicates that both formulations are\nfeasible.\nHowever, we observe clear differences in effi-\nciency and ranking quality.\nThe agent trained with the\nRank-based reward demonstrates superior efficiency. It re-\nquires an MT of 6.15, a 6.1% reduction in conversational\nlength compared to the 6.55 turns required by the agent\ntrained on the Similarity score.\nFurthermore, the Rank-\nbased reward achieves a MedR of 3.5, representing a 12.5%\nimprovement for the Similarity-based reward.\nInterestingly, the Similarity-based reward achieves a\nslightly better MR of 5.9. The Rank-based reward’s MR is\napproximately 7.6% worse by comparison. We hypothesize\nthat while the similarity-based agent is less efficient on av-\nerage, it generates more exceptionally descriptive questions\nthat result in a perfect R@1 retrieval. These highly success-\nful instances would significantly pull down the mean rank.\nConversely, the rank-based agent is more consistent, reli-\nably placing the target in the top few positions as suggested\nby its superior median rank, but with fewer perfect R@1\nretrievals. This is substantiated by an analysis of the R@1\nmetric. The agent trained on Similarity achieves an R@1 of\n0.25, whereas the agent trained on Rank only reaches 0.1.\nDespite the superior mean rank achieved with the simi-\nlarity signal, we select the Rank-based reward as the main\nmethod in this work. The significant gains in dialogue effi-\nciency and more reliable typical performance (12.5% better",
          "is_title": false,
          "page": 11,
          "text_length": 2190
        },
        {
          "text": "Table 6. Comparison of retrieval performance on CelebA image\nretrieval task using Rank vs. Similarity as the reward signal.",
          "is_title": false,
          "page": 11,
          "text_length": 123
        },
        {
          "text": "Reward Signal\nSR/R@5↑\nMT↓\nMedR↓\nMR↓\nR@1↑",
          "is_title": false,
          "page": 11,
          "text_length": 40
        },
        {
          "text": "Rank\n0.90\n6.15\n3.5\n6.35\n0.1\nSimilarity\n0.90\n6.55\n4.0\n5.9\n0.25",
          "is_title": false,
          "page": 11,
          "text_length": 61
        },
        {
          "text": "median rank) are of paramount importance for an interac-\ntive retrieval system.",
          "is_title": false,
          "page": 11,
          "text_length": 79
        },
        {
          "text": "A.2. Ranking Algorithm Comparison",
          "is_title": false,
          "page": 11,
          "text_length": 33
        },
        {
          "text": "The ranking algorithm is a critical component of our agent\nfor the image retrieval task. It translates the dialogue his-\ntory into a ranked list of candidate images at each turn. We\njustify our choice of algorithm by comparing our proposed\nmethod against two plausible and computationally efficient\nalternatives.\nOur primary ranking algorithm, detailed in Algorithm 1,\noperates on a cumulative basis. This method is designed for\nhigh precision because it leverages all information gathered\nup to the current turn. The main drawback of this approach\nis its computational cost. Recalculating similarities for the\nentire keyword history at every turn can introduce signifi-\ncant latency. We therefore design two alternative algorithms\nthat improve efficiency by avoiding these historical recalcu-\nlations.",
          "is_title": false,
          "page": 11,
          "text_length": 802
        },
        {
          "text": "Reciprocal Rank Fusion (RRF).\nRRF [6] is an algorithm\nthat combines multiple ranked lists into a single, more op-\ntimal result.\nThis method has seen wide application in\nmodern retrieval systems [5, 32]. The algorithm is entirely\ntraining-free. It operates only on the rank positions of items\nand does not require their underlying relevance scores. The\nRRF score for a candidate is the sum of the reciprocal of\nits rank across all lists. A small constant k = 60 is typi-\ncally added to the denominator. The formula is given by the\nfollowing equation.",
          "is_title": false,
          "page": 11,
          "text_length": 549
        },
        {
          "text": "ScoreRRF (c) =",
          "is_title": false,
          "page": 11,
          "text_length": 14
        },
        {
          "text": "N\nX",
          "is_title": false,
          "page": 11,
          "text_length": 3
        },
        {
          "text": "i=1",
          "is_title": false,
          "page": 11,
          "text_length": 3
        },
        {
          "text": "1\nk + ranki(c)\n(17)",
          "is_title": false,
          "page": 11,
          "text_length": 19
        },
        {
          "text": "where ranki(c) is the rank of candidate c in the i-th list from\na total of N lists.\nIn our implementation of this alternative, we use RRF to\nfuse information sequentially across dialogue turns. At each\nturn t, we apply Algorithm 1 using only the most recent key-\nwords. This process generates a turn-specific ranked list,",
          "is_title": false,
          "page": 11,
          "text_length": 321
        },
        {
          "text": "1",
          "is_title": false,
          "page": 11,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 11,
      "page": 12,
      "content": "denoted as Rankt = {rank1, rank2, . . . , rankt}. The con-\nsolidated ranking for the current turn is then produced by\napplying Equation (17) to the complete set of lists generated\nup to that point. This approach allows for turn-by-turn up-\ndates without the computational overhead of re-evaluating\nthe entire dialogue history.\n\nSequential Score Multiplication (SSM).\nSSM also op-\nerates sequentially but fuses information at the score level.\nThis method functions as a soft logical AND gate. A can-\ndidate image must satisfy the constraints from every turn to\nremain a viable option.\nAt each turn t, a turn-specific score is calculated for\neach candidate image based on its similarity to the current\nturn’s positive keywords (Simpositive) and negative key-\nwords (Simnegative), with both values scaled to a [0, 1]\nrange. The turn score is then defined by:\n\nScoret = Simpositive × (1 −Simnegative)\n(18)\n\nThe final relevance score for an image at turn t is the\ncumulative product of all turn scores up to that point, where\nSfinal,t = Sfinal,t−1 × Scoret. The initial score is set to\nSfinal,0 = 1. This cumulative multiplication ensures that\nonly images that consistently receive high scores across all\nturns will maintain a high final ranking.\n\nResults.\nWe conduct the experiment on the 100-image\ndataset. The dialogues are generated for each target image\nusing the methodology described in Section D.3.\nAs shown in Table 7, the results demonstrate the clear su-\nperiority of our proposed algorithm across all key metrics.\nOur SherlockLLM algorithm achieves an R@5 of 0.42, a\nR@10 of 0.57, and a MedR of 7.5. In contrast, the perfor-\nmance of the two methods is significantly worse. The RRF\nalgorithm’s R@5 is only 0.06, while the SSM algorithm\nfails to rank any target image in the top position. Their Me-\ndian and Mean Ranks both exceed 50. This result indicates\nthe target image did not even enter a meaningful ranking\nrange.\nThe failure of the RRF method stems from its incor-\nrect assumption that information from each turn constitutes\nparallel and independent evidence. In our interactive sce-\nnario, the dialogue imposes sequential, conditional con-\nstraints. For example, a query for “red hair” after a posi-\ntive response to “is it a man?” implies a search for a “man\nwith red hair.” The RRF fusion mechanism cannot model\nthis implicit AND condition. Instead, it simply combines\nthe ranked list for “man” with the ranked list for “red hair,”\nfailing to properly constrain the search space as the dialogue\nprogresses.\nThe SSM method fails because it operates on the flawed\nassumption that similarity scores from each turn are equally\n\nTable 7. Comparison to different image retrieval ranking algo-\nrithms.\n\nAlgorithm\nR@1↑\nR@5↑\nR@10↑\nMedR↓\nMR↓\n\nSherlockLLM\n0.15\n0.42\n0.57\n7.5\n15.33\nRRF\n0.02\n0.06\n0.1\n50.5\n50.41\nSSM\n0.0\n0.01\n0.05\n53.5\n53.73\n\nimportant and directly comparable. However, query terms\nvary in their semantics and difficulty, which leads to vastly\ndifferent score distributions from the retrieval model. Multi-\nplying these scores, which are derived from different scales,\ndistorts the true relevance of candidates and produces unre-\nliable rankings.\n\nB. Additional Case Studies\n\nB.1. Guess Number\n\nFigure 5 shows examples of three models on the Guess\nNumber task. The goal is to find the target number 143\nwithin an integer set {86, . . . , 185}. The DeepSeek and\nQwen2.5-32B execute a near-optimal binary search. In con-\ntrast, Qwen2.5-1.5B performs an inefficient linear scan by\nsequentially checking fixed-size intervals.\nWhile it cor-\nrectly identifies the target’s range in turn 9, the model fails\nto narrow the search further and exceeds the maximum turn\nlimit.\nThe Qwen2.5-7B attempts a binary search but demon-\nstrates a flawed strategy. The model commits severe logical\nerrors and repetitive questions. This indicates that 7B can\nimitate the form of an optimal policy, but lacks the robust\nreasoning for the task.\nSFT agent learns to mimic actions from the training data,\nbut it fails to optimize for turn efficiency. In the initial phase\n(turns 1-6), the agent’s performance is optimal. It consis-\ntently poses a series of questions with high or perfect EIG\nscores. However, the agent’s strategy falters in the endgame\nphase. At turn 7, instead of making a decisive guess, the\nagent asks other questions.\n\nB.2. Guess Who\n\nFigure 6 presents a comparison of the dialogue strategies\nthat the three models employed on the Guess Who task. The\nDeepSeek demonstrates a logical and efficient strategy. It\nqueries attributes step by step and consistently selects ques-\ntions with a high EIG. In contrast, the Qwen2.5-1.5B ex-\nhibits a flawed policy. It enters a repetitive loop, repeatedly\nasking a question with zero EIG. This behavior indicates a\ndeficiency in multi-turn conversation capabilities and ulti-\nmately results in task failure.\nThe Qwen2.5-7B model shows an improvement over its\n1.5B counterpart. It initially asks general attributes ques-\ntions. However, while target C33 was the only remaining\n\n2",
      "content_length": 5030,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "denoted as Rankt = {rank1, rank2, . . . , rankt}. The con-\nsolidated ranking for the current turn is then produced by\napplying Equation (17) to the complete set of lists generated\nup to that point. This approach allows for turn-by-turn up-\ndates without the computational overhead of re-evaluating\nthe entire dialogue history.",
          "is_title": false,
          "page": 12,
          "text_length": 326
        },
        {
          "text": "Sequential Score Multiplication (SSM).\nSSM also op-\nerates sequentially but fuses information at the score level.\nThis method functions as a soft logical AND gate. A can-\ndidate image must satisfy the constraints from every turn to\nremain a viable option.\nAt each turn t, a turn-specific score is calculated for\neach candidate image based on its similarity to the current\nturn’s positive keywords (Simpositive) and negative key-\nwords (Simnegative), with both values scaled to a [0, 1]\nrange. The turn score is then defined by:",
          "is_title": false,
          "page": 12,
          "text_length": 527
        },
        {
          "text": "Scoret = Simpositive × (1 −Simnegative)\n(18)",
          "is_title": false,
          "page": 12,
          "text_length": 44
        },
        {
          "text": "The final relevance score for an image at turn t is the\ncumulative product of all turn scores up to that point, where\nSfinal,t = Sfinal,t−1 × Scoret. The initial score is set to\nSfinal,0 = 1. This cumulative multiplication ensures that\nonly images that consistently receive high scores across all\nturns will maintain a high final ranking.",
          "is_title": false,
          "page": 12,
          "text_length": 338
        },
        {
          "text": "Results.\nWe conduct the experiment on the 100-image\ndataset. The dialogues are generated for each target image\nusing the methodology described in Section D.3.\nAs shown in Table 7, the results demonstrate the clear su-\nperiority of our proposed algorithm across all key metrics.\nOur SherlockLLM algorithm achieves an R@5 of 0.42, a\nR@10 of 0.57, and a MedR of 7.5. In contrast, the perfor-\nmance of the two methods is significantly worse. The RRF\nalgorithm’s R@5 is only 0.06, while the SSM algorithm\nfails to rank any target image in the top position. Their Me-\ndian and Mean Ranks both exceed 50. This result indicates\nthe target image did not even enter a meaningful ranking\nrange.\nThe failure of the RRF method stems from its incor-\nrect assumption that information from each turn constitutes\nparallel and independent evidence. In our interactive sce-\nnario, the dialogue imposes sequential, conditional con-\nstraints. For example, a query for “red hair” after a posi-\ntive response to “is it a man?” implies a search for a “man\nwith red hair.” The RRF fusion mechanism cannot model\nthis implicit AND condition. Instead, it simply combines\nthe ranked list for “man” with the ranked list for “red hair,”\nfailing to properly constrain the search space as the dialogue\nprogresses.\nThe SSM method fails because it operates on the flawed\nassumption that similarity scores from each turn are equally",
          "is_title": false,
          "page": 12,
          "text_length": 1396
        },
        {
          "text": "Table 7. Comparison to different image retrieval ranking algo-\nrithms.",
          "is_title": false,
          "page": 12,
          "text_length": 70
        },
        {
          "text": "Algorithm\nR@1↑\nR@5↑\nR@10↑\nMedR↓\nMR↓",
          "is_title": false,
          "page": 12,
          "text_length": 35
        },
        {
          "text": "SherlockLLM\n0.15\n0.42\n0.57\n7.5\n15.33\nRRF\n0.02\n0.06\n0.1\n50.5\n50.41\nSSM\n0.0\n0.01\n0.05\n53.5\n53.73",
          "is_title": false,
          "page": 12,
          "text_length": 94
        },
        {
          "text": "important and directly comparable. However, query terms\nvary in their semantics and difficulty, which leads to vastly\ndifferent score distributions from the retrieval model. Multi-\nplying these scores, which are derived from different scales,\ndistorts the true relevance of candidates and produces unre-\nliable rankings.",
          "is_title": false,
          "page": 12,
          "text_length": 320
        },
        {
          "text": "B. Additional Case Studies",
          "is_title": false,
          "page": 12,
          "text_length": 26
        },
        {
          "text": "B.1. Guess Number",
          "is_title": false,
          "page": 12,
          "text_length": 17
        },
        {
          "text": "Figure 5 shows examples of three models on the Guess\nNumber task. The goal is to find the target number 143\nwithin an integer set {86, . . . , 185}. The DeepSeek and\nQwen2.5-32B execute a near-optimal binary search. In con-\ntrast, Qwen2.5-1.5B performs an inefficient linear scan by\nsequentially checking fixed-size intervals.\nWhile it cor-\nrectly identifies the target’s range in turn 9, the model fails\nto narrow the search further and exceeds the maximum turn\nlimit.\nThe Qwen2.5-7B attempts a binary search but demon-\nstrates a flawed strategy. The model commits severe logical\nerrors and repetitive questions. This indicates that 7B can\nimitate the form of an optimal policy, but lacks the robust\nreasoning for the task.\nSFT agent learns to mimic actions from the training data,\nbut it fails to optimize for turn efficiency. In the initial phase\n(turns 1-6), the agent’s performance is optimal. It consis-\ntently poses a series of questions with high or perfect EIG\nscores. However, the agent’s strategy falters in the endgame\nphase. At turn 7, instead of making a decisive guess, the\nagent asks other questions.",
          "is_title": false,
          "page": 12,
          "text_length": 1116
        },
        {
          "text": "B.2. Guess Who",
          "is_title": false,
          "page": 12,
          "text_length": 14
        },
        {
          "text": "Figure 6 presents a comparison of the dialogue strategies\nthat the three models employed on the Guess Who task. The\nDeepSeek demonstrates a logical and efficient strategy. It\nqueries attributes step by step and consistently selects ques-\ntions with a high EIG. In contrast, the Qwen2.5-1.5B ex-\nhibits a flawed policy. It enters a repetitive loop, repeatedly\nasking a question with zero EIG. This behavior indicates a\ndeficiency in multi-turn conversation capabilities and ulti-\nmately results in task failure.\nThe Qwen2.5-7B model shows an improvement over its\n1.5B counterpart. It initially asks general attributes ques-\ntions. However, while target C33 was the only remaining",
          "is_title": false,
          "page": 12,
          "text_length": 678
        },
        {
          "text": "2",
          "is_title": false,
          "page": 12,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 12,
      "page": 13,
      "content": "No.​ Question\nAnswer\nEIG\nNo.​ Question\nAnswer\nEIG\n1​\nIs the target number less than or equal to 135?\nNo\n1.0​\n1​\nIs the target number greater than 135?\nYes\n1.0​\n2​\nIs the target number less than or equal to 160?\nYes\n1.0​\n2​\nIs the target number greater than 160?\nNo\n1.0​\n3​\nIs the target number less than or equal to 147? \nYes\n1.0​\n3​\nIs the target number greater than 147?\nNo\n0.99\n4​\nIs the target number less than or equal to 141?\nNo\n1.0​\n4​\nIs the target number greater than 141?\nYes\n1.0​\n5​\nIs the target number less than or equal to 144? \nYes\n1.0​\n5​\nIs the target number greater than 144?\nNo\n1.0​\n6​\nIs the target number 144?\nNo\n0.92\n6​\nIs the target number greater than 142?\nYes\n0.91\n7​\nIs the target number 143?\nYes\n1.0​\n7​\nIs the target number 143?\nYes\n1.0​\n\nNo.​ Question\nAnswer\nEIG\nNo.​ Question\nAnswer\nEIG\n1​\nIs the target number less than 90?\nNo\n0.24\n1​\nIs the target number less than or equal to 135?\nNo\n1.0\n\n2​\nIs the target number between 86 and 93 \n(inclusive)?\nNo\n0.0\n2​\nIs the target number less than or equal to 180?\nYes\n0.46\n\n3​\nIs the target number between 90 and 97 \n(inclusive)?\nNo\n0.41\n3​\nIs the target number less than or equal to 157?\nYes\n0.99\n\n4​\nIs the target number between 98 and 105 \n(inclusive)?\nNo\n0.43\n4​\nIs the target number less than or equal to 121?\nNo\n0.0​\n\n5​\nIs the target number between 106 and 113 \n(inclusive)?\nNo\n0.46\n5​\nIs the target number less than or equal to 144?\nYes\n0.97\n\n6​\nIs the target number between 114 and 121 \n(inclusive)?\nNo\n0.50\n6​\nIs the target number less than or equal to 139?\nNo\n0.99\n\n7​\nIs the target number between 122 and 129 \n(inclusive)?\nNo\n0.54\n7​\nIs the target number less than or equal to 148?\nYes\n0.0​\n\n8​\nIs the target number between 130 and 137 \n(inclusive)?\nNo\n0.59\n8​\nIs the target number less than or equal to 146?\nYes\n0.0​\n\n9​\nIs the target number between 138 and 145 \n(inclusive)?\nYes\n0.65\n9​\nIs the target number less than or equal to 144?\nYes\n0.0​\n\n10​ Congratulations! You''ve guessed the correct \n\nnumber: 145.\n\nI can't answer \n\nthis question 0.0​\n10​ Is the target number less than or equal to 142?\nNo\n0.97\n\n11​ Is the target number between 146 and 153 \n\n(inclusive)?\nNo\n0.0​\n11​ Is the target number less than or equal to 145?\nYes\n0.0\n\n12​ Is the target number between 154 and 161 \n\n(inclusive)?\nNo\n0.0​\n12​ Is the target number less than or equal to 144?\nYes\n0.0​\n\n13​ Is the target number between 162 and 169 \n\n(inclusive)?\nNo\n0.0​\n13​ Is the target number 144?\nNo\n1.0\n\n14​ Is the target number between 170 and 177 \n\n(inclusive)?\nNo\n0.0​\n14​ Is the target number less than or equal to 143?\nYes\n0.0​\n\n15​ Is the target number between 178 and 185 \n\n(inclusive)?\nNo\n0.0​\n15​ Is the target number 143?\nYes\n0.0\n\n16​ Is the target number exactly 185?\nNo\n0.0​\n\nNo.​ Question\nAnswer\nEIG\nNo.​ Question\nAnswer\nEIG\n1​\nIs the number even?\nNo\n1.0\n1​\nIs this number less than or equal to 135?\nNo\n1.0\n2​\nIs the number less than 135?\nNo\n0.99\n2​\nIs this number odd?\nYes\n1.0\n3​\nIs the number less than 165?\nYes\n0.98\n3​\nIs this number greater than 160?\nYes\n0.99\n4​\nIs the number less than 150?\nYes\n0.99\n4​\nIs this number greater than 147?\nNo\n1.0\n5​\nIs the number less than 142?\nNo\n1.0\n5​\nIs this number greater than or equal to 142?\nYes\n1.0\n6​\nIs the number less than 146?\nYes\n1.0\n6​\nIs this number greater than 144?\nNo\n0.92\n7​\nIs the number 143?\nYes\n1.0\n7​\nIs this number greater than or equal to 143?\nYes\n0.0​\n8​\nIs this number greater than 143?\nNo\n0.0​\n9​\nThis number: 143.\nYes\n0.0​\n\n(e) Qwen2.5-32B\n\n(a) Qwen2.5-7B+GRPO\n(b) DeepSeek-V3.1\n\n(c) Qwen2.5-1.5B\n(d) Qwen2.5-7B\n\n(f) Qwen2.5-7B + SFT\n\nFigure 5. Examples of the Guess Number performed by baseline models and SherlockLLM.\n\ncandidate after turn 6, the model proceeded with several\nrounds of unnecessary guessing before making the correct\nidentification.\n\nQwen2.5-32B is better than Qwen2.5-7B. It asks high-\nEIG questions about character attributes. However, its strat-\negy is demonstrably suboptimal and contains clear logical\ninconsistencies. It fails to perform the necessary logical de-\n\nduction on the constrained candidate pool.\n\nSimilar to the behavior of SFT in Guess Number task,\nSFT agent can mimic questioning, but its policy is weak\nwhen the optimal action shifts from information gathering\nto logical inference.\n\n3",
      "content_length": 4256,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "No.​ Question\nAnswer\nEIG\nNo.​ Question\nAnswer\nEIG\n1​\nIs the target number less than or equal to 135?\nNo\n1.0​\n1​\nIs the target number greater than 135?\nYes\n1.0​\n2​\nIs the target number less than or equal to 160?\nYes\n1.0​\n2​\nIs the target number greater than 160?\nNo\n1.0​\n3​\nIs the target number less than or equal to 147? \nYes\n1.0​\n3​\nIs the target number greater than 147?\nNo\n0.99\n4​\nIs the target number less than or equal to 141?\nNo\n1.0​\n4​\nIs the target number greater than 141?\nYes\n1.0​\n5​\nIs the target number less than or equal to 144? \nYes\n1.0​\n5​\nIs the target number greater than 144?\nNo\n1.0​\n6​\nIs the target number 144?\nNo\n0.92\n6​\nIs the target number greater than 142?\nYes\n0.91\n7​\nIs the target number 143?\nYes\n1.0​\n7​\nIs the target number 143?\nYes\n1.0​",
          "is_title": false,
          "page": 13,
          "text_length": 765
        },
        {
          "text": "No.​ Question\nAnswer\nEIG\nNo.​ Question\nAnswer\nEIG\n1​\nIs the target number less than 90?\nNo\n0.24\n1​\nIs the target number less than or equal to 135?\nNo\n1.0",
          "is_title": false,
          "page": 13,
          "text_length": 153
        },
        {
          "text": "2​\nIs the target number between 86 and 93 \n(inclusive)?\nNo\n0.0\n2​\nIs the target number less than or equal to 180?\nYes\n0.46",
          "is_title": false,
          "page": 13,
          "text_length": 122
        },
        {
          "text": "3​\nIs the target number between 90 and 97 \n(inclusive)?\nNo\n0.41\n3​\nIs the target number less than or equal to 157?\nYes\n0.99",
          "is_title": false,
          "page": 13,
          "text_length": 123
        },
        {
          "text": "4​\nIs the target number between 98 and 105 \n(inclusive)?\nNo\n0.43\n4​\nIs the target number less than or equal to 121?\nNo\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 123
        },
        {
          "text": "5​\nIs the target number between 106 and 113 \n(inclusive)?\nNo\n0.46\n5​\nIs the target number less than or equal to 144?\nYes\n0.97",
          "is_title": false,
          "page": 13,
          "text_length": 125
        },
        {
          "text": "6​\nIs the target number between 114 and 121 \n(inclusive)?\nNo\n0.50\n6​\nIs the target number less than or equal to 139?\nNo\n0.99",
          "is_title": false,
          "page": 13,
          "text_length": 124
        },
        {
          "text": "7​\nIs the target number between 122 and 129 \n(inclusive)?\nNo\n0.54\n7​\nIs the target number less than or equal to 148?\nYes\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 125
        },
        {
          "text": "8​\nIs the target number between 130 and 137 \n(inclusive)?\nNo\n0.59\n8​\nIs the target number less than or equal to 146?\nYes\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 125
        },
        {
          "text": "9​\nIs the target number between 138 and 145 \n(inclusive)?\nYes\n0.65\n9​\nIs the target number less than or equal to 144?\nYes\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 126
        },
        {
          "text": "10​ Congratulations! You''ve guessed the correct",
          "is_title": false,
          "page": 13,
          "text_length": 48
        },
        {
          "text": "number: 145.",
          "is_title": false,
          "page": 13,
          "text_length": 12
        },
        {
          "text": "I can't answer",
          "is_title": false,
          "page": 13,
          "text_length": 14
        },
        {
          "text": "this question 0.0​\n10​ Is the target number less than or equal to 142?\nNo\n0.97",
          "is_title": false,
          "page": 13,
          "text_length": 78
        },
        {
          "text": "11​ Is the target number between 146 and 153",
          "is_title": false,
          "page": 13,
          "text_length": 44
        },
        {
          "text": "(inclusive)?\nNo\n0.0​\n11​ Is the target number less than or equal to 145?\nYes\n0.0",
          "is_title": false,
          "page": 13,
          "text_length": 80
        },
        {
          "text": "12​ Is the target number between 154 and 161",
          "is_title": false,
          "page": 13,
          "text_length": 44
        },
        {
          "text": "(inclusive)?\nNo\n0.0​\n12​ Is the target number less than or equal to 144?\nYes\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 81
        },
        {
          "text": "13​ Is the target number between 162 and 169",
          "is_title": false,
          "page": 13,
          "text_length": 44
        },
        {
          "text": "(inclusive)?\nNo\n0.0​\n13​ Is the target number 144?\nNo\n1.0",
          "is_title": false,
          "page": 13,
          "text_length": 57
        },
        {
          "text": "14​ Is the target number between 170 and 177",
          "is_title": false,
          "page": 13,
          "text_length": 44
        },
        {
          "text": "(inclusive)?\nNo\n0.0​\n14​ Is the target number less than or equal to 143?\nYes\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 81
        },
        {
          "text": "15​ Is the target number between 178 and 185",
          "is_title": false,
          "page": 13,
          "text_length": 44
        },
        {
          "text": "(inclusive)?\nNo\n0.0​\n15​ Is the target number 143?\nYes\n0.0",
          "is_title": false,
          "page": 13,
          "text_length": 58
        },
        {
          "text": "16​ Is the target number exactly 185?\nNo\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 45
        },
        {
          "text": "No.​ Question\nAnswer\nEIG\nNo.​ Question\nAnswer\nEIG\n1​\nIs the number even?\nNo\n1.0\n1​\nIs this number less than or equal to 135?\nNo\n1.0\n2​\nIs the number less than 135?\nNo\n0.99\n2​\nIs this number odd?\nYes\n1.0\n3​\nIs the number less than 165?\nYes\n0.98\n3​\nIs this number greater than 160?\nYes\n0.99\n4​\nIs the number less than 150?\nYes\n0.99\n4​\nIs this number greater than 147?\nNo\n1.0\n5​\nIs the number less than 142?\nNo\n1.0\n5​\nIs this number greater than or equal to 142?\nYes\n1.0\n6​\nIs the number less than 146?\nYes\n1.0\n6​\nIs this number greater than 144?\nNo\n0.92\n7​\nIs the number 143?\nYes\n1.0\n7​\nIs this number greater than or equal to 143?\nYes\n0.0​\n8​\nIs this number greater than 143?\nNo\n0.0​\n9​\nThis number: 143.\nYes\n0.0​",
          "is_title": false,
          "page": 13,
          "text_length": 712
        },
        {
          "text": "(e) Qwen2.5-32B",
          "is_title": true,
          "page": 13,
          "text_length": 15
        },
        {
          "text": "(a) Qwen2.5-7B+GRPO\n(b) DeepSeek-V3.1",
          "is_title": true,
          "page": 13,
          "text_length": 37
        },
        {
          "text": "(c) Qwen2.5-1.5B\n(d) Qwen2.5-7B",
          "is_title": true,
          "page": 13,
          "text_length": 31
        },
        {
          "text": "(f) Qwen2.5-7B + SFT",
          "is_title": true,
          "page": 13,
          "text_length": 20
        },
        {
          "text": "Figure 5. Examples of the Guess Number performed by baseline models and SherlockLLM.",
          "is_title": false,
          "page": 13,
          "text_length": 84
        },
        {
          "text": "candidate after turn 6, the model proceeded with several\nrounds of unnecessary guessing before making the correct\nidentification.",
          "is_title": false,
          "page": 13,
          "text_length": 129
        },
        {
          "text": "Qwen2.5-32B is better than Qwen2.5-7B. It asks high-\nEIG questions about character attributes. However, its strat-\negy is demonstrably suboptimal and contains clear logical\ninconsistencies. It fails to perform the necessary logical de-",
          "is_title": false,
          "page": 13,
          "text_length": 235
        },
        {
          "text": "duction on the constrained candidate pool.",
          "is_title": false,
          "page": 13,
          "text_length": 42
        },
        {
          "text": "Similar to the behavior of SFT in Guess Number task,\nSFT agent can mimic questioning, but its policy is weak\nwhen the optimal action shifts from information gathering\nto logical inference.",
          "is_title": false,
          "page": 13,
          "text_length": 188
        },
        {
          "text": "3",
          "is_title": false,
          "page": 13,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 13,
      "page": 14,
      "content": "No.​ Question\nAnswer​ EIG\nNo.​ Question\nAnswer​ EIG\n1​\nIs the target candidate a male?\nYes​\n1.0​\n1​\nIs the target male?​\nYes​\n1.0​\n2​\nIs the target candidate's hair color blonde?\nNo​\n0.85\n2​\nDoes the target have a beard?​\nYes​\n0.91\n3​\nIs the target candidate's hairstyle bald?\nNo​\n1.0​\n3​\nDoes the target have a bald hairstyle?​\nNo​\n1.0​\n4​\nIs the target candidate's hair color red?\nNo​\n0.92​\n4​\nIs the target wearing glasses?​\nYes​\n0.91​\n5​\nIs the target candidate's hair color black?\nNo​\n1.0​\n5​\nIs the target's hair color white?​\nNo​\n1.0​\n\n6​\nIs the target candidate's hair color white and\nlong hairstyle?\nNo​\n1.0​\n6​\nI guess the target is C33.​\nYes​\n0.0​\n\n7​\nIs the target candidate C33? \nYes​\n0.0​\n\nNo.​ Question\nAnswer​ EIG\nNo.​ Question\nAnswer​ EIG\n1​\nIs this person's occupation a teacher?​\nNo​\n0.76​\n1​\nIs the target candidate a male?​\nYes​\n1.0​\n2​\nDoes the target candidate enjoy reading?​\nNo​\n0.59​\n2​\nIs the target candidate's hair color blonde?​\nNo​\n0.85​\n3​\nIs the target candidate wearing earrings?​\nYes​\n1.0​\n3​\nIs the target candidate's hair color white?​\nNo​\n0.89\n4​\nIs the target candidate a chef?​\nNo​\n0.91\n4​\nIs the target candidate's hair color red?​\nNo​\n0.91​\n5​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n5​\nIs the target candidate's hairstyle bald?​\nNo​\n1.0​\n6​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n6​\nIs the target candidate's hair color black?​\nNo​\n0.91\n7​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n7​\nIs the target candidate's hair color brown?​\nYes​\n0.0​\n8​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n8​\nIs the target candidate's occupation a student?​\nYes​\n0.0​\n9​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n9​\nIs the target candidate's hairstyle curly?​\nYes​\n0.0​\n10​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n10​\nIs the target candidate C03?​\nNo​\n0.0​\n11​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n11​\nIs the target candidate C09?​\nNo​\n0.0​\n12​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n12​\nIs the target candidate C14?​\nNo​\n0.0​\n13​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n13​\nIs the target candidate C23?​\nNo​\n0.0​\n14​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n14​\nIs the target candidate C33?​\nYes​\n0.0​\n15​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n16​\nIs the target candidate wearing earrings?​\nNo​\n​0.0\n\nNo.​ Question\nAnswer​ EIG\nNo.​ Question\nAnswer​ EIG\n1​\nIs the target candidate a male?\nYes\n1.0\n1​\nIs the person in question a man?\nYes\n1.0\n2​\nDoes the target candidate have a beard?\nYes\n0.92\n2​\nDoes the character have a beard?\nYes\n0.92\n\n3​\nDoes the target candidate have brown eyes?\nNo\n0.92\n3​\nIs it true that the individual's hair is either \nblonde or white?\nNo\n0.98\n\n4​\nDoes the target candidate have amber eyes?\nYes\n0.95\n4​\nCan you confirm if the person's eyes are brown? No\n0.97\n\n5​\nDoes the target candidate wear glasses?\nYes\n0.0​\n5​\nthe target is C36.\nNo\n0.92\n6​\nDoes the target candidate have brown hair?\nYes\n0.92\n6​\nthe target is C25.\nNo\n0.0​\n\n7​\nDoes the target candidate have a curly hairstyle? Yes\n1.0\n7​\nthe target is C23.\nNo\n0.0​\n\n8​\nC33\nYes\n0.0​\n8​\nthe target is C35.\nNo\n0.0​\n9​\nthe target is C33.\nYes\n1.0​\n\n(f) Qwen2.5-7B + SFT\n(e) Qwen2.5-32B\n\n(a) Qwen2.5-7B+GRPO\n(b) DeepSeek-V3.1\n\n(c) Qwen2.5-1.5B\n(d) Qwen2.5-7B\n\nFigure 6. Examples of the Guess Who task performed by baseline models and SherlockLLM. Each example presents the full dialogue\nhistory for identifying the target C33.\n\nB.3. CelebA Image Retrieval\n\nFigure Fig. 7 illustrates the model behaviors on the CelebA\nimage retrieval task.\nDeepSeek uses a diverse and ex-\nploratory strategy, similar to its performance on the Guess\nWho task. It successfully completes the task by querying a\nwide range of attributes. In contrast, Qwen2.5-1.5B exhibits\na failure by generating severe hallucinations after several\ninitial turns, repeatedly asking for attributes that do not exist\nin the dataset, such as “tattoos” and “scars”. Qwen2.5-7B\ndemonstrates a distinct failure mode: it continuously asks\nthe same questions. This repetitive behavior halts the re-\ntrieval process, ultimately leading to task failure.\n\nC. Guess Who Dataset\n\nWe design a set of 36 characters (C01–C36), with each char-\nacter annotated by 9 discrete attributes shown in Tab. 8. The\ncomplete character set is detailed in Tab. 9. To quantify the\ndistributional balance of each attribute, we use normalized\nentropy (NE), where a value of 1.0 indicates a perfectly uni-\nform distribution. As shown in Fig. 8, all attributes achieve\nNE = 1.0 except has beard. The lower NE for the has\nbeard attribute is a result of our modeling constraint that\nonly male characters can have beards.\n\n4",
      "content_length": 4688,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "No.​ Question\nAnswer​ EIG\nNo.​ Question\nAnswer​ EIG\n1​\nIs the target candidate a male?\nYes​\n1.0​\n1​\nIs the target male?​\nYes​\n1.0​\n2​\nIs the target candidate's hair color blonde?\nNo​\n0.85\n2​\nDoes the target have a beard?​\nYes​\n0.91\n3​\nIs the target candidate's hairstyle bald?\nNo​\n1.0​\n3​\nDoes the target have a bald hairstyle?​\nNo​\n1.0​\n4​\nIs the target candidate's hair color red?\nNo​\n0.92​\n4​\nIs the target wearing glasses?​\nYes​\n0.91​\n5​\nIs the target candidate's hair color black?\nNo​\n1.0​\n5​\nIs the target's hair color white?​\nNo​\n1.0​",
          "is_title": false,
          "page": 14,
          "text_length": 541
        },
        {
          "text": "6​\nIs the target candidate's hair color white and\nlong hairstyle?\nNo​\n1.0​\n6​\nI guess the target is C33.​\nYes​\n0.0​",
          "is_title": false,
          "page": 14,
          "text_length": 115
        },
        {
          "text": "7​\nIs the target candidate C33? \nYes​\n0.0​",
          "is_title": false,
          "page": 14,
          "text_length": 42
        },
        {
          "text": "No.​ Question\nAnswer​ EIG\nNo.​ Question\nAnswer​ EIG\n1​\nIs this person's occupation a teacher?​\nNo​\n0.76​\n1​\nIs the target candidate a male?​\nYes​\n1.0​\n2​\nDoes the target candidate enjoy reading?​\nNo​\n0.59​\n2​\nIs the target candidate's hair color blonde?​\nNo​\n0.85​\n3​\nIs the target candidate wearing earrings?​\nYes​\n1.0​\n3​\nIs the target candidate's hair color white?​\nNo​\n0.89\n4​\nIs the target candidate a chef?​\nNo​\n0.91\n4​\nIs the target candidate's hair color red?​\nNo​\n0.91​\n5​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n5​\nIs the target candidate's hairstyle bald?​\nNo​\n1.0​\n6​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n6​\nIs the target candidate's hair color black?​\nNo​\n0.91\n7​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n7​\nIs the target candidate's hair color brown?​\nYes​\n0.0​\n8​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n8​\nIs the target candidate's occupation a student?​\nYes​\n0.0​\n9​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n9​\nIs the target candidate's hairstyle curly?​\nYes​\n0.0​\n10​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n10​\nIs the target candidate C03?​\nNo​\n0.0​\n11​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n11​\nIs the target candidate C09?​\nNo​\n0.0​\n12​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n12​\nIs the target candidate C14?​\nNo​\n0.0​\n13​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n13​\nIs the target candidate C23?​\nNo​\n0.0​\n14​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n14​\nIs the target candidate C33?​\nYes​\n0.0​\n15​\nIs the target candidate wearing earrings?​\nYes​\n0.0​\n16​\nIs the target candidate wearing earrings?​\nNo​\n​0.0",
          "is_title": false,
          "page": 14,
          "text_length": 1661
        },
        {
          "text": "No.​ Question\nAnswer​ EIG\nNo.​ Question\nAnswer​ EIG\n1​\nIs the target candidate a male?\nYes\n1.0\n1​\nIs the person in question a man?\nYes\n1.0\n2​\nDoes the target candidate have a beard?\nYes\n0.92\n2​\nDoes the character have a beard?\nYes\n0.92",
          "is_title": false,
          "page": 14,
          "text_length": 235
        },
        {
          "text": "3​\nDoes the target candidate have brown eyes?\nNo\n0.92\n3​\nIs it true that the individual's hair is either \nblonde or white?\nNo\n0.98",
          "is_title": false,
          "page": 14,
          "text_length": 130
        },
        {
          "text": "4​\nDoes the target candidate have amber eyes?\nYes\n0.95\n4​\nCan you confirm if the person's eyes are brown? No\n0.97",
          "is_title": false,
          "page": 14,
          "text_length": 113
        },
        {
          "text": "5​\nDoes the target candidate wear glasses?\nYes\n0.0​\n5​\nthe target is C36.\nNo\n0.92\n6​\nDoes the target candidate have brown hair?\nYes\n0.92\n6​\nthe target is C25.\nNo\n0.0​",
          "is_title": false,
          "page": 14,
          "text_length": 166
        },
        {
          "text": "7​\nDoes the target candidate have a curly hairstyle? Yes\n1.0\n7​\nthe target is C23.\nNo\n0.0​",
          "is_title": false,
          "page": 14,
          "text_length": 90
        },
        {
          "text": "8​\nC33\nYes\n0.0​\n8​\nthe target is C35.\nNo\n0.0​\n9​\nthe target is C33.\nYes\n1.0​",
          "is_title": false,
          "page": 14,
          "text_length": 76
        },
        {
          "text": "(f) Qwen2.5-7B + SFT\n(e) Qwen2.5-32B",
          "is_title": true,
          "page": 14,
          "text_length": 36
        },
        {
          "text": "(a) Qwen2.5-7B+GRPO\n(b) DeepSeek-V3.1",
          "is_title": true,
          "page": 14,
          "text_length": 37
        },
        {
          "text": "(c) Qwen2.5-1.5B\n(d) Qwen2.5-7B",
          "is_title": true,
          "page": 14,
          "text_length": 31
        },
        {
          "text": "Figure 6. Examples of the Guess Who task performed by baseline models and SherlockLLM. Each example presents the full dialogue\nhistory for identifying the target C33.",
          "is_title": false,
          "page": 14,
          "text_length": 166
        },
        {
          "text": "B.3. CelebA Image Retrieval",
          "is_title": false,
          "page": 14,
          "text_length": 27
        },
        {
          "text": "Figure Fig. 7 illustrates the model behaviors on the CelebA\nimage retrieval task.\nDeepSeek uses a diverse and ex-\nploratory strategy, similar to its performance on the Guess\nWho task. It successfully completes the task by querying a\nwide range of attributes. In contrast, Qwen2.5-1.5B exhibits\na failure by generating severe hallucinations after several\ninitial turns, repeatedly asking for attributes that do not exist\nin the dataset, such as “tattoos” and “scars”. Qwen2.5-7B\ndemonstrates a distinct failure mode: it continuously asks\nthe same questions. This repetitive behavior halts the re-\ntrieval process, ultimately leading to task failure.",
          "is_title": false,
          "page": 14,
          "text_length": 648
        },
        {
          "text": "C. Guess Who Dataset",
          "is_title": false,
          "page": 14,
          "text_length": 20
        },
        {
          "text": "We design a set of 36 characters (C01–C36), with each char-\nacter annotated by 9 discrete attributes shown in Tab. 8. The\ncomplete character set is detailed in Tab. 9. To quantify the\ndistributional balance of each attribute, we use normalized\nentropy (NE), where a value of 1.0 indicates a perfectly uni-\nform distribution. As shown in Fig. 8, all attributes achieve\nNE = 1.0 except has beard. The lower NE for the has\nbeard attribute is a result of our modeling constraint that\nonly male characters can have beards.",
          "is_title": false,
          "page": 14,
          "text_length": 517
        },
        {
          "text": "4",
          "is_title": false,
          "page": 14,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 14,
      "page": 15,
      "content": "Target No.​\nQuestion\nAnswer\nRank​ E\n\n1​\nIs the person wearing eyeglasses?\nNo\n10\n2​\nIs the person male?\nNo\n51\n3​\nDoes the person have bangs?\nYes\n61\n4​\nDoes the person have heavy eye-makeup?\nNo\n57\n5​\nDoes the person have wavy hair?\nNo\n49\n6​\nDoes the person have an oval-shaped face?\nYes\n40\n7​\nDoes the person have a receding hairline?\nNo\n42\n8​\nDoes the person have a slightly open mouth?\nYes\n38\n9​\nDoes the person have a smile?\nYes\n13\n10​\nDoes the person have blond hair?\nNo\n4\n\n(a) DeepSeek model.\n\nTarget No.​\nQuestion\nAnswer\nRank\n\n1​\nIs the target face male?\nNo\n58\n2​\nDoes the target face wear glasses?\nNo\n51\n\n3​\nDoes the target face have any piercings?\nI can't answer \nthis question\n51\n\n4​\nIs the target face wearing jewelry?\nYes\n32\n\n5​\nDoes the target face have any tattoos?\nI can't answer \nthis question\n32\n\n6​\nDoes the target face have any scars?\nI can't answer \nthis question\n32\n\n7​\nDoes the target face have any birthmarks?\nI can't answer \nthis question\n32\n\n8​\nDoes the target face have any moles?\nI can't answer \nthis question\n32\n\n9​\nDoes the target face have any birthmarks or \nscars?\n\nI can't answer \nthis question\n32\n\n10​\nDoes the target face have any scars or \nbirthmarks?\n\nI can't answer \nthis question\n32\n\n11​\nDoes the target face have any tattoos or scars?\nI can't answer \nthis question\n32\n\n12​\nDoes the target face have any scars or \nbirthmarks?\n\nI can't answer \nthis question\n32\n\n13​\nDoes the target face have any birthmarks or \nscars?\n\nI can't answer \nthis question\n32\n\n14​\nDoes the target face have any birthmarks or \nscars?\n\nI can't answer \nthis question\n32\n\n15​\nDoes the target face have any scars or \nbirthmarks?\n\nI can't answer \nthis question\n32\n\n16​\nDoes the target face have any scars or \nbirthmarks?\n\nI can't answer \nthis question\n32\n\n17\nDoes the target face have any scars or \nbirthmarks?\n\nI can't answer \nthis question\n32\n\n18\nDoes the target face have any birthmarks or \nscars?\n\nI can't answer \nthis question\n32\n\n19\nDoes the target face have any scars or \nbirthmarks?\n\nI can't answer \nthis question\n32\n\n20\nDoes the target face have any scars or \nbirthmarks?\n\nI can't answer \nthis question\n32\n\n(b) Qwen2.5-1.5B model.\n\nTarget No.​\nQuestion\nAnswer\nRank\n\n1​\nIs the target a man or a woman?\nI can't answer \nthis question\n95\n\n2​\nIs the target wearing eyeglasses?\nNo\n83\n3​\nIs the target a man?\nNo\n43\n4​\nIs the target wearing eyeglasses?\nNo\n37\n5​\nIs the target wearing a hat?\nNo\n44\n6​\nIs the target wearing eyeglasses?\nNo\n44\n7​\nIs the target wearing a hat?\nNo\n54\n8​\nIs the target wearing lipstick?\nNo\n56\n9​\nIs the target wearing eyeglasses?\nNo\n56\n10​\nIs the target a woman?\nYes\n37\n11​\nIs the target wearing a hat?\nNo\n37\n12​\nIs the target wearing lipstick?\nNo\n40\n\n13​\nIs the target a young, attractive woman with an \noval-shaped face, clean-shaven?\nNo\n40\n\n14​\nIs the target a young woman with an oval-\nshaped face, clean-shaven?\nYes\n42\n\n15​\nIs the target wearing eyeglasses?\nNo\n42\n16​\nIs the target wearing a hat?\nNo\n43\n17\nIs the target wearing lipstick?\nNo\n42\n\n18\nIs the target a young woman with an oval-\nshaped face, clean-shaven?\nYes\n43\n\n19\nIs the target wearing a hat?\nNo\n44\n20\nIs the target wearing eyeglasses?\nNo\n44\n\n(c) Qwen2.5-7B model.\n\nFigure 7. Examples of three models on the CelebA image retrieval\ntask.\n\nTable 8. The Character Attribute Universe.\n\nAttribute\nPossible Values\n\ngender\nmale, female\nhair color\nred, blonde,\nblack, white,\nbrown\nhairstyle\ncurly, short,\nlong, bald\nwears glasses\nno, yes\nhas beard\nno, yes\neye color\namber, brown,\ngreen, blue\nhobby\nmovies,\nphotography,\nmusic, games,\nreading, sports\nwears earrings\nno, yes\noccupation\npolice, student,\nteacher, chef,\ndoctor\n\ngender\n\nhairstyle\n\nwears glasses\n\nwears earrings\n\neye\n\nhobby\n\nhair color\n\noccupation\n\nhas beard\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nNormalized entropy by attribute (1.0 = perfectly even)\n\nFigure 8. The Normalized Entropy of attributes.\n\nD. Implementation Details\n\nD.1. SFT for Guess Number task\n\nSynthetic Dialogue Dataset Construction. We construct\na synthetic dialogue dataset for the Guess Number task to\nsupport SFT. As detailed in Algorithm 2, the generation pro-\ncess for each sample begins by defining a random integer\ninterval C whose length is sampled uniformly between 100\nand 300. A hidden target number is then uniformly sampled\nfrom this interval. The current hypothesis space is repre-\nsented by the candidate set Ck. To manage task complexity,\nthe agent’s inquiries are restricted to six predefined ques-\ntion templates. These templates are odd, even, <, ≤, >,\nand ≥. At each turn, the algorithm calculates EIG for each\ntemplate over the current candidate set Ck. The question\n\n5",
      "content_length": 4604,
      "source_file": "2510.18659v1.pdf",
      "has_titles": true,
      "structured_blocks": [
        {
          "text": "Target No.​\nQuestion\nAnswer\nRank​ E",
          "is_title": false,
          "page": 15,
          "text_length": 35
        },
        {
          "text": "1​\nIs the person wearing eyeglasses?\nNo\n10\n2​\nIs the person male?\nNo\n51\n3​\nDoes the person have bangs?\nYes\n61\n4​\nDoes the person have heavy eye-makeup?\nNo\n57\n5​\nDoes the person have wavy hair?\nNo\n49\n6​\nDoes the person have an oval-shaped face?\nYes\n40\n7​\nDoes the person have a receding hairline?\nNo\n42\n8​\nDoes the person have a slightly open mouth?\nYes\n38\n9​\nDoes the person have a smile?\nYes\n13\n10​\nDoes the person have blond hair?\nNo\n4",
          "is_title": false,
          "page": 15,
          "text_length": 437
        },
        {
          "text": "(a) DeepSeek model.",
          "is_title": true,
          "page": 15,
          "text_length": 19
        },
        {
          "text": "Target No.​\nQuestion\nAnswer\nRank",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "1​\nIs the target face male?\nNo\n58\n2​\nDoes the target face wear glasses?\nNo\n51",
          "is_title": false,
          "page": 15,
          "text_length": 77
        },
        {
          "text": "3​\nDoes the target face have any piercings?\nI can't answer \nthis question\n51",
          "is_title": false,
          "page": 15,
          "text_length": 76
        },
        {
          "text": "4​\nIs the target face wearing jewelry?\nYes\n32",
          "is_title": false,
          "page": 15,
          "text_length": 45
        },
        {
          "text": "5​\nDoes the target face have any tattoos?\nI can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 74
        },
        {
          "text": "6​\nDoes the target face have any scars?\nI can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 72
        },
        {
          "text": "7​\nDoes the target face have any birthmarks?\nI can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 77
        },
        {
          "text": "8​\nDoes the target face have any moles?\nI can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 72
        },
        {
          "text": "9​\nDoes the target face have any birthmarks or \nscars?",
          "is_title": false,
          "page": 15,
          "text_length": 54
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "10​\nDoes the target face have any scars or \nbirthmarks?",
          "is_title": false,
          "page": 15,
          "text_length": 55
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "11​\nDoes the target face have any tattoos or scars?\nI can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 84
        },
        {
          "text": "12​\nDoes the target face have any scars or \nbirthmarks?",
          "is_title": false,
          "page": 15,
          "text_length": 55
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "13​\nDoes the target face have any birthmarks or \nscars?",
          "is_title": false,
          "page": 15,
          "text_length": 55
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "14​\nDoes the target face have any birthmarks or \nscars?",
          "is_title": false,
          "page": 15,
          "text_length": 55
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "15​\nDoes the target face have any scars or \nbirthmarks?",
          "is_title": false,
          "page": 15,
          "text_length": 55
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "16​\nDoes the target face have any scars or \nbirthmarks?",
          "is_title": false,
          "page": 15,
          "text_length": 55
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "17\nDoes the target face have any scars or \nbirthmarks?",
          "is_title": true,
          "page": 15,
          "text_length": 54
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "18\nDoes the target face have any birthmarks or \nscars?",
          "is_title": true,
          "page": 15,
          "text_length": 54
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "19\nDoes the target face have any scars or \nbirthmarks?",
          "is_title": true,
          "page": 15,
          "text_length": 54
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "20\nDoes the target face have any scars or \nbirthmarks?",
          "is_title": true,
          "page": 15,
          "text_length": 54
        },
        {
          "text": "I can't answer \nthis question\n32",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "(b) Qwen2.5-1.5B model.",
          "is_title": true,
          "page": 15,
          "text_length": 23
        },
        {
          "text": "Target No.​\nQuestion\nAnswer\nRank",
          "is_title": false,
          "page": 15,
          "text_length": 32
        },
        {
          "text": "1​\nIs the target a man or a woman?\nI can't answer \nthis question\n95",
          "is_title": false,
          "page": 15,
          "text_length": 67
        },
        {
          "text": "2​\nIs the target wearing eyeglasses?\nNo\n83\n3​\nIs the target a man?\nNo\n43\n4​\nIs the target wearing eyeglasses?\nNo\n37\n5​\nIs the target wearing a hat?\nNo\n44\n6​\nIs the target wearing eyeglasses?\nNo\n44\n7​\nIs the target wearing a hat?\nNo\n54\n8​\nIs the target wearing lipstick?\nNo\n56\n9​\nIs the target wearing eyeglasses?\nNo\n56\n10​\nIs the target a woman?\nYes\n37\n11​\nIs the target wearing a hat?\nNo\n37\n12​\nIs the target wearing lipstick?\nNo\n40",
          "is_title": false,
          "page": 15,
          "text_length": 433
        },
        {
          "text": "13​\nIs the target a young, attractive woman with an \noval-shaped face, clean-shaven?\nNo\n40",
          "is_title": false,
          "page": 15,
          "text_length": 90
        },
        {
          "text": "14​\nIs the target a young woman with an oval-\nshaped face, clean-shaven?\nYes\n42",
          "is_title": false,
          "page": 15,
          "text_length": 79
        },
        {
          "text": "15​\nIs the target wearing eyeglasses?\nNo\n42\n16​\nIs the target wearing a hat?\nNo\n43\n17\nIs the target wearing lipstick?\nNo\n42",
          "is_title": false,
          "page": 15,
          "text_length": 123
        },
        {
          "text": "18\nIs the target a young woman with an oval-\nshaped face, clean-shaven?\nYes\n43",
          "is_title": true,
          "page": 15,
          "text_length": 78
        },
        {
          "text": "19\nIs the target wearing a hat?\nNo\n44\n20\nIs the target wearing eyeglasses?\nNo\n44",
          "is_title": true,
          "page": 15,
          "text_length": 80
        },
        {
          "text": "(c) Qwen2.5-7B model.",
          "is_title": true,
          "page": 15,
          "text_length": 21
        },
        {
          "text": "Figure 7. Examples of three models on the CelebA image retrieval\ntask.",
          "is_title": false,
          "page": 15,
          "text_length": 70
        },
        {
          "text": "Table 8. The Character Attribute Universe.",
          "is_title": false,
          "page": 15,
          "text_length": 42
        },
        {
          "text": "Attribute\nPossible Values",
          "is_title": false,
          "page": 15,
          "text_length": 25
        },
        {
          "text": "gender\nmale, female\nhair color\nred, blonde,\nblack, white,\nbrown\nhairstyle\ncurly, short,\nlong, bald\nwears glasses\nno, yes\nhas beard\nno, yes\neye color\namber, brown,\ngreen, blue\nhobby\nmovies,\nphotography,\nmusic, games,\nreading, sports\nwears earrings\nno, yes\noccupation\npolice, student,\nteacher, chef,\ndoctor",
          "is_title": false,
          "page": 15,
          "text_length": 304
        },
        {
          "text": "gender",
          "is_title": false,
          "page": 15,
          "text_length": 6
        },
        {
          "text": "hairstyle",
          "is_title": false,
          "page": 15,
          "text_length": 9
        },
        {
          "text": "wears glasses",
          "is_title": false,
          "page": 15,
          "text_length": 13
        },
        {
          "text": "wears earrings",
          "is_title": false,
          "page": 15,
          "text_length": 14
        },
        {
          "text": "eye",
          "is_title": false,
          "page": 15,
          "text_length": 3
        },
        {
          "text": "hobby",
          "is_title": false,
          "page": 15,
          "text_length": 5
        },
        {
          "text": "hair color",
          "is_title": false,
          "page": 15,
          "text_length": 10
        },
        {
          "text": "occupation",
          "is_title": false,
          "page": 15,
          "text_length": 10
        },
        {
          "text": "has beard",
          "is_title": false,
          "page": 15,
          "text_length": 9
        },
        {
          "text": "0.0",
          "is_title": false,
          "page": 15,
          "text_length": 3
        },
        {
          "text": "0.2",
          "is_title": false,
          "page": 15,
          "text_length": 3
        },
        {
          "text": "0.4",
          "is_title": false,
          "page": 15,
          "text_length": 3
        },
        {
          "text": "0.6",
          "is_title": false,
          "page": 15,
          "text_length": 3
        },
        {
          "text": "0.8",
          "is_title": false,
          "page": 15,
          "text_length": 3
        },
        {
          "text": "1.0",
          "is_title": false,
          "page": 15,
          "text_length": 3
        },
        {
          "text": "Normalized entropy by attribute (1.0 = perfectly even)",
          "is_title": false,
          "page": 15,
          "text_length": 54
        },
        {
          "text": "Figure 8. The Normalized Entropy of attributes.",
          "is_title": false,
          "page": 15,
          "text_length": 47
        },
        {
          "text": "D. Implementation Details",
          "is_title": false,
          "page": 15,
          "text_length": 25
        },
        {
          "text": "D.1. SFT for Guess Number task",
          "is_title": false,
          "page": 15,
          "text_length": 30
        },
        {
          "text": "Synthetic Dialogue Dataset Construction. We construct\na synthetic dialogue dataset for the Guess Number task to\nsupport SFT. As detailed in Algorithm 2, the generation pro-\ncess for each sample begins by defining a random integer\ninterval C whose length is sampled uniformly between 100\nand 300. A hidden target number is then uniformly sampled\nfrom this interval. The current hypothesis space is repre-\nsented by the candidate set Ck. To manage task complexity,\nthe agent’s inquiries are restricted to six predefined ques-\ntion templates. These templates are odd, even, <, ≤, >,\nand ≥. At each turn, the algorithm calculates EIG for each\ntemplate over the current candidate set Ck. The question",
          "is_title": false,
          "page": 15,
          "text_length": 695
        },
        {
          "text": "5",
          "is_title": false,
          "page": 15,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 15,
      "page": 16,
      "content": "Table 9. Guess Who character data.\n\nname gender hair color hairstyle wears glasses has beard eye\nhobby\nwears earrings occupation\n\nC01\nmale\nred\ncurly\nno\nno\namber movies\nno\npolice\nC02\nmale\nblonde\nshort\nno\nno\nbrown photography no\nstudent\nC03\nmale\nblack\nlong\nno\nyes\nbrown music\nyes\nteacher\nC04\nfemale\nwhite\nlong\nyes\nno\namber games\nno\npolice\nC05\nfemale\nbrown\nshort\nno\nno\ngreen\nreading\nyes\nstudent\nC06\nfemale\nbrown\nlong\nno\nno\ngreen\nsports\nno\npolice\nC07\nfemale\nblack\ncurly\nyes\nno\nblue\nsports\nno\nchef\nC08\nfemale\nblonde\nshort\nyes\nno\ngreen\nmusic\nyes\nchef\nC09\nfemale\nblack\nlong\nyes\nno\nblue\nreading\nyes\nteacher\nC10\nfemale\nwhite\nshort\nno\nno\namber music\nno\nchef\nC11\nfemale\nblack\nshort\nyes\nno\nblue\ngames\nno\nchef\nC12\nmale\nred\nbald\nno\nno\nblue\nreading\nno\nchef\nC13\nmale\nblonde\nbald\nyes\nyes\namber music\nno\ndoctor\nC14\nfemale\nred\ncurly\nyes\nno\nblue\nphotography yes\nstudent\nC15\nmale\nblonde\nbald\nno\nyes\nbrown reading\nyes\npolice\nC16\nfemale\nred\nlong\nyes\nno\nbrown music\nno\ndoctor\nC17\nmale\nwhite\nbald\nyes\nyes\nblue\nreading\nno\nteacher\nC18\nfemale\nwhite\nlong\nno\nno\ngreen\ngames\nyes\nstudent\nC19\nmale\nwhite\nbald\nyes\nyes\ngreen\ngames\nyes\nteacher\nC20\nfemale\nred\nshort\nno\nno\ngreen\nreading\nyes\ndoctor\nC21\nmale\nwhite\nbald\nyes\nno\ngreen\nphotography yes\nteacher\nC22\nfemale\nblonde\nlong\nyes\nno\namber movies\nno\nstudent\nC23\nmale\nblack\nshort\nno\nno\nbrown photography yes\npolice\nC24\nfemale\nbrown\ncurly\nyes\nno\nblue\nmusic\nno\ndoctor\nC25\nmale\nblonde\ncurly\nno\nyes\nblue\nmovies\nyes\npolice\nC26\nfemale\nred\nshort\nno\nno\ngreen\nsports\nno\ndoctor\nC27\nfemale\nbrown\ncurly\nno\nno\namber movies\nyes\nchef\nC28\nfemale\nwhite\nshort\nno\nno\nbrown games\nyes\nchef\nC29\nmale\nbrown\nbald\nyes\nyes\namber movies\nyes\nteacher\nC30\nfemale\nblack\ncurly\nno\nno\namber sports\nno\nteacher\nC31\nmale\nblack\nbald\nyes\nno\nbrown movies\nno\npolice\nC32\nmale\nwhite\nlong\nyes\nyes\nblue\nphotography yes\nteacher\nC33\nmale\nbrown\ncurly\nyes\nyes\namber photography yes\nstudent\nC34\nmale\nbrown\nbald\nyes\nyes\nbrown sports\nno\nstudent\nC35\nmale\nblonde\nlong\nno\nyes\nbrown games\nyes\ndoctor\nC36\nmale\nred\ncurly\nno\nyes\ngreen\nsports\nno\ndoctor\n\nwith the highest EIG is subsequently selected. To improve\ndiversity, if multiple questions score within a near-optimal\nrange defined by a tolerance of τ = 10−4, one is sampled\nuniformly from that set. The agent makes a definitive guess\nonce the candidate set size Ck is reduced to two or fewer.\nThis procedure effectively implements a binary search pol-\nicy over the hypothesis space.\n\nFor training purposes, each complete dialogue trajectory\nis expanded into multiple instances. Specifically, a dialogue\nof n turns is segmented into n separate training examples,\nwhere each example includes the full dialogue history up\nto that turn. The resulting corpus contains 16,804 training\n\ninstances and 1,277 validation instances.\n\nSupervised Finetuning.\n\nWe employ SFT on the Qwen2.5-7B-Instruct model with\nLoRA. We set the rank to 16 and alpha to 32. The model is\ntrained for 1 epoch using the AdamW optimizer. The opti-\nmization process incorporates a weight decay of 0.1 and a\nlearning rate of 5×10−6, managed by a cosine learning rate\nschedule. The training objective is a standard autoregressive\nloss, computed exclusively on the tokens generated by the\nmodel.\n\n6",
      "content_length": 3168,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "Table 9. Guess Who character data.",
          "is_title": false,
          "page": 16,
          "text_length": 34
        },
        {
          "text": "name gender hair color hairstyle wears glasses has beard eye\nhobby\nwears earrings occupation",
          "is_title": false,
          "page": 16,
          "text_length": 92
        },
        {
          "text": "C01\nmale\nred\ncurly\nno\nno\namber movies\nno\npolice\nC02\nmale\nblonde\nshort\nno\nno\nbrown photography no\nstudent\nC03\nmale\nblack\nlong\nno\nyes\nbrown music\nyes\nteacher\nC04\nfemale\nwhite\nlong\nyes\nno\namber games\nno\npolice\nC05\nfemale\nbrown\nshort\nno\nno\ngreen\nreading\nyes\nstudent\nC06\nfemale\nbrown\nlong\nno\nno\ngreen\nsports\nno\npolice\nC07\nfemale\nblack\ncurly\nyes\nno\nblue\nsports\nno\nchef\nC08\nfemale\nblonde\nshort\nyes\nno\ngreen\nmusic\nyes\nchef\nC09\nfemale\nblack\nlong\nyes\nno\nblue\nreading\nyes\nteacher\nC10\nfemale\nwhite\nshort\nno\nno\namber music\nno\nchef\nC11\nfemale\nblack\nshort\nyes\nno\nblue\ngames\nno\nchef\nC12\nmale\nred\nbald\nno\nno\nblue\nreading\nno\nchef\nC13\nmale\nblonde\nbald\nyes\nyes\namber music\nno\ndoctor\nC14\nfemale\nred\ncurly\nyes\nno\nblue\nphotography yes\nstudent\nC15\nmale\nblonde\nbald\nno\nyes\nbrown reading\nyes\npolice\nC16\nfemale\nred\nlong\nyes\nno\nbrown music\nno\ndoctor\nC17\nmale\nwhite\nbald\nyes\nyes\nblue\nreading\nno\nteacher\nC18\nfemale\nwhite\nlong\nno\nno\ngreen\ngames\nyes\nstudent\nC19\nmale\nwhite\nbald\nyes\nyes\ngreen\ngames\nyes\nteacher\nC20\nfemale\nred\nshort\nno\nno\ngreen\nreading\nyes\ndoctor\nC21\nmale\nwhite\nbald\nyes\nno\ngreen\nphotography yes\nteacher\nC22\nfemale\nblonde\nlong\nyes\nno\namber movies\nno\nstudent\nC23\nmale\nblack\nshort\nno\nno\nbrown photography yes\npolice\nC24\nfemale\nbrown\ncurly\nyes\nno\nblue\nmusic\nno\ndoctor\nC25\nmale\nblonde\ncurly\nno\nyes\nblue\nmovies\nyes\npolice\nC26\nfemale\nred\nshort\nno\nno\ngreen\nsports\nno\ndoctor\nC27\nfemale\nbrown\ncurly\nno\nno\namber movies\nyes\nchef\nC28\nfemale\nwhite\nshort\nno\nno\nbrown games\nyes\nchef\nC29\nmale\nbrown\nbald\nyes\nyes\namber movies\nyes\nteacher\nC30\nfemale\nblack\ncurly\nno\nno\namber sports\nno\nteacher\nC31\nmale\nblack\nbald\nyes\nno\nbrown movies\nno\npolice\nC32\nmale\nwhite\nlong\nyes\nyes\nblue\nphotography yes\nteacher\nC33\nmale\nbrown\ncurly\nyes\nyes\namber photography yes\nstudent\nC34\nmale\nbrown\nbald\nyes\nyes\nbrown sports\nno\nstudent\nC35\nmale\nblonde\nlong\nno\nyes\nbrown games\nyes\ndoctor\nC36\nmale\nred\ncurly\nno\nyes\ngreen\nsports\nno\ndoctor",
          "is_title": false,
          "page": 16,
          "text_length": 1874
        },
        {
          "text": "with the highest EIG is subsequently selected. To improve\ndiversity, if multiple questions score within a near-optimal\nrange defined by a tolerance of τ = 10−4, one is sampled\nuniformly from that set. The agent makes a definitive guess\nonce the candidate set size Ck is reduced to two or fewer.\nThis procedure effectively implements a binary search pol-\nicy over the hypothesis space.",
          "is_title": false,
          "page": 16,
          "text_length": 384
        },
        {
          "text": "For training purposes, each complete dialogue trajectory\nis expanded into multiple instances. Specifically, a dialogue\nof n turns is segmented into n separate training examples,\nwhere each example includes the full dialogue history up\nto that turn. The resulting corpus contains 16,804 training",
          "is_title": false,
          "page": 16,
          "text_length": 294
        },
        {
          "text": "instances and 1,277 validation instances.",
          "is_title": false,
          "page": 16,
          "text_length": 41
        },
        {
          "text": "Supervised Finetuning.",
          "is_title": false,
          "page": 16,
          "text_length": 22
        },
        {
          "text": "We employ SFT on the Qwen2.5-7B-Instruct model with\nLoRA. We set the rank to 16 and alpha to 32. The model is\ntrained for 1 epoch using the AdamW optimizer. The opti-\nmization process incorporates a weight decay of 0.1 and a\nlearning rate of 5×10−6, managed by a cosine learning rate\nschedule. The training objective is a standard autoregressive\nloss, computed exclusively on the tokens generated by the\nmodel.",
          "is_title": false,
          "page": 16,
          "text_length": 410
        },
        {
          "text": "6",
          "is_title": false,
          "page": 16,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 16,
      "page": 17,
      "content": "Algorithm 2 Optimal Dialogue Synthesis for Guess Number task\n\nRequire: M: number of dialogues to generate\nRequire: Tmax: maximum turns per dialogue (e.g., 16)\nRequire: τ: tolerance for near-optimal question sampling (e.g., 0.0001)\nRequire: P: six type of question templates (odd, even, <, ≤, >, ≥)\nEnsure: D: unfolded dialogue dataset\n\n1: D ←∅\n2: for m = 1 to M do\n3:\nstart ←RamdomInt(100, 500);\nN ←RandomInt(100, 300)\n4:\nC =←{start, start + 1, . . . , start + N −1}; ctarget ←Sample(C)\n▷Select a secret target\n5:\nCk ←C\n▷Initialize candidate pool with all characters\n\n6:\nH ←[ ]\n7:\nwhile |C| > 1 and |H| < Tmax do\n8:\nif |Ck| <= 2 then\n9:\ng ←RandomSample(Ck)\n10:\nif g = TRUE then\n11:\nAppend (GuessText(g), yes) to H; break\n\n12:\nelse\n13:\nAppend (GuessText(g), no) to H\n14:\nCk ←{c ∈Ck | c /∈g}; continue\n15:\nend if\n16:\nend if\n17:\nQscores ←∅\n▷Store scores for all potential questions this turn\n\n18:\nfor each question type p ∈P do\n▷Question scoring phase\n19:\nq ←(p, Median(Ck))\n▷Create a question using the median value of Ck\n20:\nscore ←EIG(Ck, q)\n▷Calculate EIG\n21:\nAdd (q, score) to Qscores\n22:\nend for\n\n23:\nQbest ←{q | (q, score) ∈Qscores and scoremax −score ≤τ}\n24:\nqchosen ←RandomSample(Qbest)\n▷Select from near-optimal questions\n25:\nanswer = Answer(q, ctarget)\n26:\nAppend (q, answer) to H; Update Ck\n27:\nend while\n28:\nD ←UnfoldTrajectories(H)\n\n29: end for\n30: return D\n\n5\n6\n7\n8\nDailogue Turns\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\nFrequency\n\n589\n\n2629\n\n779\n\n2\n\nAverage Turn: 6.05\nFrequency\n\nFigure 9. Distribution of Dialogue Turns.\n\nD.2. SFT for Guess Who task\n\nSynthetic Dialogue Dataset Construction. We construct\na synthetic dialogue dataset for Guess Who task. Follow-\ning the procedure outlined in Algorithm 3, each dialogue\ncommences by sampling a board with 36 characters, each\npossessing a set of discrete attributes. A single character is\nthen selected as the hidden target. The system maintains a\ncandidate set Ck which is progressively reduced after each\nturn. At every step, the system scores all unasked single-\nvalue questions and selects one to pose.\nOur approach employs a greedy policy based on EIG.\nFor any single-value question q = (a, v), we compute its\nEIG(q) from the number of candidates Ck. The question\nwith the highest EIG score is subsequently chosen. To in-\ntroduce strategic diversity, if multiple questions fall within\n\n7",
      "content_length": 2347,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "Algorithm 2 Optimal Dialogue Synthesis for Guess Number task",
          "is_title": false,
          "page": 17,
          "text_length": 60
        },
        {
          "text": "Require: M: number of dialogues to generate\nRequire: Tmax: maximum turns per dialogue (e.g., 16)\nRequire: τ: tolerance for near-optimal question sampling (e.g., 0.0001)\nRequire: P: six type of question templates (odd, even, <, ≤, >, ≥)\nEnsure: D: unfolded dialogue dataset",
          "is_title": false,
          "page": 17,
          "text_length": 272
        },
        {
          "text": "1: D ←∅\n2: for m = 1 to M do\n3:\nstart ←RamdomInt(100, 500);\nN ←RandomInt(100, 300)\n4:\nC =←{start, start + 1, . . . , start + N −1}; ctarget ←Sample(C)\n▷Select a secret target\n5:\nCk ←C\n▷Initialize candidate pool with all characters",
          "is_title": false,
          "page": 17,
          "text_length": 230
        },
        {
          "text": "6:\nH ←[ ]\n7:\nwhile |C| > 1 and |H| < Tmax do\n8:\nif |Ck| <= 2 then\n9:\ng ←RandomSample(Ck)\n10:\nif g = TRUE then\n11:\nAppend (GuessText(g), yes) to H; break",
          "is_title": false,
          "page": 17,
          "text_length": 152
        },
        {
          "text": "12:\nelse\n13:\nAppend (GuessText(g), no) to H\n14:\nCk ←{c ∈Ck | c /∈g}; continue\n15:\nend if\n16:\nend if\n17:\nQscores ←∅\n▷Store scores for all potential questions this turn",
          "is_title": false,
          "page": 17,
          "text_length": 166
        },
        {
          "text": "18:\nfor each question type p ∈P do\n▷Question scoring phase\n19:\nq ←(p, Median(Ck))\n▷Create a question using the median value of Ck\n20:\nscore ←EIG(Ck, q)\n▷Calculate EIG\n21:\nAdd (q, score) to Qscores\n22:\nend for",
          "is_title": false,
          "page": 17,
          "text_length": 208
        },
        {
          "text": "23:\nQbest ←{q | (q, score) ∈Qscores and scoremax −score ≤τ}\n24:\nqchosen ←RandomSample(Qbest)\n▷Select from near-optimal questions\n25:\nanswer = Answer(q, ctarget)\n26:\nAppend (q, answer) to H; Update Ck\n27:\nend while\n28:\nD ←UnfoldTrajectories(H)",
          "is_title": false,
          "page": 17,
          "text_length": 242
        },
        {
          "text": "29: end for\n30: return D",
          "is_title": false,
          "page": 17,
          "text_length": 24
        },
        {
          "text": "5\n6\n7\n8\nDailogue Turns",
          "is_title": false,
          "page": 17,
          "text_length": 22
        },
        {
          "text": "0",
          "is_title": false,
          "page": 17,
          "text_length": 1
        },
        {
          "text": "500",
          "is_title": false,
          "page": 17,
          "text_length": 3
        },
        {
          "text": "1000",
          "is_title": false,
          "page": 17,
          "text_length": 4
        },
        {
          "text": "1500",
          "is_title": false,
          "page": 17,
          "text_length": 4
        },
        {
          "text": "2000",
          "is_title": false,
          "page": 17,
          "text_length": 4
        },
        {
          "text": "2500",
          "is_title": false,
          "page": 17,
          "text_length": 4
        },
        {
          "text": "Frequency",
          "is_title": false,
          "page": 17,
          "text_length": 9
        },
        {
          "text": "589",
          "is_title": false,
          "page": 17,
          "text_length": 3
        },
        {
          "text": "2629",
          "is_title": false,
          "page": 17,
          "text_length": 4
        },
        {
          "text": "779",
          "is_title": false,
          "page": 17,
          "text_length": 3
        },
        {
          "text": "2",
          "is_title": false,
          "page": 17,
          "text_length": 1
        },
        {
          "text": "Average Turn: 6.05\nFrequency",
          "is_title": false,
          "page": 17,
          "text_length": 28
        },
        {
          "text": "Figure 9. Distribution of Dialogue Turns.",
          "is_title": false,
          "page": 17,
          "text_length": 41
        },
        {
          "text": "D.2. SFT for Guess Who task",
          "is_title": false,
          "page": 17,
          "text_length": 27
        },
        {
          "text": "Synthetic Dialogue Dataset Construction. We construct\na synthetic dialogue dataset for Guess Who task. Follow-\ning the procedure outlined in Algorithm 3, each dialogue\ncommences by sampling a board with 36 characters, each\npossessing a set of discrete attributes. A single character is\nthen selected as the hidden target. The system maintains a\ncandidate set Ck which is progressively reduced after each\nturn. At every step, the system scores all unasked single-\nvalue questions and selects one to pose.\nOur approach employs a greedy policy based on EIG.\nFor any single-value question q = (a, v), we compute its\nEIG(q) from the number of candidates Ck. The question\nwith the highest EIG score is subsequently chosen. To in-\ntroduce strategic diversity, if multiple questions fall within",
          "is_title": false,
          "page": 17,
          "text_length": 786
        },
        {
          "text": "7",
          "is_title": false,
          "page": 17,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 17,
      "page": 18,
      "content": "Algorithm 3 Optimal Dialogue Synthesis for Guess Who task\n\nRequire: M: Number of dialogues to generate\nRequire: K: Candidate threshold to start guessing (e.g., 2)\nRequire: A: Set of all possible attributes (e.g., hair color, gender)\nRequire: Va: Set of possible values for attribute a ∈A\nRequire: P: A pre-computed mapping of canonical questions to paraphrases\nRequire: τ: Score tolerance for near-optimal question sampling (e.g., 0.0001)\nRequire: Tmax: Maximum turns per dialogue (e.g., 16)\nEnsure: D: A dataset of unfolded dialogue trajectories\n\n1: D ←∅\n▷Initialize the final dataset\n2: for m = 1 to M do\n▷Loop to generate M full dialogues\n\n3:\nC ←GenerateRandomCharacterSet(A, {Va})\n▷Create a new game board\n4:\nctarget ←RandomSample(C)\n▷Select a secret target character\n5:\nCk ←C\n▷Initialize candidate pool with all characters\n6:\nH ←[]\n▷Initialize empty dialogue history for this game\n7:\nwhile |Ck| > 1 and |H| < Tmax do\n8:\nif |Ck| ≤K then\n▷Guessing phase\n\n9:\ncguess ←RandomSample(Ck)\n10:\nUpdate H with final guess and outcome.\n11:\nbreak\n▷End the current game\n12:\nend if\n13:\nQscores ←∅\n▷Store scores for all potential questions this turn\n\n14:\nfor each attribute a ∈A do\n▷Question scoring phase\n15:\nfor each value v ∈Va do\n16:\nq ←(a, v)\n17:\nif q was not asked before in H then\n18:\nscore ←EIG(Ck, q)\n▷Calculate EIG\n19:\nAdd (q, score) to Qscores\n20:\nend if\n21:\nend for\n22:\nend for\n23:\nQbest ←{q | (q, score) ∈Qscores and scoremax −score ≤τ}\n24:\nqchosen ←RandomSample(Qbest)\n▷Select from near-optimal questions\n25:\nqparaphrased ←GPT(qchosen)\n▷Linguistic variation\n\n26:\nanswer ←(ctarget[qchosen.a] == qchosen.v)\n▷Get true answer from target\n27:\nAppend (qparaphrased, answer) to H\n28:\nif answer is TRUE then\n▷Update candidate pool\n29:\nCk ←{c ∈Ck | c[qchosen.a] = qchosen.v}\n30:\nelse\n31:\nCk ←{c ∈Ck | c[qchosen.a] ̸= qchosen.v}\n\n32:\nend if\n33:\nend while\n34:\nD ←UnfoldTrajectories(H)\n35: end for\n36: return D\n\na near-optimal band defined by a tolerance τ, which is set\nto 0.01% in our experiments, we sample uniformly among\nthem.\nThe system provides a binary response based on\nthe ground-truth target and prunes the candidate set accord-\ningly. The dialogue stops and makes a final guess when the\nnumber of remaining candidates is at most K (e.g., K = 2).\n\nThe distribution of dialogue lengths is presented in Figure 9,\nshowing a mean of 6.05 turns. This policy locally maxi-\nmizes information gain to rapidly compress the hypothesis\nspace, while mild randomization prevents policy degener-\nacy.\n\nTo enhance the scale and linguistic diversity of the\n\n8",
      "content_length": 2545,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "Algorithm 3 Optimal Dialogue Synthesis for Guess Who task",
          "is_title": false,
          "page": 18,
          "text_length": 57
        },
        {
          "text": "Require: M: Number of dialogues to generate\nRequire: K: Candidate threshold to start guessing (e.g., 2)\nRequire: A: Set of all possible attributes (e.g., hair color, gender)\nRequire: Va: Set of possible values for attribute a ∈A\nRequire: P: A pre-computed mapping of canonical questions to paraphrases\nRequire: τ: Score tolerance for near-optimal question sampling (e.g., 0.0001)\nRequire: Tmax: Maximum turns per dialogue (e.g., 16)\nEnsure: D: A dataset of unfolded dialogue trajectories",
          "is_title": false,
          "page": 18,
          "text_length": 487
        },
        {
          "text": "1: D ←∅\n▷Initialize the final dataset\n2: for m = 1 to M do\n▷Loop to generate M full dialogues",
          "is_title": false,
          "page": 18,
          "text_length": 93
        },
        {
          "text": "3:\nC ←GenerateRandomCharacterSet(A, {Va})\n▷Create a new game board\n4:\nctarget ←RandomSample(C)\n▷Select a secret target character\n5:\nCk ←C\n▷Initialize candidate pool with all characters\n6:\nH ←[]\n▷Initialize empty dialogue history for this game\n7:\nwhile |Ck| > 1 and |H| < Tmax do\n8:\nif |Ck| ≤K then\n▷Guessing phase",
          "is_title": false,
          "page": 18,
          "text_length": 313
        },
        {
          "text": "9:\ncguess ←RandomSample(Ck)\n10:\nUpdate H with final guess and outcome.\n11:\nbreak\n▷End the current game\n12:\nend if\n13:\nQscores ←∅\n▷Store scores for all potential questions this turn",
          "is_title": false,
          "page": 18,
          "text_length": 180
        },
        {
          "text": "14:\nfor each attribute a ∈A do\n▷Question scoring phase\n15:\nfor each value v ∈Va do\n16:\nq ←(a, v)\n17:\nif q was not asked before in H then\n18:\nscore ←EIG(Ck, q)\n▷Calculate EIG\n19:\nAdd (q, score) to Qscores\n20:\nend if\n21:\nend for\n22:\nend for\n23:\nQbest ←{q | (q, score) ∈Qscores and scoremax −score ≤τ}\n24:\nqchosen ←RandomSample(Qbest)\n▷Select from near-optimal questions\n25:\nqparaphrased ←GPT(qchosen)\n▷Linguistic variation",
          "is_title": false,
          "page": 18,
          "text_length": 420
        },
        {
          "text": "26:\nanswer ←(ctarget[qchosen.a] == qchosen.v)\n▷Get true answer from target\n27:\nAppend (qparaphrased, answer) to H\n28:\nif answer is TRUE then\n▷Update candidate pool\n29:\nCk ←{c ∈Ck | c[qchosen.a] = qchosen.v}\n30:\nelse\n31:\nCk ←{c ∈Ck | c[qchosen.a] ̸= qchosen.v}",
          "is_title": false,
          "page": 18,
          "text_length": 259
        },
        {
          "text": "32:\nend if\n33:\nend while\n34:\nD ←UnfoldTrajectories(H)\n35: end for\n36: return D",
          "is_title": false,
          "page": 18,
          "text_length": 78
        },
        {
          "text": "a near-optimal band defined by a tolerance τ, which is set\nto 0.01% in our experiments, we sample uniformly among\nthem.\nThe system provides a binary response based on\nthe ground-truth target and prunes the candidate set accord-\ningly. The dialogue stops and makes a final guess when the\nnumber of remaining candidates is at most K (e.g., K = 2).",
          "is_title": false,
          "page": 18,
          "text_length": 345
        },
        {
          "text": "The distribution of dialogue lengths is presented in Figure 9,\nshowing a mean of 6.05 turns. This policy locally maxi-\nmizes information gain to rapidly compress the hypothesis\nspace, while mild randomization prevents policy degener-\nacy.",
          "is_title": false,
          "page": 18,
          "text_length": 238
        },
        {
          "text": "To enhance the scale and linguistic diversity of the",
          "is_title": false,
          "page": 18,
          "text_length": 52
        },
        {
          "text": "8",
          "is_title": false,
          "page": 18,
          "text_length": 1
        }
      ]
    },
    {
      "document_index": 18,
      "page": 19,
      "content": "dataset, we generate dialogues by repeatedly resampling\nboards and targets. We then use GPT-4o to generate se-\nmantically equivalent paraphrases for each question. Same\nas the guess number task, each full dialogue trajectory is\nexpanded into multiple training samples. The final corpus\nconsists of 24,105 training instances and 1,034 validation\ninstances.\nSupervised Finetuning. The SFT training procedure and\nhyperparameters are identical to those used for the Guess\nNumber task (Section D.1).\n\nD.3. Fine-tuning BLIP\n\nSynthetic Dialogue Dataset Construction. To address the\nabsence of a public corpus for our task, we employ a pro-\ngrammatic synthesis approach to generate a custom-built\ndataset. The CelebA dataset served as an ideal foundation,\nas its binary attributes are directly applicable for generating\nthe yes/no dialogues.\nOur synthesis pipeline begins by creating a question-\nanswer (QA) pair for each of the 40 attributes associated\nwith an image. To enhance linguistic diversity, we manu-\nally prepare three distinct phrasings for each attribute’s cor-\nresponding question. During generation, one of these three\ntemplates is selected at random. A positive QA pair is gen-\nerated if the attribute value is 1, while a negative QA pair is\ngenerated if the value is −1.\nTo construct a full dialogue, we first select images with\nno more than 20 positive attributes. The corresponding pos-\nitive QA pairs are then randomly shuffled to form the dia-\nlogue’s core. Rather than padding all dialogues to a fixed\nlength, we use a stochastic process to determine the num-\nber of negative questions. For an image Ii with m pos-\nitive attributes, we sample an integer x from the interval\n[0, 20 −m] and then randomly select x negative QA pairs.\nSubsequently, we apply a negative sampling strategy to in-\nsert these negative QA pairs among the positive ones. This\napproach preferentially places a related negative question\nbefore a positive one (e.g., asking about “Blond Hair” or\n“Gray Hair” before “Black Hair”). This method is designed\nto simulate a realistic process of elimination, where the\nscope of inquiry is progressively narrowed to identify the\ntarget attributes. In total, we generate 40,000 dialogues us-\ning this procedure. Fig. 10 shows an example of a generated\ndialogue.\nSupervised Fine-tuning. We use the official implementa-\ntion 1 to fine-tune the BLIP base model. The model is pre-\ntrained on the Flickr30k dataset [46]. We use the AdamW\noptimizer with an initial learning rate of 5 × 10−6, a weight\ndecay of 0.05, and a training batch size of 32. While the\nmodel is fine-tuned for a total of 12 epochs, the checkpoint\nfrom the 9th epoch is selected for the final evaluation, as it\nachieves optimal retrieval performance on the validation set.\n\n1BLIP codebase https://github.com/salesforce/BLIP\n\nNo. Question\nAnswer\n1\nIs the target male?\nYes\n2\nIs the target gray hair?\nNo\n3\nIs the target's hair blond?\nNo\n4\nDoes the target have brown hair?\nYes\n5\nIs the target wavy hair?\nYes\n6\nDoes the target have under-eye bags?\nNo\n7\nIs the target of a young age?\nYes\n8\nDoes the target have no facial hair?\nYes\n9\nIs the target arched eyebrows?\nNo\n10\nIs the target's mouth slightly open?\nYes\n\nFigure 10. An example of synthetic dialogue.\n\nWe implement data augmentation to improve the model’s\ngeneralization and robustness. For image inputs, we ap-\nply standard transformations, including random horizontal\nflipping, adjustments to brightness and sharpness, random\ntranslation and rotation, and normalization. On the text side,\nwe apply an augmentation method termed ”Random Round\nRetention”. This method randomly truncates dialogues, re-\ntaining only the first N > 0 QA rounds as input. The objec-\ntive is to enhance the model’s adaptability to conversations\nof varying lengths. Additionally, since our dialogue texts\nsignificantly exceed the default 50-character limit in the of-\nficial BLIP implementation, we expand the maximum input\ntext length to 300. This modification ensures that the com-\nplete conversational context is processed by the model.\n\n9",
      "content_length": 4060,
      "source_file": "2510.18659v1.pdf",
      "has_titles": false,
      "structured_blocks": [
        {
          "text": "dataset, we generate dialogues by repeatedly resampling\nboards and targets. We then use GPT-4o to generate se-\nmantically equivalent paraphrases for each question. Same\nas the guess number task, each full dialogue trajectory is\nexpanded into multiple training samples. The final corpus\nconsists of 24,105 training instances and 1,034 validation\ninstances.\nSupervised Finetuning. The SFT training procedure and\nhyperparameters are identical to those used for the Guess\nNumber task (Section D.1).",
          "is_title": false,
          "page": 19,
          "text_length": 494
        },
        {
          "text": "D.3. Fine-tuning BLIP",
          "is_title": false,
          "page": 19,
          "text_length": 21
        },
        {
          "text": "Synthetic Dialogue Dataset Construction. To address the\nabsence of a public corpus for our task, we employ a pro-\ngrammatic synthesis approach to generate a custom-built\ndataset. The CelebA dataset served as an ideal foundation,\nas its binary attributes are directly applicable for generating\nthe yes/no dialogues.\nOur synthesis pipeline begins by creating a question-\nanswer (QA) pair for each of the 40 attributes associated\nwith an image. To enhance linguistic diversity, we manu-\nally prepare three distinct phrasings for each attribute’s cor-\nresponding question. During generation, one of these three\ntemplates is selected at random. A positive QA pair is gen-\nerated if the attribute value is 1, while a negative QA pair is\ngenerated if the value is −1.\nTo construct a full dialogue, we first select images with\nno more than 20 positive attributes. The corresponding pos-\nitive QA pairs are then randomly shuffled to form the dia-\nlogue’s core. Rather than padding all dialogues to a fixed\nlength, we use a stochastic process to determine the num-\nber of negative questions. For an image Ii with m pos-\nitive attributes, we sample an integer x from the interval\n[0, 20 −m] and then randomly select x negative QA pairs.\nSubsequently, we apply a negative sampling strategy to in-\nsert these negative QA pairs among the positive ones. This\napproach preferentially places a related negative question\nbefore a positive one (e.g., asking about “Blond Hair” or\n“Gray Hair” before “Black Hair”). This method is designed\nto simulate a realistic process of elimination, where the\nscope of inquiry is progressively narrowed to identify the\ntarget attributes. In total, we generate 40,000 dialogues us-\ning this procedure. Fig. 10 shows an example of a generated\ndialogue.\nSupervised Fine-tuning. We use the official implementa-\ntion 1 to fine-tune the BLIP base model. The model is pre-\ntrained on the Flickr30k dataset [46]. We use the AdamW\noptimizer with an initial learning rate of 5 × 10−6, a weight\ndecay of 0.05, and a training batch size of 32. While the\nmodel is fine-tuned for a total of 12 epochs, the checkpoint\nfrom the 9th epoch is selected for the final evaluation, as it\nachieves optimal retrieval performance on the validation set.",
          "is_title": false,
          "page": 19,
          "text_length": 2244
        },
        {
          "text": "1BLIP codebase https://github.com/salesforce/BLIP",
          "is_title": false,
          "page": 19,
          "text_length": 49
        },
        {
          "text": "No. Question\nAnswer\n1\nIs the target male?\nYes\n2\nIs the target gray hair?\nNo\n3\nIs the target's hair blond?\nNo\n4\nDoes the target have brown hair?\nYes\n5\nIs the target wavy hair?\nYes\n6\nDoes the target have under-eye bags?\nNo\n7\nIs the target of a young age?\nYes\n8\nDoes the target have no facial hair?\nYes\n9\nIs the target arched eyebrows?\nNo\n10\nIs the target's mouth slightly open?\nYes",
          "is_title": false,
          "page": 19,
          "text_length": 379
        },
        {
          "text": "Figure 10. An example of synthetic dialogue.",
          "is_title": false,
          "page": 19,
          "text_length": 44
        },
        {
          "text": "We implement data augmentation to improve the model’s\ngeneralization and robustness. For image inputs, we ap-\nply standard transformations, including random horizontal\nflipping, adjustments to brightness and sharpness, random\ntranslation and rotation, and normalization. On the text side,\nwe apply an augmentation method termed ”Random Round\nRetention”. This method randomly truncates dialogues, re-\ntaining only the first N > 0 QA rounds as input. The objec-\ntive is to enhance the model’s adaptability to conversations\nof varying lengths. Additionally, since our dialogue texts\nsignificantly exceed the default 50-character limit in the of-\nficial BLIP implementation, we expand the maximum input\ntext length to 300. This modification ensures that the com-\nplete conversational context is processed by the model.",
          "is_title": false,
          "page": 19,
          "text_length": 814
        },
        {
          "text": "9",
          "is_title": false,
          "page": 19,
          "text_length": 1
        }
      ]
    }
  ],
  "timestamp": "2025-10-22T13:22:47.928916"
}